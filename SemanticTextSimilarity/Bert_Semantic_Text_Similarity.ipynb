{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_Semantic_Text_Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6UPSgXt7mMo8zArQA6BuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7405431d29e14bdfb30abf60a179a701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b3c92bfc4c6e4722aa541550c1c10b00",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_659032b4bdb040848c2677c4ff86fc3f",
              "IPY_MODEL_65e1492f821c47b5827501d5f5855a72"
            ]
          }
        },
        "b3c92bfc4c6e4722aa541550c1c10b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "659032b4bdb040848c2677c4ff86fc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_28658b4646fe40599521e324bf3b6896",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2373203d0d274f3d8c79b2e02b0697fd"
          }
        },
        "65e1492f821c47b5827501d5f5855a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75977c0f8238499c9bff4dd50c548aad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 683kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f78a1b6833e4249a3ff927392c1a7ce"
          }
        },
        "28658b4646fe40599521e324bf3b6896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2373203d0d274f3d8c79b2e02b0697fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75977c0f8238499c9bff4dd50c548aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f78a1b6833e4249a3ff927392c1a7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "176587eab3ad433ab7e0ed401cef1f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f6d8b09fc2584e5582d4801eea771b1f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fdca5cb17d5244708402b6eb6bbcf0e8",
              "IPY_MODEL_7afd408a33334bf0b26c1bd1fa4e0cf1"
            ]
          }
        },
        "f6d8b09fc2584e5582d4801eea771b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdca5cb17d5244708402b6eb6bbcf0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a02485564bbc4acc80066f6d33769cca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34d7fcaf123c45198f3cdfffde971cb7"
          }
        },
        "7afd408a33334bf0b26c1bd1fa4e0cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f96b1a901abb42e88880a238eaab1866",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:53&lt;00:00, 8.15B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ac1e33f014942169639157c7540bdee"
          }
        },
        "a02485564bbc4acc80066f6d33769cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34d7fcaf123c45198f3cdfffde971cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f96b1a901abb42e88880a238eaab1866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ac1e33f014942169639157c7540bdee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "878fc8c2e17e44d1abf992e585ffa7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be8bb46c2dc640678c65b9dc7ccde5f7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_693e7691852d404d81a1a4c14ac20b77",
              "IPY_MODEL_3c044203a4d74a60bb197f4b467b842b"
            ]
          }
        },
        "be8bb46c2dc640678c65b9dc7ccde5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "693e7691852d404d81a1a4c14ac20b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_13b62c0c3ede41f49b8218ef75f3fe69",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8b811abb27a42c7b98864ca545d3fc5"
          }
        },
        "3c044203a4d74a60bb197f4b467b842b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9debdbf399504f3882128223b8f9e026",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 66.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be38de922e0541008c8adca4155676e2"
          }
        },
        "13b62c0c3ede41f49b8218ef75f3fe69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8b811abb27a42c7b98864ca545d3fc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9debdbf399504f3882128223b8f9e026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be38de922e0541008c8adca4155676e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glingden/Natural-Language-Processing-NLP/blob/master/Bert_Semantic_Text_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_guI_jjrpWt",
        "colab_type": "text"
      },
      "source": [
        "# BERT Semantic Text Similarity\n",
        "In this notebook, the BERT (bert-base-uncased) model is used for finding  semantic text similarity between a pair of sentences.  The [STS_B benchmark](https://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) dataset is used for this work. This dataset consists of separate training, validation and test data. The work in this notebook contains two parts:\n",
        "1. Sentence level embedding extraction from the pre-trained  bert model and apply cosine similarity between the pair sentences (No fine-tunned, but only using sentence vector representation from the pretrained model)\n",
        "\n",
        "2. Fine-tune: The STS_B train dataset is used for fine tunning the model and validaiton dataset is used for model validation. And, the fine-tunned model is used to predict the test dataset. For fine-tunning, the two model network architectures are used: 1. Linear Regression ouput, and 2.Siamese Network with cosine similarity output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGfcD5iLrtAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIdMhqsNL-VT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "30d27aeb-69bf-4673-c91a-4e8abb2aec97"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(tf.__version__)\n",
        "!python -V"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1+cu101\n",
            "2.2.0\n",
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9FEdKjUp9Xv",
        "colab_type": "text"
      },
      "source": [
        "GPU from Google Colab is used in this work. So, check if GPU is avaiable, and if not rasie error. We have to identify and specify GPU as device in order to use it. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWARCoGcpiy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b6ac3017-9777-43ac-e9b7-687b064c299b"
      },
      "source": [
        "#check GPU is available or not \n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  current_dev = torch.cuda.current_device()\n",
        "  print('Found GPU :', torch.cuda.get_device_name(0))\n",
        "  print(current_dev)\n",
        "  \n",
        "\n",
        "else:\n",
        "    print('Not found, use CPU instead')\n",
        "    device = torch.device(\"cpu\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU : Tesla P4\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk2eRMkcHA6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN-iiHv5J5rF",
        "colab_type": "text"
      },
      "source": [
        "# **Load Dataset** <br> \n",
        "\n",
        "Dataset is stored at  Google drive. So, google drive  is mounted at '/content/drive' to access them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fyQjjuUaV91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9cb29d2c-45da-4695-d24d-fae56ae9775e"
      },
      "source": [
        "#mount the drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOC8EINhKX0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset\n",
        "dataset_types = [ \"sts-train.csv\", \"sts-dev.csv\", \"sts-test.csv\",] # 3 datasets\n",
        "col_names = [\"genre\", \"file\", \"years\", \"_\", \"score(0-5)\", \"sentence_1\", \"sentence_2\"] #columns names\n",
        "\n",
        "#collect as a list of pandas dataframes\n",
        "df_list = []\n",
        "for dataset in dataset_types:\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/Google_Colab/stsbenchmark_dataset/\"+ dataset, \n",
        "                 delimiter=',' , \n",
        "                 header= None,\n",
        "                 names= col_names\n",
        "                )\n",
        "  \n",
        "\n",
        "  df_list.append(df)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAB1ImAYLalv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#separate dataset (train, dev and test)\n",
        "df_train = df_list[0]\n",
        "df_dev = df_list[1]\n",
        "df_test = df_list[2]\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_YrEnaHOrGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b0449ada-7290-4580-e97e-2ada80853db1"
      },
      "source": [
        "#show random 5 rows in each dataset (train, dev and test)\n",
        "print(\"Show train_data shape: {}\".format(df_train.shape))\n",
        "print(\"Show dev_data shape: {}\".format(df_dev.shape))\n",
        "print(\"Show test_data shape: {}\".format(df_test.shape))\n",
        "print(\"Show Random 5 rows:\")\n",
        "df_train.sample(5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Show train_data shape: (5749, 7)\n",
            "Show dev_data shape: (1500, 7)\n",
            "Show test_data shape: (1379, 7)\n",
            "Show Random 5 rows:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>genre</th>\n",
              "      <th>file</th>\n",
              "      <th>years</th>\n",
              "      <th>_</th>\n",
              "      <th>score(0-5)</th>\n",
              "      <th>sentence_1</th>\n",
              "      <th>sentence_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3907</th>\n",
              "      <td>main-news</td>\n",
              "      <td>headlines</td>\n",
              "      <td>2013</td>\n",
              "      <td>195</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Clinton calls for friends of Syria to unite</td>\n",
              "      <td>Plastic artist calls for greater support for n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2099</th>\n",
              "      <td>main-forum</td>\n",
              "      <td>deft-forum</td>\n",
              "      <td>2014</td>\n",
              "      <td>99</td>\n",
              "      <td>3.60</td>\n",
              "      <td>Commodity Expanded Value-Form, .</td>\n",
              "      <td>Elementary Commodity-Capital Value-Form .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3386</th>\n",
              "      <td>main-news</td>\n",
              "      <td>MSRpar</td>\n",
              "      <td>2012train</td>\n",
              "      <td>662</td>\n",
              "      <td>2.25</td>\n",
              "      <td>But the cancer society said its study had been...</td>\n",
              "      <td>The American Cancer Society and several scient...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1603</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>images</td>\n",
              "      <td>2015</td>\n",
              "      <td>328</td>\n",
              "      <td>0.20</td>\n",
              "      <td>A dog runs through a field chasing a ball.</td>\n",
              "      <td>A child runs through the grass.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>main-captions</td>\n",
              "      <td>MSRvid</td>\n",
              "      <td>2012train</td>\n",
              "      <td>384</td>\n",
              "      <td>0.00</td>\n",
              "      <td>The man danced in the diner.</td>\n",
              "      <td>The bomb exploded in the desert.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              genre  ...                                         sentence_2\n",
              "3907      main-news  ...  Plastic artist calls for greater support for n...\n",
              "2099     main-forum  ...          Elementary Commodity-Capital Value-Form .\n",
              "3386      main-news  ...  The American Cancer Society and several scient...\n",
              "1603  main-captions  ...                    A child runs through the grass.\n",
              "763   main-captions  ...                   The bomb exploded in the desert.\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiwBL8bglyEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsWuhhJA0gal",
        "colab_type": "text"
      },
      "source": [
        "# **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdTCfSMTzmed",
        "colab_type": "text"
      },
      "source": [
        "Prepare text data according to BERT format. We have to tokenize the text sequences/sentences as per the BERT requirements before feeding them into Model. For this, we will use 'BertTokenizer' from  'transformers' from hugging face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQGuLbxehNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "ce910366-305f-49a9-c9d5-177005b72f13"
      },
      "source": [
        "#tranformer library from Hugging face\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/25/89050e69ed53c2a3b7f8c67844b3c8339c1192612ba89a172cf85b298948/transformers-3.0.1-py3-none-any.whl (757kB)\n",
            "\r\u001b[K     |▍                               | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 9.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 440kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 460kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 471kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 481kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 491kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 501kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 512kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 522kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 532kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 542kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 552kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 563kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 573kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 583kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 593kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 604kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 614kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 624kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 634kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 645kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 655kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 665kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 675kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 686kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 696kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 706kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 716kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 727kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 737kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 747kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 757kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ce593b467f594c8286a45287192a1222afd149e7e1f792cfb36771b8687a7290\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcFhA_hLzRpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7405431d29e14bdfb30abf60a179a701",
            "b3c92bfc4c6e4722aa541550c1c10b00",
            "659032b4bdb040848c2677c4ff86fc3f",
            "65e1492f821c47b5827501d5f5855a72",
            "28658b4646fe40599521e324bf3b6896",
            "2373203d0d274f3d8c79b2e02b0697fd",
            "75977c0f8238499c9bff4dd50c548aad",
            "9f78a1b6833e4249a3ff927392c1a7ce"
          ]
        },
        "outputId": "62bc89c1-50ac-4ee0-8de3-09b87fd0cca0"
      },
      "source": [
        "#import libraries from hugging face\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer= BertTokenizer.from_pretrained('bert-base-uncased') #initiate tokenizer "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7405431d29e14bdfb30abf60a179a701",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-nLTbRvew5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5e893f9b-bc42-4c76-ee91-1070774fa565"
      },
      "source": [
        "#check transformer\n",
        "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('just normal'))\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-30 09:26:38.841473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "[{'label': 'POSITIVE', 'score': 0.9987471699714661}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AN3HUqX5hT",
        "colab_type": "text"
      },
      "source": [
        "#Hugging face Tokenizer\n",
        "Example of Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHDUgN4VX3OQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "06364796-ae27-431f-b463-15083b3a249b"
      },
      "source": [
        "#text\n",
        "text = df_train.sentence_1[0]\n",
        "print(text)\n",
        "\n",
        "\n",
        "#Tokenize the text\n",
        "tokenize = tokenizer.tokenize(text)\n",
        "print(tokenize)\n",
        "\n",
        "\n",
        "#encode text\n",
        "input_ids =tokenizer.encode(text) # add '[CLS]' and '[SEP]' tokens\n",
        "print(input_ids)\n",
        "\n",
        "#convert input_ids back to tokens\n",
        "ids_to_tokens= tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(ids_to_tokens)\n",
        "\n",
        "#convert tokens back to input_ids\n",
        "ids_to_tokens= tokenizer.convert_tokens_to_ids(ids_to_tokens)\n",
        "print(ids_to_tokens)\n",
        "print()\n",
        "\n",
        "#Encode text with input_ids, token_type_ids, and attention_mask\n",
        "print(tokenizer(text)) #just use tokenizer and pass text to encode\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A plane is taking off.\n",
            "['a', 'plane', 'is', 'taking', 'off', '.']\n",
            "[101, 1037, 4946, 2003, 2635, 2125, 1012, 102]\n",
            "['[CLS]', 'a', 'plane', 'is', 'taking', 'off', '.', '[SEP]']\n",
            "[101, 1037, 4946, 2003, 2635, 2125, 1012, 102]\n",
            "\n",
            "{'input_ids': [101, 1037, 4946, 2003, 2635, 2125, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbJkTMr1X3Bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mJLJNF5eL8c",
        "colab_type": "text"
      },
      "source": [
        "#Count tokens in Sequences\n",
        "Since we have to have equal length of every sequence, check the maximun token length in every sentences in all datasets and  we will use this max length for all sequences as fix-length size while doing sequence padding.\n",
        "\n",
        "Or, we may use the other  best max-length. So that there is no risk of loss of much tokens while doing truncating/padding  in the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hRXkexz0YSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "43762ee6-50f5-42e6-bffa-fac56c293876"
      },
      "source": [
        "#counts tokens in sequences\n",
        "dataset_all = [df_train.sentence_1, df_train.sentence_2, df_dev.sentence_1, df_dev.sentence_2,df_test.sentence_1,df_test.sentence_2] #list of all dataset\n",
        "dataset_type_name = ['df_train.sentence_1', 'df_train.sentence_2', 'df_dev.sentence_1', 'df_dev.sentence_2', 'df_test.sentence_1', 'df_test.sentence_2'] #name of dataset\n",
        "\n",
        "max_len = 0 # tokens max count in overall datasets\n",
        "token_len_list = [] #store token counts for each dataset\n",
        "higest_token_len = {} # store higest token counts in each dataset\n",
        "\n",
        "\n",
        "#zip and iterate over all datasets\n",
        "for nam_data, each_dataset in zip(dataset_type_name, dataset_all):\n",
        "  token_len_dataset = [] #store each sequence count of each dataset\n",
        "  max_len_seq= 0 # max count of  sequence in each dataset \n",
        "\n",
        "  #iterate over sequences in each dataset\n",
        "  for sent in each_dataset:\n",
        "    tokens = tokenizer.encode(sent) #tokenize\n",
        "    tokens_len = len(tokens) # count length\n",
        "    token_len_dataset.append(tokens_len) \n",
        "    \n",
        "    \n",
        "    # keep tracking the higest counts in each dataset\n",
        "    if tokens_len > max_len_seq:\n",
        "      max_len_seq = tokens_len\n",
        "    \n",
        "    #higest count overall dataset\n",
        "    if max_len_seq > max_len:\n",
        "      max_len = max_len_seq\n",
        "       \n",
        "  higest_token_len[nam_data] =  max_len_seq \n",
        "  token_len_list.append(token_len_dataset)\n",
        "\n",
        "print(\"Higest tokens number in 3 datasets: %s .\" % (max_len))\n",
        "print()\n",
        "print('Higest counts in each dataset:\\n')\n",
        "higest_token_len\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Higest tokens number in 3 datasets: 70 .\n",
            "\n",
            "Higest counts in each dataset:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'df_dev.sentence_1': 45,\n",
              " 'df_dev.sentence_2': 53,\n",
              " 'df_test.sentence_1': 43,\n",
              " 'df_test.sentence_2': 46,\n",
              " 'df_train.sentence_1': 70,\n",
              " 'df_train.sentence_2': 63}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wzZ_weSCN0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB-hYEgEaLWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "6fa129d1-eda8-4c5f-c3ca-bd1903b35ace"
      },
      "source": [
        "#Plot the diagram to show the tokens' length distribution in dataset\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# data to plot\n",
        "data = token_len_list \n",
        "\n",
        "#creates subplots  with nrows=3, ncols=2\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12,11))\n",
        "sns.set(style='darkgrid') #set plot style\n",
        "\n",
        "#change 'axes' to 1d, zip and emumerate\n",
        "for i, (name, axe)  in enumerate(zip(dataset_type_name, axes.flatten())):\n",
        "  sns.distplot(token_len_list[i], axlabel ='Tokens Count', ax=axe) #distplot type\n",
        "  name = name + ': (tokens_max_count= '+ str(higest_token_len[name]) + ')' #title (for example: df_train.sentence_1:(max_count=70))\n",
        "  axe.set_title(name) #title for each suplots\n",
        "fig.tight_layout()#fit nicely\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAAMMCAYAAABKddZ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxTVd4/8E+Spvte0pLSspS1SAuFsg2CgmUb2mlBoQzKqOygMOrMSHWURR5EeBwc6YiMKDgM+tPBBR5KRcQNiqNSRbayFVoK3Um6pUvW+/ujJjZ2S2majc/79erL9N5zb76nqT18zzn3HJEgCAKIiIiIiIjIasT2DoCIiIiIiMjVMNEiIiIiIiKyMiZaREREREREVsZEi4iIiIiIyMqYaBEREREREVkZEy0iIiIiIiIrY6JFRERERERkZUy0HFRaWhpeeeUV0/fvvvsufvOb3yAuLg4VFRVd+t6LFi3Cxx9/3KXvQb/Izc3FrFmzcLtb2k2aNAnffPONlaMiZ7Jy5Up8/fXX9g6D7mBss+4cbLOos1566SW8++679g7DJphoOQGtVouXXnoJu3btwqlTpxAUFNRq2YEDB+L69euder8333wTM2fO7NQ9rOnXDbij+fvf/46kpCQMHjwY6enpHb7+1VdfxcKFCyESiQCwEXIm6enp+POf/2yVe8XFxZl9RUdHY8OGDabz//3vfzFt2jQMHToU8+fPR2Fhoenc4sWL8eqrr1olDqLOYpvluG2WQqHAU089hbvvvhsjRozA3Llzcfr06Q7dg22W87JmmwUAhw4dwvTp0zFs2DAkJCQgOzsbwC/J+MiRIzFy5Eg88sgjyM3NNV23YMEC/POf/4RGo7FaLI6KiZYTUCgUUKvV6NevX6fvpdPprBARNdWrVy/8+c9/xj333NPha8vKyvDdd98hISGhCyIjZ3Lq1CnTV1ZWFjw9PTFt2jQAgFKpxOOPP44//vGP+P777zFkyBA8+eSTpmtjY2OhUqlw9uxZe4VPZMI2y3HV1dUhJiYGH330Eb7//nvMnDkTS5YsQW1trUXXs80ioxMnTuDll1/Gpk2b8OOPP+Kdd95BZGQkACA0NBTbtm3D999/j2+//RaTJk0ya7NCQ0MRFRWFL774wl7h2wwTLQeRk5ODmTNnIi4uDk888QTUajUAIC8vz/SPrZEjR+IPf/hDq/d48MEHAQDJycmIi4tDZmYmvvvuO0yYMAFvvPEGxo0bh2eeeQZVVVVYunQpxowZg5EjR2Lp0qUoKSkx3Wf+/PnYt28fAOCjjz7C73//e2zevBkjR47EpEmTWp2iJAgCXnzxRYwdOxbDhw9HUlISLl++DADQaDTYvHkz7r33XvzmN7/BmjVr0NDQAACmGHft2oWxY8fi7rvvxocffggAeP/993Hw4EG89dZbiIuLw7JlywAApaWlWLlyJcaMGYNJkyZhz549pjjS09Pxxz/+EU8//TTi4uIwY8YMs3+AFhcX4/HHH8eYMWMwevRovPDCC6ZzH3zwAaZPn46RI0di4cKFZqMGrZk5cybuuece+Pj4tFv217755hsMHjwYHh4eAIC//OUvKCoqwrJlyxAXF4edO3cCAD7//HPMmDED8fHxmD9/Pq5evdri/a5evYpJkyYhIyMDAPDll18iOTkZ8fHxmDt3Li5evGgqO2nSJLz11ltISkrCiBEjzH7vlEolli5divj4eIwaNQrz5s2DwWBosy6TJk3Cm2++iaSkJAwbNgzPPvssbt26hUWLFiEuLg6PPPIIqqqqTOVXrVqFcePGYcSIEXjwwQdx5coVAI2/K8nJyfj3v/8NANDr9Zg7dy7+8Y9/tPn+er0eO3bsQEJCAuLi4jBr1iwUFxcDAH788Ufcf//9GDFiBO6//378+OOPZnE37Y1t2uN38+ZNDBw4EB9//DHuvfdejB49Gq+//joA4NixY/jnP/+JTz75BHFxcfjd737XZnwdceTIEQQHByM+Ph4A8Nlnn6F///6YPn06PDw8sHLlSly8eNHs92DUqFGcPkg2wzbLOdusyMhIPProowgNDYVEIkFqaiq0Wi3y8vLavM6IbRbbrKbvu2LFCgwbNgxisRhhYWEICwsDAPj7+yMiIgIikQiCIEAikaCgoMDs+jumzRLI7tRqtXDvvfcKu3fvFjQajfDJJ58IgwcPFrZu3SoIgiDcuHFDGDBggKDVatu914ABA4T8/HzT999++60QHR0tbNmyRVCr1UJ9fb2gVCqFw4cPC3V1dUJNTY2wcuVKYfny5aZrHnroIeE///mPIAiC8OGHHwqDBw8W3n//fUGn0wnvvPOOMG7cOMFgMDR772PHjgkzZ84UqqqqBIPBIOTm5gqlpaWCIAjCxo0bhaVLlwoVFRVCTU2NsHTpUuHll182i/Hvf/+7oNFohK+++kqIjY0VKisrBUEQhNWrV5t+FoIgCHq9Xpg5c6aQnp4uqNVqoaCgQJg0aZJw7NgxQRAEYdu2bcKQIUOEr776StDpdMLLL78szJ49WxAEQdDpdEJSUpKwceNGoba2VmhoaBBOnjwpCIIgfPbZZ0JCQoKQm5sraLVa4bXXXhNSU1Mt/BQF4U9/+pOwbds2s2OFhYXCiBEjhMLCwhaveemll4R169aZHZs4caJw4sQJ0/fXrl0Thg4dKmRlZQkajUZ44403hISEBEGtVpuVP3funHDPPfcIX3zxhSAIgnD+/HlhzJgxwk8//STodDrho48+EiZOnGh23f333y+UlJQIFRUVwrRp04R3331XEARBePnll4Xnn39e0Gg0gkajEU6ePNniZ/7ruGfPni2Ul5cLJSUlwpgxY4SUlBTh/PnzQkNDgzB//nwhPT3dVH7fvn1CTU2NoFarhf/5n/8Rfve735nOXbp0SYiPjxdyc3OF7du3C7NnzxZ0Ol2b779z504hMTFRuHr1qmAwGIQLFy4ISqVSqKioEOLj44WPP/5Y0Gq1wsGDB4X4+HhBqVS2+PPetm2b8Kc//UkQhF/+3/vrX/8q1NfXCxcuXBDuuusuITc3t1lZo7Vr1wojRoxo8SsxMbHNOhjNnz/f7Hdpw4YNwpo1a8zKzJgxQzh8+LDp+127dgmPPfaYRfcn6gy2Wa7RZgmCIOTk5AhDhgwRqqurBUFgm8U2y7I2S6fTCXfddZfwz3/+U0hISBDGjx8vrF+/XqivrzcrN2LECCE6OloYOHCg8Nprr5md+/TTT4WUlJQ2f0augCNaDuD06dPQarV4+OGHIZVKMW3aNMTExFjt/mKxGKtWrYK7uzs8PT0RFBSEqVOnwsvLC76+vli+fDlOnjzZ6vXh4eGYM2cOJBIJZs6cifLycty6datZOTc3N9TW1uLatWsQBAF9+/ZFaGgoBEHAf/7zHzz77LMIDAyEr68vli5dikOHDpld+9hjj0EqleKee+6Bt7d3qz1sZ8+eNU2lcnd3R2RkJObMmYPMzExTmREjRuCee+6BRCJBcnKyqVfszJkzKCsrw9NPPw1vb294eHiYRg3ee+89LFmyBH379oWbmxuWLVuGCxcuWDSq1dbPLjs7G+Hh4S2er6mpaXckLDMzE/fccw/GjRsHqVSKhQsXoqGhAadOnTKVyc7OxvLly7F582ZMnDgRQGPPampqKoYOHWr67KRSKX766SfTdfPnz0dYWBgCAwMxceJEXLhwAUDj51FeXo6ioiJIpVLEx8eb5uO35aGHHkK3bt0QFhaG+Ph4xMbGmno/J0+ejJycHFPZBx54AL6+vnB3dzeN0NTU1AAABgwYgOXLl2PFihXYtWsXtmzZAolE0uZ779u3D3/84x8RFRUFkUiEQYMGISgoCF999RV69eqFlJQUuLm5ITExEVFRUfjyyy/brY/R448/Dk9PTwwaNAiDBg0y62X9tXXr1iE7O7vFr4MHD7b7XoWFhTh58iRSUlJMx+rq6uDn52dWztfX12y6j4+PD6qrqy2uE9HtYpvlGm2WSqXC008/jccff9z094VtFtssS9qsW7duQavV4vDhw3jnnXewf/9+5OTkmEbPjIz3ef755zF48GCzc3dKm+Vm7wCocc5zWFiY2R+F1v7I3Y6goCDTMD8A1NfXY9OmTTh+/LhpWLy2thZ6vb7FPwzdunUzvfby8gLQ+A+/Xxs7diwefPBBvPDCCygsLMSUKVOwevVqqNVq1NfXY9asWaaygiCYDesHBgbCze2XX0cvL68W3wNo/IdoWVmZqbEBGofgm37fNGZPT0+o1WrodDoUFxcjPDzc7L2MioqK8OKLL2Lz5s1mcZaWlqJHjx4txtJZ/v7+7c6NLysrM/t9EIvFkMvlKC0tNR177733MHLkSIwePdp0rKioCPv378fevXtNx7RaLcrKykzfy2Qy02svLy/TuYULF+If//gHFixYAABITU3FkiVL2q1P05+7h4dHs8/B+Jnq9Xq88sorOHz4MJRKJcTixj6fiooKU4OfkpKCV155BVOmTEHv3r3bfe+SkhL07Nmz2fFf//yAxv+/mv78OlKvtn43reHAgQMYMWKEaa47AHh7e0OlUpmVq62tNfsHT21tLfz9/bssLiIjtlnO32Y1NDRg2bJlGDp0KJYuXdpm2abYZrHNAhp/NkBj4hsaGgoAePTRR/H666+bPYsFNLZfv//97zF27FhkZmYiJCQEwJ3TZjHRcgAymQylpaUQBMHUcBUVFZn9Q6szft2rs2vXLuTl5eE///kPZDIZLly4gJSUlNteqrWpP/zhD/jDH/4AhUKBJ554Am+++SZWrVoFT09PHDp0yDR/tzPxy+VyRERE4MiRIx2+l1wuR3FxMXQ6XbOGSy6XY9myZVZ91qY9AwcOxP79+9ssExoaanpuAGhsSIuLi81+luvXr8fOnTvx4osv4tlnnwXwS32WL1/e4bh8fX2RlpaGtLQ0XL58GQ8//DBiYmIwduzYDt+rJQcPHsTnn3+O3bt3IyIiAjU1NRg5cqTZ7+D69esxceJEZGVlITs72+wfJS3p3r07CgoKMGDAALPjoaGhKCoqMjtWXFyM8ePHA2hshOrr603nysvLLa5HSz2ma9asabUXMDw83KxXvCUHDhzA4sWLzY7179/fbPnquro6FBQUmC02cPXqVQwaNMji2IluF9usjsXvaG2WRqPBY489hrCwMLPnvSzBNottFgAEBASge/fuZvdrawTRYDCgvr4epaWlpkTrTmmzOHXQAQwbNgxubm7Ys2cPtFotjhw5cturh3Xr1g03btxos0xtbS08PDzg7++PysrKdh/YtNSZM2dMU0q8vLzg7u4OsVgMsViM2bNn48UXX4RCoQDQ+GDw8ePHLbpvSEgIbt68afo+NjYWPj4+eOONN9DQ0AC9Xo/Lly/jzJkz7d4rNjYWMpkMf/vb31BXVwe1Wo0ffvgBADB37ly88cYbpgdca2pq8Mknn7R7T61WC7VaDUEQoNPpoFarodfrLarbuHHjkJOTY3qgF2j+GU6fPh1ff/01/vvf/0Kr1WLXrl1wd3dHXFycqYyPjw/efPNNZGdn4+WXXwYAzJ49G++99x5Onz4NQRBQV1eHr776qtnISEu+/PJLXL9+HYIgwM/PDxKJxKJpGJaqra2Fu7s7goKCUF9fj61bt5qd379/P86fP49NmzbhueeeQ1paWru9qLNnz8arr76K/Px8CIKAixcvoqKiAvfccw/y8/Nx8OBB6HQ6ZGZmIjc3F/feey8AYNCgQcjMzIRWq8XZs2fx6aefWlyPkJAQFBYWmvV0v/DCC2YrCDb9ai/J+vHHH1FaWmpaTMBo8uTJuHLlCj799FOo1Wq89tprGDhwIPr27Wsqc/LkSUyYMMHi2IluF9ustjlym6XVarFq1Sp4eHhg8+bNppEZS7HNYptlNGvWLPz73/+GQqFAVVUV3n77bVOMJ06cQE5ODvR6PVQqFV566SX4+/s3a7OMyaMrY6LlANzd3ZGeno6PP/4Yo0aNQmZmJiZPnnxb93r88ceRlpaG+Ph4s/nfTT388MNQq9UYM2YMUlNTO/WLvmbNGqxZswZA4x+i5557DqNGjcLEiRMRGBiIhQsXAmhcmahXr16YM2cOhg8fjkceecTiVY4eeOAB5ObmIj4+HitWrIBEIsGOHTtw8eJF3HfffRgzZgyee+45i/4YG6+9fv06Jk6ciAkTJpgapsmTJ2PRokV46qmnMHz4cCQmJuLYsWPt3vP5559HbGwsMjIysGPHDsTGxuLAgQMAGnt54+LimvVOGXXr1g2jR4/G559/bjq2ZMkSvP7664iPj8dbb72FqKgo/O///i82bNiAMWPG4Msvv8SOHTvg7u5udi9/f3/s2rULx44dw9///nfExMRgw4YNeOGFFzBy5EhMmTIFH330Ubv1AYDr16/j0UcfRVxcHFJTU/H73/8eY8aMsehaS6SkpCA8PBzjx4/HjBkzMGzYMNO5oqIibNq0CZs3b4aPjw+SkpIwZMgQbNq0qc17Pvroo5g+fToWLFiA4cOH469//SvUajWCgoKwY8cO7N69G6NHj8abb76JHTt2IDg4GADwxBNPoKCgAKNGjUJ6ejqSkpIsrocxIRo9erRV9vHZv38/Jk+eDF9fX7PjwcHBSE9PxyuvvIKRI0fizJkzZg39mTNn4O3tjdjY2E7HQNQetlltc+Q269SpU/jyyy9x4sQJjBw50rRvn3H/I7ZZLWOb1dyKFSsQExODqVOn4re//S0GDx5sGo2srq7GU089hfj4eCQkJKCgoABvvvmmaUpwWVkZcnNz74htAkSCNcbeiei25ebmYvXq1fjggw+s2gNHd46VK1figQceuK293IiIOoJtFnXWSy+9hMjISNMWD66MiRYREREREZGVcTEMJ5Odnd3sQXmjpkunkvXwZ96oqKgIM2bMaPHcoUOHrLrqWGsWLVpkej6hqaVLl5o2BiUix8G/n7bHn3kjtlnkCDiiRUREREREZGVcDIOIiIiIiMjKmGgRERERERFZmUs8o1VRUQuDwXlnQIaE+EKhaH+ZV0fnCvVgHRyHK9TDFeoA2L4eYrEIQUE+XXLvvLw8pKWlobKyEoGBgdi8eTN69+5tViYrKwtbt27F5cuXMX/+fKxevbrZfa5du4aZM2di3rx5LZ5vi7HNcpXfj19z1XoBrls31su5sF6Opa02yyUSLYNBcOpEC4DTx2/kCvVgHRyHK9TDFeoAuE491q5di3nz5iE5ORkHDhzAmjVrsGfPHrMykZGR2LhxIw4fPgyNRtPsHnq9HmvXrr3tPWCatlmu8nP9NVetF+C6dWO9nAvr5Rw4dZCIiO4ICoUCOTk5SExMBAAkJiYiJycHSqXSrFyvXr0QHR0NN7eW+yLfeOMN3Hvvvc1GwoiIiJpyiREtIiKi9hQXFyMsLAwSiQQAIJFIEBoaiuLiYgQHB1t0j4sXLyIrKwt79uzB9u3bbyuOkBBf02uZzO+27uHoXLVegOvWjfVyLqyXc7Ao0ersnPann34aly5dMn1/6dIlvPbaa7jvvvuQnp6Od999F6GhoQCA4cOHY+3atVaoGhERkfVotVo8//zz2LRpkylZux0KhQoGgwCZzA/l5TVWjNAxuGq9ANetG+vlXFgvxyIWi8w60JqyKNHq7Jz2LVu2mF5fvHgRDz/8MMaPH286lpKS0uGHiYmIiDpCLpejtLQUer0eEokEer0eZWVlkMvlFl1fXl6OgoICLFmyBABQXV0NQRCgUqmwYcOGrgydiIicULvPaFlrTrvRBx98gKSkJLi7u3cibCIioo4JCQlBdHQ0MjIyAAAZGRmIjo62eNpgeHg4vvvuO3zxxRf44osv8PDDD2POnDlMsoiIqEXtJlptzWnvKI1Gg4MHD+L+++83O37o0CEkJSVhwYIFOHXqVIfvS0REZIl169Zh7969mDp1Kvbu3Yv169cDABYvXoyzZ88CALKzszFhwgTs3r0b7733HiZMmIDjx4/bM2wiInJCNl0M4+jRowgPD0d0dLTp2Ny5c7Fs2TJIpVKcOHECK1asQGZmJoKCgiy+b2vzIp1BTZ0GZco6oIX5/l6ebvDzdq6RP1d4iJF1cByuUA9XqAPgOvXo27cv9u3b1+z4zp07Ta/j4+Nx7Nixdu+1cuVKq8Z2J9IZALVW1+y4h9QNblwXmYicXLuJVmfntDf14YcfNhvNkslkptfjxo2DXC7HlStXMGrUKIvva3yw2BnVqnW4eKMKNaqGZudGRoehoVZth6huj7M+xNgU6+A4XKEerlAHwPb1aOvBYnItaq0OJy+UNjs+MjoMbh5cGJmInFu7/UWdndNuVFJSgh9++AFJSUlmx0tLf/kDe+HCBRQWFqJPnz4dujcREREREZEjsai7aN26dUhLS8P27dvh7++PzZs3A2ic075q1SrExMQgOzsbTz31FFQqFQRBwKFDh7Bx40bT6oIff/wxJk6ciICAALN7b926FefPn4dYLIZUKsWWLVvMRrmIiIiIiIicjUWJljXmtC9fvrzF48akjYiIiIiIyFXwUVMiIiIiIiIrY6JFRERERERkZUy0iIiIiIiIrIyJFhERERERkZUx0SIiIiIiIrIyJlpERERERERWxkSLiIiIiIjIyphoERERERERWRkTLSIiIiIiIitjokVERERERGRlTLSIiIiIiIisjIkWERERERGRlTHRIiIiIiIisjImWkRERERERFbGRIuIiO4YeXl5SE1NxdSpU5Gamor8/PxmZbKysjBr1iwMGTIEmzdvNjv32muvYcaMGUhKSsKsWbNw/PhxG0VORETOxs3eARAREdnK2rVrMW/ePCQnJ+PAgQNYs2YN9uzZY1YmMjISGzduxOHDh6HRaMzOxcbGYsGCBfDy8sLFixfx0EMPISsrC56enrasBhEROQGOaBER0R1BoVAgJycHiYmJAIDExETk5ORAqVSalevVqxeio6Ph5ta8L3L8+PHw8vICAAwcOBCCIKCysrLrgyciIqfDES0iIrojFBcXIywsDBKJBAAgkUgQGhqK4uJiBAcHd/h++/fvR8+ePdG9e/cOXRcS4mt6LZP5dfh9nYGl9RKUdfDzbT4a6O3tAVmwt7XDsoo7/TNzNqyXc3G1elmUaOXl5SEtLQ2VlZUIDAzE5s2b0bt3b7MyWVlZ2Lp1Ky5fvoz58+dj9erVpnPp6el49913ERoaCgAYPnw41q5dCwCor6/HM888g/Pnz0MikWD16tWYOHGilapHRERkfd9//z1effVV7Nq1q8PXKhQqGAwCZDI/lJfXdEF09qHR6vHeF7nQ6A1YOH0QRCJRu9fUqXWoUTU0P16nRrle3xVhdoqrfWZGrJdzYb0ci1gsMutAa8qiRKuzc9oBICUlxSz5Mnrrrbfg6+uLzz77DPn5+XjwwQdx5MgR+Pj4WBIaERGRReRyOUpLS6HX6yGRSKDX61FWVga5XN6h+5w6dQp/+ctfsH37dkRFRXVRtM6lrKIOr318DjfKVACAYVEhiB8UaueoiIjsq91ntKwxp70tn3zyCVJTUwEAvXv3xpAhQ3Ds2LEO3YOIiKg9ISEhiI6ORkZGBgAgIyMD0dHRHZo2eObMGTz55JPYtm0b7rrrrq4K1amUVdRh/dvZUFY3YNX9sYgM88XHx6/BYBDsHRoRkV21mxVZa077oUOHkJWVBZlMhpUrVyIuLg4AUFRUhB49epjKyeVylJSUdKgSrQ3XOQNBWQcATjdHvTWuMLeWdXAcrlAPV6gD4Dr1WLduHdLS0rB9+3b4+/ublm9fvHgxVq1ahZiYGGRnZ+Opp56CSqWCIAg4dOgQNm7ciPHjx2P9+vVoaGjAmjVrTPfcsmULBg4caK8q2d2pK7dQr9bhfxaNRng3H3j7eOClPSfxbU4JfjOkY6OFRESuxCaLYcydOxfLli2DVCrFiRMnsGLFCmRmZiIoKMgq9zfOd3dGdWodADjVHPXWOOvc2qZYB8fhCvVwhToAtq9HW/PdO6tv377Yt29fs+M7d+40vY6Pj291ZsWHH37YJXE5s6uFVegW4Inwbo1T/sfGyNEzzBcHsvIwKjoMbhIucExEd6Z2//o1ndMO4LbmtMtkMkilUgDAuHHjIJfLceXKFQBAeHg4CgsLTWWLi4s7vIITERER2cfVomr07RFg+l4sFmHWhCiUVzbgxNliO0ZGRGRf7SZa1pjTXlpaanp94cIFFBYWok+fPgCAadOm4f333wcA5Ofn4+zZsxg/fnyHKkFERES2p6xuQEWNGlHh/mbHY6JC0EPmg2/Pl7ZyJRGR67No6mBn57Rv3boV58+fh1gshlQqxZYtWyCTyQAACxcuRFpaGiZPngyxWIwXXngBvr7O+8wVERHRneJqUTUAoF+TES0AEIlEGNavGz75tgB1DVp4e0rtER4RkV1ZlGh1dk67MTFribe3N7Zt22ZJGERERORArhZWQeomRmRo8w7Sof264dB/r+NcnhKjosPsEB0RkX3xCVUiIiK6LVcLq9C7u1+LC15Eyf3h6yXF6dxbdoiMiMj+mGgRERFRh2l1BlwvrTFbCKMpsViEmKgQnL2mdNqVgYmIOoOJFhEREXXY9dIa6PQC+v5qIYymhvYLgapei2s/P8tFRHQnYaJFREREHXatsAoAWh3RAoAhfYIhFolw+iqnDxLRnYeJFhEREXVYblE1Qvw9Eejr0WoZb08pBkQG4HSuwoaRERE5BiZaRERE1GFXC6vQt0fr0waNYvt2w81yFRRVDTaIiojIcTDRIiIiIovpDICipnGj4rBgb9SqdaYvnaF5+ZioYABAznWljSMlIrIvi/bRIiIiIgIAtVaHYz8VAQAqVWqcvFBqOjfqru4oU9ahTq0zHQvw84CvlxSXCyoxPjbc5vESEdkLEy0iIiLqkNp6LQDA10tqdlyt1ePC9TLUqMynCfbtEYBLNyptFh8RkSPg1EEiIiLqEFUriVZr+kUE4FZVA5TVfE6LiO4cTLSIiIioQ1T1WkjEIni6Sywq3+/nJeAvFXBUi4juHEy0iIiIqENq67Xw8ZJCJBJZVD68mw+8Pdxw6UZFF0dGROQ4mGgRERFRh6jqdfD1svwxb7FYhAGRgbh0o6oLoyIicixMtIiIiFvFtRIAACAASURBVKhDVPVai5/PMhoQGYhSZR0qVeouioqIyLEw0SIiojtGXl4eUlNTMXXqVKSmpiI/P79ZmaysLMyaNQtDhgzB5s2bzc7p9XqsX78eCQkJmDx5Mvbt22ejyB2HWquHWquHTwcTrYE9AwEAl7n6IBHdIZhoERHRHWPt2rWYN28ePv30U8ybNw9r1qxpViYyMhIbN27EwoULm507ePAgCgoKcOTIEbz//vtIT0/HzZs3bRG6wzCuHNjREa2eYb7wdJdwQQwiumMw0SIiojuCQqFATk4OEhMTAQCJiYnIycmBUqk0K9erVy9ER0fDza35M0iZmZmYPXs2xGIxgoODkZCQgMOHD9skfkehrG6c+ufr2bFESyIWo18E99MiojuHRYlWZ6davPbaa5gxYwaSkpIwa9YsHD9+3HQuLS0NEyZMQHJyMpKTk/H66693rkZEREQtKC4uRlhYGCSSxiXJJRIJQkNDUVxc3KF7hIeHm76Xy+UoKSmxeqyOTPHziFZHpw4CQP+IQBTdqkVdg9baYRERORyLlgwyTrVITk7GgQMHsGbNGuzZs8esjHGqxeHDh6HRaMzOxcbGYsGCBfDy8sLFixfx0EMPISsrC56engCAJUuW4KGHHrJSlYiIiBxXSIiv6bVM5mfHSG5PTV3jHlqhIT7NlneXShv/WeHn62l23NvbA7JgbwyPDsPHx65BUadDr8hgCMq6ZmWblndEzviZWYL1ci6sl3NoN9EyTrXYvXs3gMapFhs2bIBSqURwcLCpXK9evQAAR48ebZZojR8/3vR64MCBEAQBlZWV6N69u1UqQURE1B65XI7S0lLo9XpIJBLo9XqUlZVBLpd36B5FRUWIjY0F0HyEyxIKhQoGgwCZzA/l5TUdutYRlCpq4ePpBlVt89UDtVodAKBG1WB2vK5OjXK9HkFeUogAnLpQgshgL9Spdc3KNi3vaJz1M2sP6+VcWC/HIhaLzDrQzM61d7E1plo0tX//fvTs2dMsydq9ezeSkpKwYsUKXL169bbuS0RE1JaQkBBER0cjIyMDAJCRkYHo6GizTsP2TJs2Dfv27YPBYIBSqcTRo0cxderUrgrZISmq1bc1bRAAvD3dIO/mg2tF1VaOiojI8Vi+26AVfP/993j11Vexa9cu07Enn3wSMpkMYrEY+/fvx6JFi3D06FFTYmeJ1rJIZyAo6wA0n2YBOPbUida4wpAv6+A4XKEerlAHwHXqsW7dOqSlpWH79u3w9/c3PVO8ePFirFq1CjExMcjOzsZTTz0FlUoFQRBw6NAhbNy4EePHj0dycjJOnz6NKVOmAAAee+wxREZG2rNKNldR04DunWibouT++Cn3FgRBsGJURESOp91EyxpTLQDg1KlT+Mtf/oLt27cjKirKdDwsLMz0OiUlBZs2bUJJSQl69Ohh8b2N0zCcUZ265WkWgONOnWiNsw75NsU6OA5XqIcr1AGwfT3amobRWX379m1x76udO3eaXsfHx+PYsWMtXi+RSLB+/fouic0ZqLV61NRp0a/H7Y1oAUBUD39knS1GeWU9fLzdrRgdEZFjaXfqoDWmWpw5cwZPPvkktm3bhrvuusvsXGlpqen18ePHIRaLzZIvIiIicgyKqtvbQ6upKLk/AOAqpw8SkYuzaOpgZ6darF+/Hg0NDWYbQ27ZsgUDBw7E6tWroVAoIBKJ4Ovri9dff73FvUuIiIjIvm5V3f7S7kY9ZD5wl4pxragasf26WSs0IiKHY1FG09mpFh9++GGr93777bctCYGIiIjsTFFVD6BzI1oSsRh9uvtzQQwicnkcOrKDerUOW/7fKUglYoQFe8HXxwNhgc0XwyAiInIkt6oa4CYRwcvD8gWrWhIV7o8jJ29AqzNYKTIiIsfT7jNaZH3n85S4XlIDrc6AU5dv4dNvr5vmvRMRETkqRXUDgvw8mm1U3FFR4f7QGwTcLFdZKTIiIsfDRMsOzuUp4OXhhuceHoE1C0ZCLBZxCgURETm8iho1An09OnydSCxCrVpn+pJ38wEA5BU7/6qcRESt4dRBGxMEAWevKTG4dxAkYjF8PKXoI/dHXnE1RgyUQSzuXC8hERFRV6lUqdGre8f3VFNr9Th9udzsmLenG/KLqxETZfkqxkREzoQjWjZWeKsWFTVqxESFmI4N7BWEBo0ehbdq7RgZERFR6wRBQKVKgwCfjo9otSTE3xM3yzh1kIhcFxMtGzt3TQkAGNLnlx68nt394ekuwbXCKnuFRURE1KbaBh20OgMCfK2zyXCIvwfKK+uh0emtcj8iIkfDRMvGzl5ToIfMB8H+v6wyKBGL0EfujxtltVBr2OAQEZHjqVSpAcBqI1rBAY3toLJabZX7ERE5GiZaNtSg0eHKzUrE9Alpdi4q3B8GQUB+CR8MJiIix2NMtAKtNqL1c6LFVXeJyEUx0bKhiwWV0OkFDGnhwd9gfw8E+rpz9UEiInJIFTWNiZa/lRItLw83BPi4Q1HNRIuIXBMTLRs6e00BD6kE/SMCm50TiUTo1d0P5ZX1qFfr7BAdERFR6ypVGgDWmzoIABGhvlBw6iARuSgmWjZ05UYV+kcGQOrW8o+9Z5gvAOAGV2EiIiIHU6lSw9dL2mobdjsiQn1RXauBVmew2j2JiBwFEy0bMQgCyirqEB7i02qZQF8P+HpJcaOUiRYRETmWyhq11Z7PMooIbexgVHL6IBG5ICZaNlKl0kCjMyAsyKvVMiKRCD3DfFGsqONyt0RE5FAqVWoE+lpv2iDwS6LF57SIyBUx0bKRsoo6AEBokHeb5SJDfWEQBBSWc/NiIiJyHJUqDQL9rJto+fu4w8vDDQquPEhELoiJlo2UVtQDQJsjWgAgC/KCp7uE0weJiMhhGAwCqlQaq49oAY0bF3MvLSJyRUy0bKS0og4Sschso+KWiEUiRIT6orC8lg8HExFZWV5eHlJTUzF16lSkpqYiPz+/WRm9Xo/169cjISEBkydPxr59+0znFAoFlixZgqSkJEyfPh3r1q2DTuf6K8VW12lgEAQEWfkZLQAICfBEFRfEICIXxETLRsoq6iEL9IJYLGq3bM9QX2j1Bly+UWmDyIiI7hxr167FvHnz8Omnn2LevHlYs2ZNszIHDx5EQUEBjhw5gvfffx/p6em4efMmAGDHjh3o27cvDh48iP/7v//D+fPnceTIEVtXw+Z+2ay4K0a0ft64mM9pEZGLYaJlI6XK+nanDRrJQ7whdRPjh0tlXRwVEdGdQ6FQICcnB4mJiQCAxMRE5OTkQKlUmpXLzMzE7NmzIRaLERwcjISEBBw+fBhA46JFtbW1MBgM0Gg00Gq1CAsLs3ldbK2ypnEPLWs/owXANNODC2IQkauxKNGyZKpFVlYWZs2ahSFDhmDz5s1m59qahtHWOVchCALKKuvaXQjDSCIRo3d3P5y+coubFxMRWUlxcTHCwsIgkUgAABKJBKGhoSguLm5WLjw83PS9XC5HSUkJAGDFihXIy8vD3XffbfoaMWKE7SphJxVdOKLl7ekGLw8Jn9MiIpfjZkkh41SL5ORkHDhwAGvWrMGePXvMykRGRmLjxo04fPgwNBqN2bmm0zAqKyuRkpKCsWPHIiIios1zrqKqVgON1oBQC0e0AKBvjwBcuVmF7EtlGB8b3v4FRETU5Q4fPoyBAwfiX//6F2pra7F48WIcPnwY06ZNs/geISG+ptcymV9XhGl1GoMAsQjo2ysYiqoG+Pm2/LyxVNr4z4pfn5dK3Vq8xng8LNgHFTVqUxlvbw/Igi3rnLQ1Z/nMOor1ci6sl3NoN9EyTrXYvXs3gMapFhs2bIBSqURwcLCpXK9evQAAR48ebZZotTYNY9GiRW2ecxWlysal3cOCLU+0ZIGekAV64ZuzJUy0iIisQC6Xo7S0FHq9HhKJBHq9HmVlZZDL5c3KFRUVITY2FoD5CNfevXvx4osvQiwWw8/PD5MmTcJ3333XoURLoVDBYBAgk/mhvLzGehXsQkWlNfD3cYdSWYs6tQ41qpan+Wm1jbMwfn1eq235GuNxf28p8ouroaysg9RNjLo6Ncr1jrefpDN9Zh3BejkX1suxiMUisw60ptpNtNqaatE00WrvHq1Nw2jrnKVaq5yj+Ola4/z/0BBfCD//HI0k0sb/ttTTN25oOPZ/fRV6sRjdQ3y6PE5rcIWeCNbBcbhCPVyhDoBr1CMkJATR0dHIyMhAcnIyMjIyEB0d3awtmzZtGvbt24cpU6agsrISR48exTvvvAMAiIiIwLFjxxAbGwuNRoP//ve/mDx5sj2qY1MVXbBZcVMhAT8viFHTgDALp9kTETk6i6YOOjpj76CjunqjAmKxCOdyy5utOjh0gAxA894/AIjrF4L9X19FxrGrSL67j01i7Qxn7YloinVwHK5QD1eoA2D7erTVO9hZ69atQ1paGrZv3w5/f3/TM8WLFy/GqlWrEBMTg+TkZJw+fRpTpkwBADz22GOIjIwEADz77LNYu3YtkpKSoNfrMXr0aMyZM6dLYnUklTUadAtoe3uSzjCtPFilZqJFRC6j3UTL0qkW7d2jtWkYbZ1zFaXKOnQL8LRoafemgv09Ed0rCN+cK8bvxvWGSNSx64mIyFzfvn1bXHRp586dptcSiQTr169v8fqePXuaptLfSSpVavSPCOiy+xsXxODKg0TkStpddbDpVAsArU61aItxGobBYIBSqcTRo0cxderUds+5irKKenQLtPz5rKZ+M6Q7yisbkFtYZeWoiIiI2qfVGaCq1yKwCzYrbirY35OJFhG5FIuWd1+3bh327t2LqVOnYu/evaaevsWLF+Ps2bMAgOzsbEyYMAG7d+/Ge++9hwkTJuD48eMAgOTkZERERGDKlCmYM2eO2TSMts65AkEQUFpZD9ltTrkYPkAGN4kY31/gnlpERGR7VV24tHtTIf6eqFZpoNUZuvR9iIhsxaJntCyZahEfH49jx461eH1b0zDaOucKqms1UGv0kHVgafemvDzcMLRvCE5eLMPv7+vf4emHREREnVGpalxJOKgLNituKiTAEwKAihqOahGRa7BoRItuX2lFPQBAdptTBwFg9OAwVNdqcLGgwlphERERWaQrNytuKsS/8f6KKm5cTESugYlWFyuzQqIV0zcEHu4SfH+h1FphERERWaSy5udEq4tHtLw83ODpzgUxiMh1MNHqYmWV9RCJgOBONFAeUgni+nfDD5fKodNz7joREdlOpUoNN4kYPp5duyOMSCRCSIAnlEy0iMhFMNHqYsrqBgT5eUAi6dyPenR0GGobdDiXp7RSZERERO2rVKkR6Otuky1GQvw9UaXSoEGj6/L3IiLqaky0upiyugHB/p3f5PGuPsHw8XTD9zmcPkhERLZTUaPu8mmDRqFBXhAA5Bc7/0bfRERMtLqYslpt2vG+M9wkYgzr3w1nrylgMAhWiIyIiKh9lSpNly+EYdQt0BMiANeKuHckETk/JlpdyCAIUNY0INjfOg3U4N7BqG3QoaCMPX1ERGQblSo1gmyUaLm7SRDo54GrhdU2eT8ioq7ERKsL1dRqoNMLCPbr/IgWAET3CgIAXMjnMu9ERNT16tU6NGj0CPRzt9l7hgZ5Ib+kGnoDF38iIufGRKsLKaobl8S1xtRBoHEPk/BuPsi5zkSLiIi6XqWN9tBqKjTICxqtATfKVDZ7TyKirsBEqwsZl6i11tRBoHFU68qNSmh17OkjIqKuVanSAIDNpg4CQOjP+05eucnntIjIuTHR6kLGTRdDAqwzogUAg3sFQaMz8EFhIiLqcrbarLgpHy8pgvw8kMtEi4icHBOtLqSoboCHuwTeHtbb5HFgzyCIREAOn9MiIqIu9svUQds9owUAUeH+uHKzEoLAVXaJyHkx0epCxqXdrbnJo7enG/rI/XGBz2kREVEXq1Cp4eUhgae79ToMLRHVIwCVKg0UVQ02fV8iImtiotWFFNXWW9q9qeheQcgrrka9Wmf1exMRERlV1qhtuhCGUVS4PwA+p0VEzo2JVhdSVjdYbcXBpgb3CoLeIODyjUqr35uIiMjIlpsVNxUe4gMvDwmu3GQ7R0TOi4lWF9Fo9aip0yK4CxKtfhEBcJOIcamADRARUUfk5eUhNTUVU6dORWpqKvLz85uV0ev1WL9+PRISEjB58mTs27fP7HxmZiaSkpKQmJiIpKQk3Lp1y0bR216lSm3z57MAQCwWYUBEIJ9HJiKnZttJ13eQihrjHlrW7wmUuknQW+6HK4VMtIiIOmLt2rWYN28ekpOTceDAAaxZswZ79uwxK3Pw4EEUFBTgyJEjqKysREpKCsaOHYuIiAicPXsW//jHP/Cvf/0LMpkMNTU1cHe3fSJiC4IgNCZaNlxxsKkhUSE4fVWB0oo6hAV52yUGIqLOsGhEq7M9gE8//TSSk5NNX4MGDcLnn38OAEhPT8fYsWNN59avX2+dmtmZaWn3ToxoicQi1Kp1zb50BqB/jwBcL6mBVqe3VshERC5NoVAgJycHiYmJAIDExETk5ORAqVSalcvMzMTs2bMhFosRHByMhIQEHD58GADw9ttvY8GCBZDJZAAAPz8/eHjYJxHpaqp6LXR6wS5TBwFgSFQwAODcNWU7JYmIHJNFI1qd7QHcsmWLqdzFixfx8MMPY/z48aZjKSkpWL16tZWq5BiMiVZQJxIttVaP05fLmx0fGR2GfhEB+OS7AuQV12BAZOBtvwcR0Z2iuLgYYWFhkEgkAACJRILQ0FAUFxcjODjYrFx4eLjpe7lcjpKSEgDA1atXERERgQcffBB1dXWYPHkyli9fbtXVZR2Forpxs2IvDzfUNll8yWCjFdfDgrwhC/TE+Twl7hsRYZs3JSKyonYTLWMP4O7duwE09gBu2LABSqXSrGFqrQdw0aJFZvf74IMPkJSU5LJTLYyU1WqIAAR1UU9gvx4BAIDcwiomWkRENqLX63Hp0iXs3r0bGo0GixYtQnh4OFJSUiy+R0iIr+m1TObXFWFaxZmfn4+qqtPi4o1fVv8b2CsIfr4tdyJKpY3/rPj1eanUrcVrWjvu7e0BWbA3Rg7uji+ybyAwyAdSN8d4rNyRP7POYL2cC+vlHNpNtKzRA2ik0Whw8OBBvP3222bHDx06hKysLMhkMqxcuRJxcXGdqZNDUFQ3wN/XvcsaBj9vd3QP9kYul74lIrKIXC5HaWkp9Ho9JBIJ9Ho9ysrKIJfLm5UrKipCbGwsAPP2LTw8HNOmTYO7uzvc3d1x33334cyZMx1KtBQKFQwGATKZH8rLa6xXQSsrU6gAAILBgBrVL/tZabU6s++b0mobR75+fb61a1o7XlenRrlej75yP2Rq9PjvTzcR3SvotutiLY7+md0u1su5sF6ORSwWmXWgNWXTxTCOHj2K8PBwREdHm47NnTsXy5Ytg1QqxYkTJ7BixQpkZmYiKMjyP6itVc6eVPU6dA/2MWXmgrKu1d48oHnvn/FcWz19Mf264dtzJejWzddhpq24Qk8E6+A4XKEerlAHwDXqERISgujoaGRkZCA5ORkZGRmIjo426zQEgGnTpmHfvn2YMmUKKisrcfToUbzzzjsAGmd1fP3110hOToZOp8O3336LqVOn2qM6Xa5K9cvUQXsZ1DMIErEI564pHCLRIiLqiHb/elqjB9Doww8/xP333292zPhAMQCMGzcOcrkcV65cwahRoyyuhLF30JEUK2oRGepryszr1K335gHNe/+M59rq6esR4o2aOg3OXiqFPMTHyjXoOGftiWiKdXAcrlAPV6gDYPt6tNU72Fnr1q1DWloatm/fDn9/f2zevBkAsHjxYqxatQoxMTFITk7G6dOnMWXKFADAY489hsjISADAjBkzcO7cOfz2t7+FWCzG3XffjQceeKBLYrW3qloNPN0lkIjt15Hn5eGG/hEBOJenxOyJdguDiOi2tJtoWaMHEABKSkrwww8/YOvWrWbXlZaWIiwsDABw4cIFFBYWok+fPtaom90IggBldQOG9Qvp0vfpH/Hzc1o3qxwi0SIicnR9+/Ztti8WAOzcudP0WiKRtLoCrlgsxjPPPINnnnmmy2J0FJUqtV1Hs4yGRIXgg6+u/rynl2uu8EhErsmiv6Cd7QEEgI8//hgTJ05EQECA2b23bt2K8+fPQywWQyqVYsuWLWajXM6opl4Lrc7QJZsVN9U92Bu+XlJcKazC+KHh7V9ARERkoSqVBt6OkGj1CcYHX13F2asKtnVE5FQs+gva2R5AAFi+fHmLx41JmytRWmEPLUuIRCL06xHABTGIiMjqqmo1CA3ysncYiAz1RYi/J05ducVEi4icimOslepiFFVqAF2faAFAv4gAlCjrUFOn6fL3IiKiO4PeYEBNnWOMaIlEIsQN6IZzeUrUN9nPi4jI0THR6gLKmsYRrWD/rp9L3nQ/LSIiImuorNFAEAAfT/snWgAwYoAMOr0B5/KU9g6FiMhiTLS6gLK6Ae5uYvh6Sbv8vfrI/SARizh9kIiIrEbx8xR4Hxu0Y5boHxEIXy8pfrxcbu9QiIgsxkSrCyiq1Qj297TJ3lZSNwl6d/fDFY5oERGRlRifNXaUES2xWIS4/t1wOvcWtDqDvcMhIrIIE60uoKxusMm0QaN+EQHIL65h40NERFZhHNHy9nSMES0AGD5AhgaNHheuV9g7FCIiizDR6gKK6oYuX9q9qX49AqHTG3C9xPk3RiUiIvtTVqvh4+kGqZvj/DNhcO8geLhLOH2QiJyG4/wFdRFanQFVKo1NVhw06vfzxsVXCitt9p5EROS6FNUNCLJhO2YJqZsEsVEh+OlKOQwGwd7hEBG1i4mWlVWoGpd2t+XUwQAfd4QGeXFBDCIisgpldQOC/GzXjlkqflAoquu0uHSDHYtE5PiYaFmZsso2mxX/Wv8eAcgtrIIgsJePiIg6R1GtRrADJlqxfUPgIZXg+wul9g6FiKhdTLSszPgAsa0TrX4RAaip06Ksot6m70tERK6lrkGHerXOIUe0PKQSxPXvhuyLZdDpuQAUETk2JlpWZlwS19YNVL+IQADAFU4fJCKiTvilHbPfM1oisQi1al2LXyMGhaG2QYecfK4+SESOzTE2yHAhimo1/L2lcJdKbPq+8hBveHu4IbewEnfHym363kRE5DqMMzOC/D1wq9I+syTUWj1Ot7K64LD+Mnh7uOH7C6WI7Rti48iIiCzHRMvKFNUNCPTzQK1aZ3bcmgskGXv6fi2qRwAu3+CIFhER3T7jiFawn/0SrbZI3cQYPlCG7Itl0Or0kLrZtmOTiMhSTLSsTFHVAHepGCd/9aDu0AEyq71Haz19/SMDcO6aAreq6tEtwMtq70dERHcORbUaErEIfj7u9g6lVaOjw5B1phhnrioxYqD12lciImviM1pWJAgCKmrU8PGU2uX9B/cKBgCcy1M2O6czoMW57jo+S0xERE0Yl3YXi0T2DqVVg3oFws9biu9ySuwdChFRqziiZUV1ah3UWj18vOzzYw0L9kKIvyfOXlXg3mE9zM6ptbpmo2wAMDI6DG4e/DUgIqJGiuoGm6+c21ESsRijo8Pw1U+FUNVr4etlnw5OIqK2cETLihQ/76FlrxEtkUiEmKhgXLhewWVviYhakJeXh9TUVEydOhWpqanIz89vVkav12P9+vVISEjA5MmTsW/fvmZlrl27hqFDh2Lz5s02iNq2lNUNCAlw7EQLAO6OlUOnF/BdDvfUIiLHxETLipTVagCAj6f9RoiGRIWgQaNHLpd5JyJqZu3atZg3bx4+/fRTzJs3D2vWrGlW5uDBgygoKMCRI0fw/vvvIz09HTdv3jSd1+v1WLt2LRISEmwZuk3oDQZU1GgQ7OAjWgDQM8wPPcN8kXWm2N6hEBG1yKJEq7M9gOnp6Rg7diySk5ORnJyM9evXm87V19fjiSeewOTJkzFt2jR8+eWXna+VnRiXxPWx4xSG6F5BkIhFOJunsFsMRESOSKFQICcnB4mJiQCAxMRE5OTkQKk0f641MzMTs2fPhlgsRnBwMBISEnD48GHT+TfeeAP33nsvevfubcvwbaJKpYFBEBDi73ibFbdkfGw4rpfWoKC0xt6hEBE1Y1GiZY0ewJSUFBw4cAAHDhzA2rVrTcffeust+Pr64rPPPsOOHTvw3HPPoba21gpVsz1ldQPcJCJ4uttvqVkvDzf0jwjAuWvNF8QgIrqTFRcXIywsDBJJ499oiUSC0NBQFBcXNysXHh5u+l4ul6OkpHHRhYsXLyIrKwuPPPKIzeK2JWOHoaM/o2U0enAY3CQijmoRkUNqd46bsQdw9+7dABp7ADds2AClUong4GBTudZ6ABctWtTm/T/55BO89NJLAIDevXtjyJAhOHbsGKZPn96ZetmForoBgb4eENl5paYhUSH44KurqKhRI8jPOXoliYgcnVarxfPPP49NmzaZkrXbERLia3otk/lZIzSrOf/zXoz9eofAQyqBn2/zhEsqdWvxuPEcgGbnW7umo8cBwNvbA7JgbwCADMCYIXJ8d6EMK+YMs8meWo72mVkL6+VcWC/n0G6i1VYPYNNEq60eQAA4dOgQsrKyIJPJsHLlSsTFxQEAioqK0KNHj1avs0TTRsueaup16Bbo1eHGBGjeKLV3TUvHjY3PhBGR+OCrq7heXosBUd0AAIKyrs1rrMUV/gdhHRyHK9TDFeoAuEY95HI5SktLodfrIZFIoNfrUVZWBrlc3qxcUVERYmNjAfzSvpWXl6OgoABLliwBAFRXV0MQBKhUKmzYsMHiOBQKFQwGATKZH8rLHWvKW/7NisYXOh3qtDrUqBqaldG2ctx4DkCz861d09HjAFBXp0a5Xm/6fuRAGbJOF+HIN3kYFR3WcsWsxBE/M2tgvZwL6+VYxGJRq7mITVZtmDt3LpYtWwapVIoTJ05gxYoVyMzMRFBQkFXub2y07K34lgoDewZ1uDEBmjdK6+u9ZAAAIABJREFU7V3T0nFj4+PjJkKIvwe+yC7AsKjGZLhO3fY11uCs/4M0xTo4DleohyvUAbB9PdpqtDojJCQE0dHRyMjIQHJyMjIyMhAdHW3WaQgA06ZNw759+zBlyhRUVlbi6NGjeOeddxAeHo7vvvvOVC49PR11dXVYvXq11WO1F2W1Gj6ebvB0d0OtWmfvcCxyV+9gdAvwxNHsm12eaBERdUS7z2g17QEE0G4PoFFxcTG6d+8OAJDJZJBKGxeIGDduHORyOa5cuQIACA8PR2FhYYvXORO1Ro9KlQayQC97hwKRSISxQ+Q4n6dERY3a3uEQETmMdevWYe/evZg6dSr27t1rWpxp8eLFOHv2LAAgOTkZERERmDJlCubMmYPHHnsMkZGR9gzbZpxhD61fE4tFmDwyErmFVbhayBV3ichxtDui1dkeQAAoLS1FWFhjL9OFCxdQWFiIPn36mK57//33ERMTg/z8fJw9exZ/+9vfrF3PLldeWQ8A6Bbo6RCja3fHdEfGN/n45lwxZoztbe9wiIgcQt++fVvcF2vnzp2m1xKJxGx13NasXLnSqrE5grKKevSQ+dg7jA67O0aO/cfzcOTkDSzvEWDvcIiIAFg4dXDdunVIS0vD9u3b4e/vb9qgcfHixVi1ahViYmKQnJyM06dPY8qUKQBg1gO4detWnD9/HmKxGFKpFFu2bIFMJgMALFy4EGlpaZg8eTLEYjFeeOEF+Po6xjNXHVH2c6IlC/RCqbLOztEAoUHeGBAZiONnivHbMb3sHQ4RETk4g0FAeWU94gZ0s3cobRKJRS1Oaxw/NByfnSzArcp6dHOA2SVERBYlWp3tATQmZi3x9vbGtm3bLAnDoZVV/DyiFeAYiRbQ2MO3K/MCrtysQo9Q50teiYjIdhTVDdAbBIQFWW+BpK6g1upx+nJ5s+N3x8rxefYNHP3hJube198OkRERmbNoHy1qX1lFHXy9pPD2tMn6IhaJHySDh1SCrLPcX4SIiNpm7DAMC3LO0aAgPw+MHBSKY6eLoKrX2jscIiImWtZSWlGPUAdrnDzd3TByUChOXiyDWmOdlQWJiMg1lVY0zsYIdfARrbb8dkwvqDV6ZHyTb+9QiIiYaFlLmQMmWgAwYWg41Bo9TpzjqBYREbWurKIe7m5iBPq62zuU2xYR6ovfxHTHFz/eNC1SRURkL0y0rECrM0BZ3YBQB3z4tl9EAAb3DsJn39+AVmewdzhEROSgjB2GIpHI3qF0yszxURCLRPjw66v2DoWI7nBMtKzgVlU9BMBhHyCeOT4KqnotLl6vsHco/5+9+46PqkofP/6Zkl5IJqSSEAQpoYcAAQslIqCEJgb4srKu0lTU1dVdWXVBUFBWdy2IsBb4Lj++uyoqIEXATlEQEOmoYCCQCpmE9Knn98eQISEJSSBhMuF5v155JXPvuXfOuTOZZ557zzlXCCFEE5WdV+LW3QbLGQK9Gdo3hh+O5pCaWeDq6gghrmOSaDWA8gHEru46WD7l7aU/ES396HKDgcOpRswWGaslhBCisvKp3d11IoxL3ZEYS4CvB//5/BdsdunNIYRwjaYzRZ4bayqJVk1T3gIM69eaw6lGjpzMo2f7pn2PFCGEENeO1Q6ZucVYbYoWAV7Oe1TZlYsrdhV8vPRMTGrPO+uPsHFnGiNvauPqKgkhrkNyRasB5OSV4uOlw9/Hw9VVqVGrUH9iIwI4ctJISZlMeyuEEMLBZLGybX8GAMbzZew+ms3uo9lY3fxKUL8u4fSNC+PT7anShVAI4RKSaDWA7PwSwoJ8m/wA4l4dWmJXsPfn6q96CSGEuD4VljhOwAX4Nd0ThvWl0WiYPKwjgX6evL3uiNzmRAhxzUmi1QCa6tTulwrw9aTrDQZSMwvJNpa4ujpCCCGaiMISMzqtBl+v5jWiwM/bg6kj4sg2lrD8s6PYVd36Q1rtVDvmubDE3Mg1FkI0J83rE9UFrDY7uefL6NMpzNVVqZOubQ2cSD/PD0dzGNE/1tXVEUII0QQUlFgI8PVo8j0zLqd8QqhLtW9tYNzAtnz87W+EBHqTMvjGWvdlsljZfTS7yvKBCa1x3yMkhLjWJNG6SsaCMmx25RZXtAD0Oi29O4Xx7U8Z/HI6n8QuEa6ukhBCCBcrLDET6Ou+NyqGmieE6hMXzp39YjEWmPhsVxqGQG9uS4h2QQ2FENcb6Tp4lXIu3Hm+qd5Dqzqtw/2JMPiy/3guJWVVz/4JIYS4ftiVovDCFa3mSqPRMOn29vRoF8J/vviFfb/KWGUhROOTROsqNZWp3etDo9GQ0CkUk8XGlh/SXF0dIYQQLpRfaMJuVwT6ufcVrdrotFoeGN2VNhEB/GvtYX7LkJkIhRCNSxKtK1BxkOzpnCI8PbTo9VqKTVa3ue9ISKA37VoF8u1P6Zy9cFVOCCHE9ac8BjTnK1rlvDx1PHp3DwL9PHn9o/3k5MnEUEKIxiOJ1hUoHyS7+2g2x07lEejryZ5jOW5335H49i3RajR89M0JV1dFCCGEi5T3zAhw8zFaddXCz5PHx/fAble8+uF+ikrl3pJCiMYhidZVUEqRV2jCEOjl6qpcEV9vD5ISotl9LIdfz+S7ujpCCNHoUlNTmTBhAsOGDWPChAmcPHmyShmbzcbcuXMZMmQIt99+O6tWrXKuW7x4MSNGjGDkyJHcddddbNu27RrWvnGkn3X0zPDzbp7zY5XPRljxJ9Dfi5l3dSe3wMQbHx/AYpV7bAkhGl6dPlVTU1OZNWsW+fn5BAUFsXDhQtq0aVOpjM1m44UXXmDbtm1oNBqmT59OSkoK4AhMGzduRKvV4uHhweOPP86tt94KwKxZs/juu+8IDg4GYPjw4Tz44IMN2MTGU1xmxWy1ExzgnokWwJDeMew6ks3/ff4Ls+/tg1YrE9cKIZqvOXPmMGnSJEaPHs3atWuZPXs2K1asqFRm3bp1pKWlsWXLFvLz8xkzZgz9+/cnOjqa7t27c//99+Pj48OxY8e455572L59O97e3i5q0dU7nVOMIcDbrad2v5zLzUY4bWRnlqw5xHsbjjJ9VBe0zfQYCCFco05XtMoD0+bNm5k0aRKzZ8+uUqZiYPrggw9YtGgRZ86cAaB79+589NFHrFu3jgULFvD4449TVlbm3Hb69OmsXbuWtWvXuk2SBZBXaAIgOMB9A6yXp44JSTeSll3Et/szXF0dIYRoNLm5uRw5coTk5GQAkpOTOXLkCEajsVK5jRs3kpKSglarxWAwMGTIEDZt2gTArbfeio+PY/Kjjh07opQiP999ewTY7HYyzxW7bc+Mq9WnUxgpg9vxw9EcPvzqOKqONzQWQoi6qDXRksBUs4uJlnsHqD6dwujUOohPvj0hfdWFEM1WZmYm4eHh6HQ6AHQ6HWFhYWRmZlYpFxUV5XwcGRlJVlZWlf2tWbOG1q1bExHhvvcjzMotwWJz754ZV6q8S+GtPaIY2DOKLbtPs2Z7qltNbCWEaNpq7Tp4ucBkMBgqlbvSwLR8+XI++OADYmJieOKJJ2jXrl29GhES4l+v8ldLGUsI8PemsMRCoJ8nhqCL99Dy8NAT4F/1CtfllgP13qY+yy+3ztfXizCDLw+Pj+fRf37Dhl1pPJzSs9p91EVoaMAVb9tUSBuajubQjubQBmg+7WgoP/zwA6+//jrLli2r97YVY5arj+uhNMdJz5iIwCox4kpjDVSNadcintV3uR0NP592tL9Lu5ZkGktYt+MkBSUWxiW1r/H5Xf2aNRZpl3uRdrmHazrytbrA9PjjjxMaGopWq2XNmjVMnTqVL774wpnY1UVubhH2a3j6qcRkpbCojJy8EoIDvCgsutgN0mKxVnpcl+VAvbepz/LLrSspMXHWZsNXr2FIQjSbd56ifVQgvTqEVrsfcExvb7JUvdFxiMGPsmJTjdu5g9DQAM6eLXR1Na5Kc2gDNI92NIc2wLVvh1araZQTaJGRkWRnZ2Oz2dDpdNhsNnJycoiMjKxSLiMjg+7duwNVTyTu27ePP//5z7z11lu0bdu23vUoj1lN4f1x6NezeOi06DVV49CVxhq4+n1dSTy72uV9O4VRUmrh233pBPh5EOznWe24NVe/Zo2hKbwXG4O0y724a7suF7Nq7TpYMTABtQamcpmZmZWuWpUHpsWLF1cKTOHh4Wi1jmqMGTOGkpKSaq+ENTUWq53CEkuz6m4xbmBb2kQE8N6GI2QZa763SMXp7Sv+lJZVTb6EEKKpCAkJIS4ujvXr1wOwfv164uLiKvXOAMekTKtWrcJut2M0Gvniiy8YNmwYAAcOHODxxx/njTfeoEuXLte8DQ3tdE4RkS39ZCIkHF+WBvSMIjYigPXbT/L9oWxsTagPYcV7eF76Y3WfO8sIcV2pNdFq7MCUnZ3t/Hvbtm1otVrCw8OvumGNLb+oeYzPqshDr+OhsV3RabUs/uQgJrNMdyuEaF6ee+45Vq5cybBhw1i5ciVz584FYNq0aRw8eBCA0aNHEx0dzdChQxk/fjwzZ84kJiYGgLlz51JWVsbs2bMZPXo0o0eP5ueff3ZZe66GUoq07EJiwvxcXZUmQ6/TMqBHJLf3jeF4+nm2/JDG+aKm0VOjppOcu49mV9vLRAjhenXqOvjcc88xa9Ys3nrrLQIDA1m4cCHgCEyPPvoo3bp1Y/To0ezfv5+hQ4cC1BiYyv3973+nY8eOPPXUU+Tm5qLRaPD392fJkiXo9U3/Xh55BY4PXoMbzzhYnZYtfJgxqgv//OAnFq85yENjuuLt2fRfDyGEqIt27dpVui9WuXfeecf5t06ncyZgl/r4448brW7XWm5BGcVlVlqFXttxzk2dRqNheL9YykxWdh3JZt2OU3RrZ6BrW0PtGwshRAV1+gbdmIHpf//3f+tShSbHWGjCQ6/Fz6f5JSFdbjDw++EdWbH5Z15a+SN/TOnRrK7cCSGEgLTsIgCiw/w5l1/q4to0PTdEBhJh8GX30Rz2H8/lt4wCggJ96dXO0GzvOSaEaFh1uo+WqCqvsIzgAK9m+2E7sGcr/nh3D7LzS3lhxR4OnzTWvlEDqqkvuvRDF0KIhpGWXYhGA61aStfBmvh46RnQM4rbEqLRajQs/mg/L/93H2nZ7jdgXwhx7TW/yzHXgF0p8gpNtGvVwtVVaVTd24Xw19/14q3Vh/jH+z8R374lE25rj5+PR6M/d3lf9Ev1iQtH7yVvWyGEuFpp2UVEGHzx9Kj7LL/Xq1ahfkSGtEFptKzdeoK5y3dzc/dI7hrQliB/6fEhhKiefGO9Arnny7DaFIZm0J2u/IaNl/Ly0KPXQuvwAJ6f2pctu0+z/rtTPPP2ThI6hhJu8JXuhEII4cbScgppHx3k6mq4Da1Ww8CEGPrFhbJux0m+3HuGXUey6dc5nNt7xxAdJmPdhBCVSaJ1BY6fcdzgsGWQj4trcvVMFhv7fzlbZXnFK0ceeh0j+rfhpq6RfLbzFFv3Z2C22okM8aVDTBAxYf4yNbAQQriR88VmjAUmWodLclBfft4eTLytPYN7tWLzrjS+O5TFtgOZtGrpxw1RgcSGB+Dp4RiZYbbYySs0YSwso7DEQkmZhVKTDS9PHX7eelr4eREbEUDbyEBiI/zx0MvVRSGaE0m0rsDRk3n4eukJ8vd0dVWuqeAALybd3oEhfWP44Itf+fl0Pt/+lIGPl552UYHc2NpAtMH9k08hhGjuDpw4B0CXNjKT3pUKD/bl98M7cdfAdmw/kMmxtDx++vUc2w9kViqn02oI8vci0M8TP28PQlr4YLbYKC6zcPSUke8PO+4dqtdpubFVIHGxwcTFGmgTGYBeJ0PphXBnkmjVk81u51haPq1a+jXbiTBq4+ftQbd2IXS5wUD6uWJ+OZ3P4ZNGZr/9PdGh/vS4MYSuNxho16qFBAkhhGiC9h/PxRDoRUyYPyVyz8Sr4u/jwfDE1gxPbI1SivwiMzabHTSOHiEBvh5oL/N9Ia/QRGpmAb+czufYqTxWb0tl9bZUvDx03BjdgqgQPyJCfAnw8yCvsAwfLz1eHrrr9juIEO5EEq16Ss0spNRkJaqlr6ur0qhqGrsFYFeO31qthpgwf2LC/Ck1WdF76Nl7JItNu9LY8P0p9DotrcP9uSEykBsiA7ghMpBwg+9lA44QQojGZbHaOJxq5KauEfJlvZ6sNjvmy4xr1mg0VcYvW+1Qaq66jYdej8VqxdNTR8fYYDrGBjPylhswW+ycOJPP0bQ8Tpw5z7dn8jFbKk+5q9dpCPTzpIWfJxEGX26MboGfl3QDFaKpkUSrng79lotGA5EhzXs63JrGbgH06BBaZZmPl56BCa25vVcrSk1Wjp7K4/iZ8/yWWcD2A5l8uffMhXI62kQEckNkIFEtfQn09cTf1wOrVVFUZqG41PGTV2wm81wx/j4e+Pt6YAjwuuKZsax2xyyGlyoPjEIIcT05lpaPyWKjx40tXV0Vt2Oy2NhTzxlxa5pFt0eH0BrHSPfuFEbvTmGAY6bj/EITZ84Vs++Xs5SUWSksNVNYbCHLWEJqZiHfH84mOtSPvnHhJHYOJ7QZjCEXojmQRKueDqUaiY0IwMtTBqzWxMdLT68OofS6kJDZ7YqM3GJSMwpIzSokNaOAzT+kYSu/NFYNjQZQULGEIdCL0zlFdIgOok1EAKHBPnW6OiZTxQshxEU/HT+Hp4eWuFiZcbCh1KUXyNXsy8tLzw1RLTAWlFVarpTifJEZnU7LwRO5fLL1Nz7Z+hvRof50vcFAp9hgwg0+GAK88ZAzi0Jcc/Itsx6KSi2kZhQwvF+sq6viVrRaDdGh/kSH+nNrD8cyi9VGboGJwhIzRSUWPPRa/Hw88PPxwN9bjw344Ug2JWUWCkssnM0vJdtYyvb9mXz9YzrgGDgc6OdBoK8ngX6eBPp60sLf0Y0iOtSfyBC5P4wQQlSklGL/8XN0aWOQGe4aUH17gVzJvqrbj0ajISjAiz5x4Yy8qQ3nzpey+1gOh34z8sXe02z6Ic1Z1sdLj06rQavVOH5rNOh0Gvx9PAgJ8sHHQ0dokDehQT6EBfsQGuSDn3fj3zdTiOZMEq16OHLSiALi2gRzLr/U1dVpcmrqu17eD/1SAX6etAzyrbb7XrHJik6rIcDXkwBfT6JaOrpq9uoQSn6hiZNZhWQZSygsNnO+xMz5IjOnc4ooKDY7r5RpNBAW5EN4iC9Wqx1fLz3eXno89Vr0ei1ncoqIDPElwMdDxikIIa4Lp3OKMBaYGHXzDa6uimgELVv4cEdiLHckxmIy2ziZVcC582XkFpRRVGLBrhR2u8Jmd/y22OwUl1rIzS/jbH4JhSWWSvvz89bTMsiRdEUYfIgJC6B1mH+de5QIcb2TRKseDv6Wi5+3ntjwAEm0qlFT3/Wa+qFD/bvv6XRaWocH0Do8oNr1NrudnLxS0s8Wc+ZsEennijmdU0Tu+bIqXRW3/HAacAwqDg3yoVWoPx3bGAjy8SA6zI/QFj51vj9YTePAQMaCCSGajv3HHdO692gX4uKaiMbm5amjY+tgOlaz7tKY5evrRUmJCWWHvMIycvJKOZtfytnzjt+nswvZ98tZZxz19/GgY0wQHVsH0al1MFGhflUSL4mLQkiiVWcmi40DJ3Lp3MYgN+dtohwf6nYC/b0I9Pci7gbH/WHsCvYczcJitVNqsmKx2bFaFTHh/pSWWckrNJFlLCEtq5C9P+egLuRjnnotUS39aBXqR3Sov/N3Cz/PKlfAahoHBjIWTAjRNNjsdr4/nE3bqEBa+HvVvoFwGzWNEaupR4ldwd5jF2NWgL83hUVl9IkLd84mfKlSs41TWQWcOVvEbxkF/Hr6PHsvnET189Y7kroLiVerUD9MFluNcbFvlwiMhWaOnzlPbkEZBUVmzFY7bSIC6RAdSGTLqombEO5Ivv3V0Vd7z1BYYuG2hGhXV6VZqSk41DR4uLYBxxUDR7keHULRaDR4eugqjdnq2T4Uv0sSoIBAHw78nM2ZHMfVsDNnizj4m5EdB7OcZXy99ESE+BJp8HX8DvHD20tPXqEJvU6D3e74QmO70D3j6Ekj3p46/Lw98PXWy6BkIcQ1Z7XDF3vOkGUsYdqozpU+R+s7WYNoei43rquu472g9hibZSxBr9PSISaIDjFBFJVa8PX24GRGAcfS8vixQuLVLroFnnot/j4eeHro0GigoNhMfqGZr/elczq7yLlvrcYxbmzb/gwAggO8GNI7moE9ovCVcWLCjUmiVQclZVY27jxFt7YhdIgJqvFDSNRffQb9Xq785bapSXUBRRWbCTP4EhMeWKlbQ0GJmfSzxaSfLSLTWEJWbgmHTxrZcSiL2mzhdOXn1eDoqtjSjxujW9CpdTCtw/3RaSX5EkI0jrzCMtZuSyXc4EOZqfIV+Pp+dormq74x1t/Hg75dIkjsHA6AsaCM42fO8+uZfH49c57c82VVttFpNcSE+9O9XQhRLX1p4eeFp4cj/rWJDCQ9p5jvDmWy6usTfLr9JLd0j+T23tGEBTfv+5eK5kkSrTrYsjuN4jIrdw1o6+qqiAZUXUAp7z7Rt0sEJsvF07w6nZbWEQG0iw6u1A2j1GQlJ6+U88Vmfj5lxGZXaC6cmdPpHL+7tW2Jp15LiclKcaljBsWMC2PH9v3qGC/h46WnW1sD3duF0K1tCAG+ntfmIAghrgtbfkjDZLHRu1OYTP4jGtSlsVSjgQ4xQaTc1p6dBzMpNVkxWWzY7YpAP0/8fDyI7xhWbUIXHuJHWLAv8R1DOZNTxNc/nuGbfel8tfcMPdu3ZFjf1rSPbiHvYeE2JNGqRWGJmc27T5PQMZTYiOonYBDNz5V0wzCZq7/S2Ta6Baqavjkeej3n8os5nn6eY6fyOJxq5IejOWiAtq0C6d42hBtbtSA2IkC6Tgghrli2sYRvf0qnXatAQgK9XV0dcR3x8dLjU48xypfG3k6xwbQOD6CwxMz2A5ns+/UcsREBJMW3oseNLQn0k5OSommr07s/NTWVWbNmkZ+fT1BQEAsXLqRNmzaVythsNl544QW2bduGRqNh+vTppKSkXNU6VzNZbPzvZ8cwm22MuVWuZokrc7mk7ZfT+YDj7F/76BbkFphQSnH0ZB6rt6U6y7Zs4bi3SUgLb4IDvAhp4U3LFj60bOGNr7ceb08PmcFJiDpozHjWFJ3IOM+bnxzEQ68lvr10ERTux9dbz8D4Voy5tS3fH8ri8z2nWf7ZMTQ4uhp2iGlBTJjjXp0hLbzx9dLLFS/RZNQp0ZozZw6TJk1i9OjRrF27ltmzZ7NixYpKZdatW0daWhpbtmwhPz+fMWPG0L9/f6Kjo694nSsZC8pY9PFB0rILmXBbe1pduI+TEI1Fo9HQsoU3fbtEcGf/NhSXWkjLKeR0dhEZ54rJLSjjp+PnKLrkPiceei0RBl9iwwOICfenc7uWeGrAEOhV73FfSjkm8LBY7Vhsdrz0Ojw9tBK0RLPRmPGsKbErxXcHs1ix+WeCAzyZObYbZ84W1b6hEE2QRqvBalf06RxO77gwzuQUcTjVyNFTeXy5Nx2rze4s66nXEuDrmIDDx9sDrcaxzFPvmJDDrhz/H+VRTSnHBFYKUBcmsQLw9tTj7akj0NfzwslNb0JaeBMS6E2Ar9x/U9RNrYlWbm4uR44cYfny5QAkJyfz/PPPYzQaMRgMznIbN24kJSUFrVaLwWBgyJAhbNq0ialTp17xumvNbLFxPP08h1KNfHcwE7PVzqN3d6fHjS2veV3E9evSK2DBAV4EBzimYu7RIZQ9R7IpKrVQVGqhsMTsTLz2nzjH9oOZ8MWvgGPAsZ+3Hh9vD3w8dei0GjRaDepCImW22p0JlbXC70s7OWo1Gny99fhe6ALi633hd4XHFf/28tSh12rQ67XotVr0Og16nRbdhd96nRad9uIymcJXXCuNHc9cyWa3YywwkZNfyqHfcvnhaA55hSY6tQ7iobHd0Gg1kmgJt1Vdz5CQFt786X/isVrt5BhLyMgt5nyRmfPFZopKLdhsCqvNcVuXUpONgmKzI74pKLkwEZZSjnHVWo2GQD9PtFrQabUoFOeLTWTnObYrM9sqPbenXutMuiomYP4+Hnh7ORI0H089Pl6O2Y51Wo0kZtepWhOtzMxMwsPD0ekc02LrdDrCwsLIzMysFJgyMzOJiopyPo6MjCQrK+uq1tXV1dzXav/xc2z8/hTFJitmi+MfSafV0K1dCKNuaUt4sE+VbfQ6bY1jZmpad7nlPl56bNb6bVOf5dduX5omWq+6Ly9/LZpavSqua+HvVeUeOD06hKJsdgpLLRSWWsk6V4Sx0ITJZKPEZMVksaLsCjugBXR6R8JTVGJBq9Wg0+BIerRaWoX5oddo0eo0WG12ykw2ysw2zBY7pSYLZWYbZWYrBSVmTGYbJout2rrWlU6rQavToNdcSLwuBCStpvZpp6v7z6/Xp0EdC1cXHzXVbXzJIp1Oi73Cmdbadlpte6p7mgsLnauqbYem5lV1VH749TptpTPGtbm1WwR94sKv+Hkb616FjR3P6qpi++rb1s0/pHHwt1ysNjs2O47fNjtmix37hZsA6rQaetzYkh43htC9XQg6rZZSs+2afUZVF9Oa4mequ8W5xnyO2mJfU6tvOZtdcexUHnDhCpRBT7jBMTth3A0GTucUU1RsqrRN3A0GjqYaq+yrpuUAHWODySsoI7/IxPkiM/lFJvKLzBQUmTmZXcSRC3WoiQacJx91ei36C//3CkeyV54AOh9XjH0XHpQvUzjigv1CgCwvqrhYQFUoDzgSyAsJpUbr+K3VatDi+AxyLOPanfis4Wn0Oh1W2xV8p6jmu8Llvj5oKtRBr9UydkDbau8dV1eX+xxvFpNhBAdfebe+pBB/khLb1Hu76MgWNa5rGx3cIMvdcV8x4YFNsl7Xw76EEO6hYswKCalfcJ90R+crfl53i1tN9TlcGeeay3M05L6uVX06smHZAAAgAElEQVSFuBK1DuCIjIwkOzsb24UM02azkZOTQ2RkZJVyGRkZzseZmZlERERc1TohhBCioTR2PBNCCCEqqjXRCgkJIS4ujvXr1wOwfv164uLiKnWzABg+fDirVq3CbrdjNBr54osvGDZs2FWtE0IIIRpKY8czIYQQoiKNUqqWURBw4sQJZs2aRUFBAYGBgSxcuJC2bdsybdo0Hn30Ubp164bNZmPevHns2LEDgGnTpjFhwgSAK14nhBBCNKTGjGdCCCFERXVKtIQQQgghhBBC1J3c4lQIIYQQQgghGpgkWkIIIYQQQgjRwCTREkIIIYQQQogGJomWEEIIIYQQQjQwSbSEEEIIIYQQooFJonUNLVy4kKSkJDp27Mgvv/ziXJ6amsqECRMYNmwYEyZM4OTJk66rZB3k5eUxbdo0hg0bxsiRI3n44YcxGo0A/PTTT4waNYphw4Zx//33k5ub6+La1uyhhx5i1KhRjBkzhkmTJnH06FHA/V4PgDfffLPS+8qdXgeApKQkhg8fzujRoxk9ejTbtm0D3KsdJpOJOXPmMHToUEaOHMnf/vY3wH3eT2fOnHEe/9GjR5OUlETfvn0B92mDO2kux7S5xLWKmkuMq0lzin2XcvdYWJ3mEB+r4+4xs86UuGZ2796tMjIy1ODBg9XPP//sXD558mS1Zs0apZRSa9asUZMnT3ZVFeskLy9P7dy50/n4pZdeUn/961+VzWZTQ4YMUbt371ZKKbV48WI1a9YsV1WzVgUFBc6/P//8czVmzBillPu9HocOHVJTpkxxvq/c7XVQSlX5n1BKuV07nn/+eTV//nxlt9uVUkqdPXtWKeV+76dyL7zwgpo7d65Syn3b0JQ1l2PaXOJaRc0lxtWkucS+SzWHWFid5hAfq9PcYmZNJNFygYr/NOfOnVMJCQnKarUqpZSyWq0qISFB5ebmurKK9bJp0yZ17733qv3796sRI0Y4l+fm5qqePXu6sGZ1t3r1ajV27Fi3ez1MJpMaP368On36tPN95Y6vQ3WBxJ3aUVRUpBISElRRUVGl5e72fipnMplUYmKiOnTokNu2oSlrjse0ucW1ippDjKuJu8a+SzWXWFgdd4+P1WluMfNy9K6+ona9y8zMJDw8HJ1OB4BOpyMsLIzMzEwMBoOLa1c7u93Of//7X5KSksjMzCQqKsq5zmAwYLfbyc/PJygoyIW1rNkzzzzDjh07UErx7rvvut3r8frrrzNq1Ciio6Ody9zxdQB48sknUUqRkJDAn/70J7dqx+nTpwkKCuLNN99k165d+Pn58cc//hFvb2+3ej+V++qrrwgPD6dLly4cOnTILdvQlLnb50x9Naf2uXuMq4m7x75LNadYWB13jo/VaW4x83JkjJa4Ks8//zy+vr7cc889rq7KFZk/fz7ffPMNjz/+OH//+99dXZ162bdvH4cOHWLSpEmurspV+7//+z8+/fRTPv74Y5RSzJs3z9VVqhebzcbp06fp3Lkzn3zyCU8++SSPPPIIJSUlrq7aFfn4448ZN26cq6shhMu5e4yriTvHvks1p1hYHXePj9VpbjHzciTRcrHIyEiys7Ox2WyA482Xk5NDZGSki2tWu4ULF3Lq1Clee+01tFotkZGRZGRkONcbjUa0Wq1bnF0ZM2YMu3btIiIiwm1ej927d3PixAluu+02kpKSyMrKYsqUKZw6dcrtXofy4+vp6cmkSZP48ccf3er9FBkZiV6vJzk5GYAePXoQHByMt7e327yfymVnZ7N7925GjhwJuPdnVFPV3I9pc2lfc4pxNXHH2Hep5hQLq+Pu8bE6zSlm1kYSLRcLCQkhLi6O9evXA7B+/Xri4uKa/CXSf/7znxw6dIjFixfj6ekJQNeuXSkrK2PPnj0AvP/++wwfPtyV1axRcXExmZmZzsdfffUVLVq0cKvXY/r06Wzfvp2vvvqKr776ioiICN577z2mTp3qNq8DQElJCYWFhQAopdi4cSNxcXFu9X4yGAwkJiayY8cOwDFrUm5uLm3atHGb91O51atXM3DgQIKDgwH3/Yxqypr7MW0O7XP3GFeT5hD7LtVcYmF1mkN8rE5zipm10SillKsrcb144YUX2LJlC+fOnSM4OJigoCA2bNjAiRMnmDVrFgUFBQQGBrJw4ULatm3r6urW6NdffyU5OZk2bdrg7e0NQHR0NIsXL+bHH39kzpw5mEwmWrVqxcsvv0zLli1dXOOqzp07x0MPPURpaSlarZYWLVrw1FNP0aVLF7d7PcolJSWxdOlSOnTo4DavAzj6aj/yyCPYbDbsdjvt2rXj2WefJSwszO3a8fTTT5Ofn49er+exxx5j4MCBbvd+GjZsGM888wwDBgxwLnO3NriD5nJMm0tcq6g5xLiaNMfYdyl3jYXVaS7xsTrNJWbWRhItIYQQQgghhGhg0nVQCCGEEEIIIRqYJFpCCCGEEEII0cAk0RJCCCGEEEKIBiaJlhBCCCGEEEI0MEm0hBBCCCGEEKKBSaIlxFXYtWtXpWmwhRBCiKZKYpYQ15be1RUQoqmIj493/l1aWoqnpyc6nQ6AuXPnMmrUKFdV7bJycnJ47bXX2Lp1K8XFxYSHh3PnnXcydepUfH19G+15Fy1axKlTp3jllVca7TmEEEJUT2JW/UjMEq4giZYQF+zbt8/5d1JSEi+88AI33XSTC2tUu/z8fCZOnEh8fDzvv/8+0dHRZGZm8t5775GWlkanTp1cXUUhhBCNQGKWEE2fdB0UohZms5n58+dzyy23cMsttzB//nzMZnO1ZVesWMGdd95JVlYWZrOZhQsXMmjQIG666SZmz55NWVkZcLH7xrJly+jfvz+33HILH3/8sXM/3377LXfeeSfx8fHceuutvPfee9U+3/Lly/Hz8+Pll18mOjoagMjISJ599llnwPrxxx8ZN24cCQkJjBs3jh9//NG5fVJSEt99953z8aJFi3jyyScBOHPmDB07dmT16tUMGjSIxMRElixZAsDWrVv517/+xWeffUZ8fHyTPXMqhBDXG4lZErNE0yGJlhC1WLJkCfv372ft2rV8+umnHDx4kLfeeqtKuTfffJPVq1ezcuVKIiIieOWVV0hNTWXNmjVs2bKFnJwcFi9e7Cx/7tw5CgsL2bp1K/Pnz2fevHmcP38egGeeeYZ58+axb98+1q9fT79+/aqt2/fff8/tt9+OVlv9v3J+fj4zZsxg8uTJ7Nq1i/vuu48ZM2aQl5dX5/bv3buXTZs28e9//5vFixdz4sQJBgwYwIwZM7jjjjvYt28fn376aZ33J4QQovFIzJKYJZoOSbSEqMW6deuYOXMmISEhGAwGZs6cWelDWinFiy++yI4dO1ixYgUGgwGlFB9++CFPP/00QUFB+Pv7M2PGDDZs2ODcTq/XM3PmTDw8PBg4cCC+vr6kpqY61x0/fpyioiJatGhBly5dqq1bfn4+oaGhNdb9m2++ITY2ljFjxqDX60lOTqZt27Z8/fXXdW7/ww8/jLe3N506daJTp04cO3asztsKIYS4tiRmScwSTYeM0RKiFjk5OURFRTkfR0VFkZOT43xcWFjIhx9+yKuvvkpAQAAARqOR0tJS7rrrLmc5pRR2u935OCgoCL3+4r+gj48PJSUlALzxxhssWbKEf/zjH3Ts2JEnnnii0sDnivs4e/ZsneteXv/s7Oy6Np+WLVtWW0chhBBNj8QsiVmi6ZArWkLUIiwsjIyMDOfjzMxMwsLCnI8DAwNZunQpf/3rX9m7dy8AwcHBeHt7s2HDBvbs2cOePXvYu3dvpcHLl9O9e3eWLFnCd999x5AhQ3jssceqLde/f38+//zzSsHwcnUvr394eDjgCEKlpaXOdZcLgJfSaDR1LiuEEOLakJhVPYlZwhUk0RKiFiNGjGDJkiUYjUaMRiOLFy9m5MiRlcokJibyyiuv8Mgjj3DgwAG0Wi0pKSksWLCA3NxcALKzs9m2bVutz2c2m/n0008pLCzEw8MDPz+/Gvuz33fffRQXF/PUU0+Rnp7ufJ4XX3yRY8eOMXDgQE6ePMm6deuwWq1s3LiR48ePM2jQIAA6derExo0bsVgsHDx4kM2bN9f5uISEhJCenl5jwBRCCHHtScyqnsQs4QqSaAlRi4ceeoiuXbsyatQoRo0aRZcuXXjooYeqlLv55ptZsGABDzzwAIcPH+bPf/4zsbGxjB8/nl69evGHP/zB2Z+9NmvXriUpKYlevXrx/vvv8/LLL1dbLigoiP/+97/o9XrGjx9PfHw89957LwEBAcTGxhIcHMzSpUtZvnw5iYmJvPvuuyxduhSDwQDAY489RlpaGn379mXRokVVgvHlDB8+HHAE7LFjx9Z5OyGEEI1HYlb1JGYJV9AopZSrKyGEEEIIIYQQzYlc0RJCCCGEEEKIBiaJlhBCCCGEEEI0MEm0hBBCCCGEEKKBSaIlhBBCCCGEEA1MEi0hhBBCCCGEaGCSaAkhhBBCCCFEA5NESwghhBBCCCEamCRaQgghhBBCCNHAJNESQgghhBBCiAYmiZYQQgghhBBCNDBJtIQQQgghhBCigUmiJYQQQgghhBANTBItIYQQQgghhGhgkmgJIYQQQgghRAOTREsIIYQQQgghGpgkWkIIIYQQQgjRwCTRchOzZs3i1VdfdT7+z3/+w0033UR8fDx5eXl13s/kyZNZtWpVY1RR1MPx48e56667UEpd0fZJSUl89913DVwr4U7uvvtufv31V1dXQ4gqJF41LxKvxNV65JFH+Pbbb11dDZeQRMsNWSwWXnrpJZYtW8a+ffsIDg52dZUa3KWBuql57bXXGDlyJJ07d2bRokX13v71119nypQpaDQaQAKRO1m0aBFPPvlkg+7z5MmTdOvWrdJ+d+3aRadOnYiPj3f+rF692rn+/vvv54033mjQegjR0CReuVZubi5/+tOfuOWWW0hISGDixIns37+/XvuQeOW+GjJeTZ48mW7dujnj0bBhw5zrdu7cyciRI+nduzeJiYnMnDmT7Oxs5/pp06bx+uuvN0g93I0kWm4oNzcXk8nEjTfe6OqqXLdiY2N58sknGThwYL23zcnJYdeuXQwZMqQRaibc0bx58+jWrVuV5WFhYezbt8/5M3bsWOe62267jV27dnH27NlrWVUh6kXilWuVlJTQrVs3PvnkE3744QfGjh3L9OnTKS4urtP2Eq9ERbNnz3bGo82bNzuX33jjjbz77rvs2bOHbdu2ERsby5w5c5zru3fvTlFREQcPHnRFtV1KEq0m6siRI4wdO5b4+Hgee+wxTCYTAKmpqQwfPhyAPn368Pvf//6y+9mxYwfDhw8nISGBefPmVbn0/9FHH3HHHXfQp08fpkyZQnp6OgBz5sxh4cKFlco++OCDLF++vNIypRQLFiygf//+9OrVi5EjR/LLL78AYDabWbhwIYMGDeKmm25i9uzZlJWVAY6z9QMGDGDZsmX079+fW265hY8//hiADz74gHXr1vHee+8RHx/PAw88AEB2djaPPPII/fr1IykpiRUrVjjrsWjRIv74xz/yl7/8hfj4eEaMGFHpHzozM5OHH36Yfv36kZiYyLx582o9BpczduxYBg4ciJ+fX61lL/Xdd9/RuXNnvLy8APjzn/9MRkYGDzzwAPHx8bzzzjsAfPnll4wYMYLevXszefJkTpw4Ue3+Tpw4QVJSEuvXrwfg66+/ZvTo0fTu3ZuJEydy7NgxZ9mkpCTee+89Ro4cSUJCQqX3ltFoZMaMGfTu3Zu+ffsyadIk7Hb7ZduSlJTEu+++y8iRI+nZsydPP/00586dY+rUqcTHx/OHP/yB8+fPO8s/+uij3HzzzSQkJPC73/3O2fXNbDYzevRo/t//+38A2Gw2Jk6cyJtvvnnZ57fZbCxdupQhQ4YQHx/PXXfdRWZmJgA//vgj48aNIyEhgXHjxvHjjz9WqnfFM7IVz/qdOXOGjh07snr1agYNGkRiYiJLliwBYOvWrfzrX//is88+Iz4+nlGjRl22fnWxYcMGAgIC6N+/f7228/LyokuXLmzfvv2q6yDE1ZB41XTjVUxMDPfddx9hYWHodDomTJiAxWIhNTX1stuVk3gl8aouWrZsSXh4uPOxTqcjLS2tUpm+fften90HlWhyTCaTGjRokFq+fLkym83qs88+U507d1b//Oc/lVJKnT59WnXo0EFZLJbL7ic3N1f17NlTffbZZ8psNqvly5eruLg49eGHHyqllPr888/VkCFD1PHjx5XFYlGLFy9WEyZMUEop9cMPP6gBAwYou92ulFIqPz9fdevWTWVlZVV6jq1bt6qxY8eq8+fPK7vdro4fP66ys7OVUkrNnz9fzZgxQ+Xl5anCwkI1Y8YM9corryillNq5c6eKi4tTr732mjKbzeqbb75R3bt3V/n5+UoppZ566ilne5VSymazqbFjx6pFixYpk8mk0tLSVFJSktq6datSSqk33nhDde3aVX3zzTfKarWqV155RaWkpCillLJarWrkyJFq/vz5qri4WJWVlandu3fXegzq4oknnlBvvPFGpWXp6ekqISFBpaenV7vNSy+9pJ577rlKywYPHqx27NjhfPzbb7+pHj16qO3btyuz2azefvttNWTIEGUymSqVP3TokBo4cKD66quvlFJKHT58WPXr10/99NNPymq1qk8++UQNHjy40nbjxo1TWVlZKi8vTw0fPlz95z//UUop9corr6i//e1vymw2K7PZrHbv3u18/WsyePBglZKSos6ePauysrJUv3791JgxY9Thw4dVWVmZmjx5slq0aJGz/KpVq1RhYaEymUzqhRdeUKNGjXKu+/nnn1Xv3r3V8ePH1VtvvaVSUlKU1Wq97PO/8847Kjk5WZ04cULZ7XZ19OhRZTQaVV5enurdu7davXq1slgsat26dap3797KaDRWe7zfeOMN9cQTTyilLv5/PfPMM6q0tFQdPXpUdenSRR0/frxK2XJz5sxRCQkJ1f4kJyfXWP/CwkI1dOhQlZmZWWW/O3fuVF26dFH9+/dXgwcPdr5/K3r++efVggULLnuMhGhMEq/cJ14ppdSRI0dU165dVUFBgVJK4pXEq7rHq3vuuUclJiaqvn37qgkTJqidO3dWWl/+XurYsaPq3Lmz+vjjjyutX7ZsmZo5c+Zlj1FzJFe0mqD9+/djsVi499578fDwYPjw4dV2K6rN1q1bad++PcOHD8fDw4N7772Xli1bOte///77TJ8+nXbt2qHX63nggQc4evQo6enp9O7dG41Gw549ewDYvHkzPXv2rHTGAkCv11NcXMxvv/2GUop27doRFhaGUooPP/yQp59+mqCgIPz9/ZkxYwYbNmyotO3MmTPx8PBg4MCB+Pr61niW7eDBgxiNRh5++GE8PT2JiYlh/PjxbNy40VkmISGBgQMHotPpGD16tPPM2IEDB8jJyeEvf/kLvr6+eHl50bt371qPwZWKiopiz549REVFVbu+sLCw1ithGzduZODAgdx88814eHgwZcoUysrK2Ldvn7PMnj17ePDBB1m4cCGDBw8GHGdXJ0yYQI8ePdDpdIwdOxYPDw9++ukn53aTJ08mPDycoKAgBg8ezNGjRwHH63H27FkyMjLw8PBwvgdqc8899zjPZvXu3Zvu3bs7z4DefvvtHDlyxFn27rvvxt/fH09PTx555BGOHTtGYWEhAB06dODBBx/koYceYtmyZfz9739Hp9Nd9rlXrVrFH//4R9q2bYtGo6FTp04EBwfzzTffEBsby5gxY9Dr9SQnJ9O2bVu+/vrrWttT7uGHH8bb25tOnTrRqVOnSmdaL/Xcc8+xZ8+ean/WrVtX43avvfYa48aNIyIiosq6tm3bsmbNGrZv386///1vDh8+zEsvvVSpjJ+fHwUFBXVukxANTeJVVU01XhUVFfGXv/yFhx9+mICAAEDilcSruserJ598ki+++IJt27YxYcIEHnjggUpXrcrfSzt37nS2s6LrNV7pXV0BUVVOTg7h4eGVPjRq+hCsbT8Vv8BpNBoiIyOdjzMyMliwYEGlLhdKKbKzs2nVqhV33nkn69evp0+fPqxbt67ay879+/fnd7/7HfPmzSM9PZ2hQ4fy1FNPYTKZKC0t5a677qq074qX9oOCgtDrL74FfXx8KCkpqbYt6enp5OTkOAMOOC7DV3xcMSh7e3tjMpmwWq1kZmYSFRVV6bnqegwaQ2BgYK3943Nyciq95lqtlsjIyEqDS99//3369OlDYmKic1lGRgZr1qxh5cqVzmUWi4WcnBzn49DQUOffPj4+znVTpkzhzTff5P777wdgwoQJTJ8+vdb2VDzuXl5eVV6H8tfUZrPx6quvsmnTJoxGI1qt4zxPXl6eM+iPGTOGV199laFDh9KmTZtanzsrK4vWrVtXWX7p8QPH/1DF41efdl3uvXmljh49yvfff19pgouKQkNDna9VTEwMf/7zn5kxY0albkTFxcUEBgY2aL2EqA+JV1U1xXhVVlbGAw88QI8ePZgxY8Zly1Yk8UriVbkePXo4/x47dizr16/n22+/ZfLkyZXKBQUFMXbsWEaPHs3WrVud7+XrNV5JotUEhYaGkp2djVLKGbwyMjKIiYmp936ysrKcj5VSzv7AAJGRkTzwwAM19ttNTk7m/vvvZ/r06Rw4cIDFixdXW+73v/89v//978nNzeWxxx7j3Xff5dFHH8Xb25sNGzZUOatYF5eemYqMjCQ6OpotW7bUe1+RkZFkZmZitVqrBK/ajkFj6NixI2vWrLlsmbCwMOfYAbj42lU8lnPnzuWdd95hwYIFPP3008DF9jz44IP1rpe/vz+zZs1i1qxZ/PLLL9x7771069at3mOHarJu3Tq+/PJLli9fTnR0NIWFhfTp06fSOIy5c+cyePBgtm/fzp49eyp9MalOREQEaWlpdOjQodLysLAwMjIyKi3LzMzk1ltvBRyBqLS01LmuPhNKVHfWdPbs2TWeCYyKiqp0Zrzcrl27SE9Pd57dLSkpwWazMXbs2GqTL41GU2XMyokTJ67pe1eIS0m8avrxymw2M3PmTMLDwyudqKkLiVcSry6370tjUjmbzUZubi5FRUUEBQUBjnjVqVOnOta8+ZCug01Qz5490ev1rFixAovFwpYtW65oppaBAwfy66+/smXLFqxWKytWrODcuXPO9RMnTuTtt992DvAsLCzks88+c67v3LkzwcHBPPvss9xyyy3Vnok4cOCAs+uIj48Pnp6eaLVatFotKSkpLFiwgNzcXMAxOHjbtm11qntISAhnzpxxPu7evTt+fn68/fbblJWVYbPZ+OWXXzhw4ECt++revTuhoaH84x//oKSkBJPJxN69e+t0DGpisVgwmUwopbBarZhMJmw2W53advPNN3PkyBHnoF5wnI06ffq08/Edd9zBt99+y/fff4/FYmHZsmV4enoSHx/vLOPn5+ec5eeVV14BICUlhffff5/9+/ejlKKkpIRvvvmGoqKiWuv19ddfc+rUKZRSBAQEoNPp6tQVo66Ki4vx9PQkODiY0tJS/vnPf1Zav2bNGg4fPsyLL77Is88+y6xZs2o9k5qSksLrr7/OyZMnUUpx7Ngx8vLyGDhwICdPnmTdunVYrVY2btzI8ePHGTRoEACdOnVi48aNWCwWDh48WGn2pNqEhISQnp5e6Wz3vHnzKs0OWPGnpqA1YcIEPv/8c9asWcOaNWuYOHEigwYN4r333gMc0+Wmp6c7v7S88sor3Hbbbc7tTSYThw8f5qabbqpz3YVoaBKvmna8slgsPProo3h5ebFw4ULnlZm6kngl8QqgoKCAbdu2Oa+8fvrpp+zZs8eZDG7ZsoXffvsNu92O0WjkxRdfpHPnzs4kC2D37t0MGDCgznVvLiTRaoI8PT1ZtGgRq1evpm/fvmzcuJHbb7+93vsxGAy8/vrr/OMf/yAxMZFTp07Rq1cv5/rbb7+dqVOn8qc//YlevXqRnJzM1q1bK+0jOTmZ7777juTkZOey2bNnM3v2bMDxYfTss8/St29fBg8eTFBQEFOmTAEcsxPFxsYyfvx4evXqxR/+8Ic6z3R09913c/z4cXr37s1DDz2ETqdj6dKlHDt2jNtuu41+/frx7LPP1ukDuXzbU6dOMXjwYAYMGOAMTnU5BtX529/+Rvfu3Vm/fj1Lly6le/furF27FnCczY2Pj69yhqpcy5YtSUxM5Msvv3Qumz59OkuWLKF379689957tG3blpdffpnnn3+efv368fXXX7N06VI8PT0r7SswMJBly5axdetWXnvtNbp168bzzz/PvHnz6NOnD0OHDuWTTz6ptT0Ap06d4r777iM+Pp4JEybwP//zP/Tr169O29bFmDFjiIqK4tZbb2XEiBH07NnTuS4jI4MXX3yRhQsX4ufnx8iRI+natSsvvvjiZfd53333cccdd3D//ffTq1cvnnnmGUwmE8HBwSxdupTly5eTmJjIu+++y9KlSzEYDAA89thjpKWl0bdvXxYtWsTIkSPr3I7yWdQSExMrTbdeXz4+Ps7ugaGhofj6+uLp6ems49GjR5k4cSI9e/Zk4sSJdOzYkWeeeca5/VdffUXfvn2v6Ay8EA1F4lXTjlf79u3j66+/ZseOHfTp08d5D6Ty8WwSr6on8aoyq9XKa6+9Rr9+/ejXrx8rV65k8eLF3HDDDYDjxMTUqVOds3lqtdpKszAeOHAAX19funfvfsV1cFcaVdN1PyFEozl+/DhPPfUUH330UYOehRPXj5SUFObPn1+lG4oQQjQkiVfiaj3yyCPcfffdV3TvUXcniZYQQgghhBBCNDCZDMPN7dmzh2nTplW7ruLUqqJ+5Lg6ZGRkMGLEiGrXbdiw4YpmF6uvqVOnOscoVDRjxgznzUGFEE2ffK42DjmuDhKvRFMkV7SEEEIIIYQQooHJZBhCCCGEEEII0cCaRdMxTsMAACAASURBVNfBvLxi7PaGuTAXEuJPbm7tMwNdD+RYXCTH4iI5FhfJsbioIY+FVqshONivQfbVFF1JzLqe3mvXS1uvl3aCtLW5krY6XC5mNYtEy25XDZZole9POMixuEiOxUVyLC6SY3GRHIu6udKYdT0d3+ulrddLO0Ha2lxJWy9Pug4KIYQQQgghRAOTREsIIYQQQgghGpgkWkIIIYQQQgjRwCTREkIIIYQQQogGJomWEEIIIYQQQjSwq060UlNTmTBhAsOGDWPChAmcPHmySpnt27dz11130bVrVxYuXFhpnc1mY+7cuQwZMoTbb7+dVatWXW2VhBBCCCGEEMKlrjrRmjNnDpMmTWLz5s1MmjSJ2bNnVykTExPD/PnzmTJlSpV169atIy0tjS1btvDBBx+waNEizpw5c7XVEkIIIYQQQgiXuapEKzc3lyNHjpCcnAxAcnIyR44cwWg0VioXGxtLXFwcen3V23Zt3LiRlJQUtFotBoOBIUOGsGnTpquplqgjqx2KTdYaf3KMJVWWWe2urrUQQgh3UVuckfgihGjOruqGxZmZmYSHh6PT6QDQ6XSEhYWRmZmJwWCo8z6ioqKcjyMjI8nKyqpXPUJC/OtVvjahoQENur+mKsdYwrHfcuu1Ta+OYYQafBupRk3b9fK+qAs5FhfJsbhIjoW4lMliZffR7DqV7RMXjt7rqr6WCCFEk9IsPtFyc4sa7M7UoaEBnD1b2CD7aupKTFYKi8pqXB/g711lfUmJibM2W2NXrcm5nt4XtZFjcZEci4sa8lhotZoGP4EmhBBCXGtX1XUwMjKS7OxsbBe+eNtsNnJycoiMjKzXPjIyMpyPMzMziYiIuJpqCSGEEEIIIYRLXVWiFRISQlxcHOvXrwdg/fr1xMXF1bnbIMDw4cNZtWoVdrsdo9HIF198wbBhw66mWkIIIYQQQgjhUlc96+Bzzz3HypUrGTZsGCtXrmTu3LkATJs2jYMHDwKwZ88eBgwYwPLly3n//fcZMGAA27ZtA2D06NFER0czdOhQxo8fz8yZM4mJibnaagkhhBBCCCGEy1z1GK127dpVe++rd955x/l379692bp1a7Xb63Q6Z3ImhBBCCCGEEM3BVV/REkIIIYQQQghRmSRaQgghhBBCCNHAJNESQgghhBBCiAYmiZYQQgghhBBCNDBJtIQQQgghhBCigV31rINCCCGEu0hNTWXWrFnk5+cTFBTEwoULadOmTaUyNpuNF154gW3btqHRaJg+fTopKSkA5Obm8te//pXMzEysViuJiYk8++yz6PUSToUQQlQmV7SEEEJcN+bMmcOkSZPYvHkzkyZNYvbs2VXKrFu3jrS0NLZs2cIHH3zAokWLOHPmDABLly6lXbt2rFu3jk8//ZTDhw+zZcuWa90MIYQQbkASLSGEENeF3Nxcjhw5QnJyMgDJyckcOXIEo9FYqdzGjRtJSUlBq9ViMBgYMmQImzZtAkCj0VBcXIzdbsdsNmOxWAgPD7/mbRFCCNH0SaIlhBDiupCZmUl4eDg6nQ4AnU5HWFgYmZmZVcpFRUU5H0dGRpKVlQXAQw89RGpqKrfccovzJyEh4do1ohnTaDUUm6zkGEsoNlkv+2O1u7q2QghRO+lULoQQQtTRpk2b6NixI//+978pLi5m2rRpbNq0ieHDh9d5HyEh/lf03KGhAVe0nSspYwkB/t51KmtHw8+n8+tUtlfHMEINvldTtSbBHV/TKyVtbZ6krZcniZYQQojrQmRkJNnZ2dhsNnQ6HTabjZycHCIjI6uUy8jIoHv37kDlK1wrV65kwYIFaLVaAgICSEpKYteuXfVKtHJzi7DbVb3qHhoawNmzhfXapikoMVkpLCqrU1mLxVE2wN+71m1KSkyctdkaooou466v6ZWQtjZP0lYHrVZT4wk06ToohBDiuhASEkJcXBzr168HYP369cTFxWEwGCqVGz58OKtWrcJut2M0Gvniiy8YNmwYANHR0WzduhUAs9nM999/T/v27a9tQ4QQQrgFSbSEEEJcN5577jlWrlzJsGHDWLlyJXPnzgVg2rRpHDx4EIDRo0cTHR3N0KFDGT9+PDNnziQmJgaAp59+mr179zJy5EjGjBlDmzZtGD9+vMvaI4QQoumSroNCCCGuG+3atWPVqlVVlr/zzjvOv3U6nTMBu1Tr1q1Zvnx5o9VPCCFE8yFXtIQQQgghhBCigUmiJYQQQgghhBANTBItIf4/e3ce3vZ5Hfj++wNAAAQJLgBBEuAqUgupfbflPbYkKjZdyklkdZSkuc3Ynk7cZG46tx27t5GsOm2u5+kz09aJ24mn8VJNGkdOE1u0Iiuy7FiyZe07tXMTCZAEAS4gSILY7h+UKFELdxILz+d5+Igk3h9+BxBJ4Pze855XCCGEEEKICSaJlhBCCCGEEEJMMEm0hBBCCCGEEGKCSaIlhBBCCCGEEBNMEi0hhBBCCCGEmGCSaAkhhBBCCCHEBJNESwghhBBCCCEmmCRaQgghhBBCCDHBJNESQgghhBBCiAmmiXQAQgghhBAA4XAYV0cP7Z09+ANhTCk6EnXyVkUIEZvkr5cQQgghIi4UCvP7E3bqm7sGvqfXqnlkSQ6Z6YkRjEwIIcZm3IlWTU0NL7zwAu3t7aSlpfHKK69QWFg4aEwwGOSHP/wh+/btQ1EUnnvuOTZs2ACAy+XixRdfxOFwEAgEuOeee/irv/orNBrJAYUQQojpIBwO896+auqbu1hWkklakhaAQ+ea2X3oKvctyKbIlhLhKIUQYnTGvUZry5YtbNq0iQ8//JBNmzaxefPm28bs2LGD+vp6du/ezTvvvMOrr75KQ0MDAP/8z/9McXExO3bs4P333+fs2bPs3r17vGEJIYQQIkZU1bax/6SD0oJ07p1vJceSRI4liS/fW4AlTc/+Uw6q7Z2RDlMIIUZlXImWy+WiqqqK8vJyAMrLy6mqqsLtdg8at3PnTjZs2IBKpcJkMrF69Wp27doFgKIoeL1eQqEQfX19+P1+srKyxhOWEEIIIWKEq6OXoxecLJqZwfISy6Db9Fo1q1fkYUnTc+hcM929gQhFKYQQozeuRMvhcJCVlYVarQZArVaTmZmJw+G4bZzNZhv42mq10tTUBMB3vvMdampqeOCBBwY+li1bNp6whBBCCBEjztW1oVErbHhsJoqi3Ha7WqVw33wrwWCYg1XNhMPhCEQphBCjF/GFULt27WLOnDm89dZbeL1enn32WXbt2sW6detGfB9mc/KExmSxGCf0/qJV2N2NMVk/5JhbbzcYdFhMhskMK2pNl5+LkZDn4gZ5Lm6Q50KMVo8vQK3Dw6y81CG7C6Yma1k8K4OjF5zUOjysnJs9hVEKIcTYjCvRslqtNDc3EwwGUavVBINBWlpasFqtt42z2+0sXLgQGDzDtW3bNv72b/8WlUqF0Wjk0Ucf5eDBg6NKtFyuLkKhibnCZbEYcTo9E3Jf0a7bF8DT1XvX243J+ttu7+724QwGJzu0qDOdfi6GI8/FDfJc3DCRz4VKpUz4BTQRnS41dBAKhynJTx92bGlhOnVNHg6da6H8/hkkSdt3IUSUG1fpoNlsprS0lMrKSgAqKyspLS3FZDINGrdu3Tq2b99OKBTC7XazZ88eysrKAMjNzeXTTz8FoK+vjwMHDjBr1qzxhCWEEEKIKBcKhblQ347VbCA1WTvseJWicO+8LHz+IB8fa5iCCIUQYnzG3XXwpZdeYtu2bZSVlbFt2za2bt0KwLPPPsvp06cBqKioIDc3l7Vr1/L000/z/PPPk5eXB8Bf/uVfcvToUZ588knWr19PYWEhTz/99HjDEkIIIUQUq2v20OMLUFow/GzWdaYUPflZyXxyvJGuHv8kRieEEOM37nn34uJitm/fftv3X3/99YHP1Wr1QAJ2q/z8fN54443xhiGEEEKIGHKhvh2jIYEcS9Kojls8M4Mdn9Xy24N1bHhk5iRFJ4QQ4zfuGS0hhBBCiNHo8QVoaeuhyJZyx06DQ0kz6lhWYuGjow10ePsmKUIhhBg/SbSEEEIIMaUcrm6AUc9mXfflewvwB0L89ou6iQxLCCEmlCRaQgghhJhS9lYvugQ15pShtxi5m8x0A/fOzeb3J+x4e2WtlhAiOkmiJYQQQogpEw6Hsbd6sWUYRl02eLOylXn4/EE+PWGfwOiEEGLiSKIlhhQOh+ntCxAOT8w+ZUIIIaY3d6eP3r7gmMsGARSVgjktkdl5afzuyFU6u/vw+gJ3/AiEJjB4IYQYBdntT9xRfbOHq2eaaGzporcvSLbJwPISS6TDEkIIEePsrV4ArOaxJ1o+f5CTF53kZiZx8Wo7735yhSJbyh3HrijNQiObGwshIkBmtMRt6po8fHLcTpOrG1tGEguKzbg9vVR+Xsc7H10iFJLZLSGEEGPT2OrFnKIjcQKSn5yMJFKTtFTVuqXyQggRdeQSjxjE2d7D/lMOMlL1fPXRWfT09LfOnVuYzslLrew/5UCvVVPxYNGI71OXoEEjKb0QQkx7ff4gzvYe5s8wTcj9KYpCaWE6X5xtptndQ7bZMCH3K4QQE0ESLTHA093Hx8caSdRp+NLSHDTqG9mRLkHNyrlZmFL17DnSQG9f8K5lGreSsg0hhBDQ39Y9HAbbONZn3arIlsLxi61U1bol0RJCRBWZZxADDp93EgyFeWxZ7l1LOioeKiIrPZEDZ5pwdfROcYRCCCFiWZO7mwS1Cktq4oTdp0atYk5+Gg1OL52ygbEQIopIoiUAaPP4aGjpYm5hOqnJ2ruO06hVPLTYhk6rZv8ph6zXEkIIMWLO9h4y0vSoVGNv634nc/LTUKkUqmrdE3q/QggxHpJoCQDOVLvQqBVK8tOHHZuo03DP3Cw6vH1cqG+fguiEEGJi1NTUsHHjRsrKyti4cSO1tbW3jQkGg2zdupXVq1ezZs0atm/fPuj2nTt38uSTT1JeXs6TTz5Ja2vrFEUf2wLBEG0eHxmpY9ukeCiJOg1FthSuNHbS2xeY8PsXQoixkIUzAk93H7UOD6WF6ei06hEdk2tJwmo2cPJyKzNsRvRa+VESQkS/LVu2sGnTJioqKnjvvffYvHkzb7/99qAxO3bsoL6+nt27d9Pe3s769etZtWoVubm5nD59mh//+Me89dZbWCwWPB4PWu3dqwDEDa7OXsJhyEibuLLBm80tSOdyQwcXr3awsNg8KecQQojRkBktwdkaN4qiMLdw5F2gFEVhRUkm/mCIk5ddkxidEEJMDJfLRVVVFeXl5QCUl5dTVVWF2z243Gznzp1s2LABlUqFyWRi9erV7Nq1C4A333yTb3/721gs/fsKGo1GdDrd1D6QGNXa3r+udzJmtADSjDpsGUmcr2sjGJJdioUQkSeJ1jTX4wtwuaGTmbkpGPSjm5VKM+qYnZfGxfp22jy+SYpQCCEmhsPhICsrC7W6f+ZerVaTmZmJw+G4bZzNZhv42mq10tTUBMCVK1e4evUqX//613nqqad47bXXZP+mEWrt6CU5MWFC9s+6m7mF6fT2BamxeybtHEIIMVJS7zXN1To8hMJhSgqGX5t1J4tnZlBt7+TUFRcPL7YNf4AQQsSwYDDIhQsXeOONN+jr6+OZZ57BZrOxfv36Ed+H2Zw8pnNbLMYxHRdJYXc3xuT+GSxXZy/Z5qSBr2+VkKAZuO1uY+409mazk3Qcu9jKhavtLJ6TiaIoGAw6LKbobPsei/+nYyWPNT7JYx2aJFrTXG1TJ+lGHWnJYyt90WnVzMlP40y1m/Yu35jvRwghJpvVaqW5uZlgMIharSYYDNLS0oLVar1tnN1uZ+HChcDgGS6bzca6devQarVotVoee+wxTp06NapEy+XqGnXHVovFiNMZe7M03b4Anq5eenwBurr9pOUn4Om689Ygfn//WGOy/q5jbh17JyX5aXx+pomLdW5sGUl0d/twBoPjfiwTLVb/T8dCHmt8ksfaT6VS7noBTUoHp7E2jw9ney8F2eO7GjG3MB21SuFMtbTVFUJEL7PZTGlpKZWVlQBUVlZSWlqKyTR4feq6devYvn07oVAIt9vNnj17KCsrA/rXde3fv59wOIzf7+eLL76gpKRkyh9LrHG29wCQMYH7Z91Nf4MmNedq2yb9XEIIMRRJtKaxk5f6WxIXZI0v0dJrNczOS6PG0YmnWzaLFEJEr5deeolt27ZRVlbGtm3b2Lp1KwDPPvssp0+fBqCiooLc3FzWrl3L008/zfPPP09eXh4ATzzxBGazmccff5z169czc+ZMvva1r0Xs8cSK1o5eFAVMKZNf9aBWqSjJT6Ox1Ut7l6wfFkJEjpQOTmPHLzlJN+qG3KB4pObNSOdCfTtna9zcOy97AqITQoiJV1xcfNu+WACvv/76wOdqtXogAbuVSqXixRdf5MUXX5y0GONRa3svJqMOjXpqru/Ozk/jdLWbqto21qzIn5JzCiHErWRGa5pyd/ZSbe8cd9ngdQZ9AjNzU7jc0EmPTzaLFEII0S8UDuPq6J20/bPuRK/VUJyTQnVjJx0yqyWEiBBJtKapoxecwPjLBm82t9BEKBzmQn37hN2nEEKI2Nbp7cMfDE3a/ll3M7fQRDgc5pPj9ik9rxBCXCeJ1jR1+EILtoykCSkbvC4lSUuuJYmLV9sJBmWzSCGEEP0VFADmlKlNtFKStBRkG9l/yk53r1RaCCGmniRa05C318+Vxg4WFpsn/L5LCq5tFumYHu0+hRBCDK3N40OlKKQkTdyFvZGaV2Sity/Ix8cbpvzcQgghidY0dL6unXAY5uSPbZPioVjNBtKStZyrayMcHt0+MUIIIeKPu9NHmlGLSqVM+bnNKXpKC9L53ZEG+vzRt5eWECK+SaI1DZ2rc6NLUFNonfjdvBVFoaQgnTaPj+a2ngm/fyGEELGlvctHujFym9mvWZFHp7eP/acdEYtBCDE9SaI1DVXVtjEnP23S2uwW2VLQJag5XyebRQohxHTm6e6jxxeMaKI1MzeVmTmpfHCgDn9A1g8LIaaOJFrTjLuzlyZ3N6UFE182eJ1GrWJWXir1zV2ygbEQQkxjjU4vQEQTLUVRqHhgBm0eH/tPSQdCIcTUGXeiVVNTw8aNGykrK2Pjxo3U1tbeNiYYDLJ161ZWr17NmjVrbtsscufOnTz55JOUl5fz5JNP0traOt6wxF2cuzbLNLfQNKnnmZOfhqIgrd6FEGIaa2yNfKIFMLcwnZm5qVTKrJYQYgqNO9HasmULmzZt4sMPP2TTpk1s3rz5tjE7duygvr6e3bt388477/Dqq6/S0NDfAej06dP8+Mc/5mc/+xmVlZX8/Oc/x2ic+LVDol9VbRtGQwI5lqRJPU+SPoGCLCOXGjro7ZO2ukIIMR01OrtI1GnQazURjePmWa1PT8qslhBiaowr0XK5XFRVVVFeXg5AeXk5VVVVuN3uQeN27tzJhg0bUKlUmEwmVq9eza5duwB48803+fa3v43FYgHAaDSi00X2yle8CofDVNW5KS1IR6VMfven0sJ0/IEQB6uaJ/1cQgghoo+91YspwrNZ180t6J/V2vlFHf6AdCAUQky+cSVaDoeDrKws1Go1AGq1mszMTBwOx23jbDbbwNdWq5WmpiYArly5wtWrV/n617/OU089xWuvvSZtwSeJ3dVNR1ffpJcNXmdJSyQjVc/vjzcSkv9TIYSYVgLBEE2ubtKiJNFSFIWnrs1qfXS0MdLhCCGmgcjO5dO/fuvChQu88cYb9PX18cwzz2Cz2Vi/fv2I78NsTp7QmCyW+Cxd/OK8E4AHluZhMRkIu7sxJuuHPObW2xMSNMMec7OlJZnsPlhPXWs3K+dmjz7oKBKvPxdjIc/FDfJc3CDPhbhZk6ubYCgcNTNaAKWFJubPMPHBgVoeXGQlSZ8Q6ZCEEHFsXImW1WqlubmZYDCIWq0mGAzS0tKC1Wq9bZzdbmfhwoXA4Bkum83GunXr0Gq1aLVaHnvsMU6dOjWqRMvl6iIUmpgZE4vFiNPpmZD7ijbHzjWRkapHFQzidHro9gXwdPXedbwxWX/b7X7/0MfcKjNVT1qylnf3XGTGJK8Lm0zx/HMxWvJc3CDPxQ0T+VyoVMqEX0ATU+9qSxcQ+UYYt/raI8VsfeMwO7+oY8MjMyMdjhAijo2rdNBsNlNaWkplZSUAlZWVlJaWYjINLk1bt24d27dvJxQK4Xa72bNnD2VlZUD/uq79+/cTDofx+/188cUXlJSUjCcscRfVjk6KbClTek6VSuHhxTmcq2ujvlnekAohxHRxtaULjVohJUkb0TgUlYLXFxj4MKclsrwkkz2HG2ho7Rp0mzQkFEJMpHGXDr700ku88MILvPbaa6SkpPDKK68A8Oyzz/K9732PBQsWUFFRwcmTJ1m7di0Azz//PHl5eQA88cQTnDlzhscffxyVSsUDDzzA1772tfGGJW7R3uXD3emjaEXqlJ/7vgXZ7DpYz+8OX+U/ls+d8vMLIYSYeledXVjNSahUk998aSg+f5CTF52DvpeXmczRC07e+u157l9wowpnRWkWGl3EV1UIIeLEuP+aFBcX37YvFsDrr78+8LlarWbr1q13PF6lUvHiiy/y4osvjjcUMYQaeycARdapndECMOgTeGChlU+ON/LVR4pJS46uMhIhhBATr6Gli5KC9EiHcUfJhgRKCtKoqm2jJD8dc+rI1x4LIcRIjXsfLREbqh2dqFUK+VmRWfewZnkuoVCYvccaInJ+IYQQU6erx0+Htw9rRvSuzV1YbEavVXPoXLN0OxZCTApJtKaJansnuZnJaBPUETl/ZrqBxbMy+PhYIz6/7F8ihBDxzN7qBSDbZIhwJHenTVCzZHYGzvZeahyyhlgIMfEk0ZoGQqEwNRFohHGrspX5eHsDfH6mKaJxCCGEmFzXEy2rOXoTLYCZOamYU/QcveDEL50whBATTBKtacDh8tLbF4zI+iy40fHJZkkiPyuZDw/V4+n1D+r0JF2fhBAifjS2etFp1VHX2v1WiqKwsjSTHl+AU1dckQ5HCBFnpLXONFDtuNYII0IzWjd3fCrIMrLvlIP3Pq0mN/PO68Wk65MQQsQ2e6sXmzkJRYlsx8GRsKQnUpyTQlWtG3url1k5U9+dVwgRn2RGaxqosXeSqNOQFQW18gXZRgx6DVW1bZEORQghxCSxu7zYMiL/mjNSy+ZY0GrU/GLPJULSGEMIMUEk0ZoGqu2dFFmNqKLgyqJKpVBSkE6Tuxt3Z2+kwxFCCDHBvL1+Orr6sEVxx8Fb6bUals2xUOPo5NOT9kiHI4SIE5JoxTmfP0iD08sMW/SUQszOTUWjVmRWSwgh4tD1Rhg5MZRoARTnpDAzN5V3P75CR5cv0uEIIeKAJFpxrq7JQygcjnjHwZtpE9TMyk2jxtGJp7sv0uEIIYSYQI3XEi2bObYSLUVR2PjYLPoCIf5190XZW0sIMW6SaMW5avu1RhgR6jh4N/NmpKOgcKbaHelQhBBCTCB7qxddghpTqj7SoYxatslAxQOFHLvo5PD5lkiHI4SIcZJoxblqRycZqXpSkrSRDmUQgz6BmbmpXGnsoKvHH+lwhBBCTBB7qxer2RAV64LHYt09+RRmG9m2+yKdUnUhhBgHSbTiXI29I6rKBm82v8gEwNkamdUSQoh4YW/1xlQjjFupVSq+/UQpPb4A23ZfjHQ4QogYJolWHOvo8uHq9EVd2eB1yYkJFOWkcqmhg+7eQKTDEUIIMU7dvX7au/pirhHGrXItyVQ8MIMj51v44mxTpMMRQsQoSbTi2MD6rCjqOHirBUUmwuEwp6tdkQ5FCCHEONlbuwGwxmiipagUvL4AXl+Ah5fkUGRL4V93X+Cqs2vg+9c/AqFIRyuEiHaaSAcgJk+1oxO1SiE/KznSodyV0aBlVm4qF6+2U1qQHnVryYQQQoyc3RWbrd2v8/mDnLzoHPh60Uwz9c0eXvv306xZmTdo3dmK0iw0OnkbJYS4O5nRimPV9k5yLcloE9SRDmVIC4szUKsUTlxqjXQoQgghxqHR6UWrUWGOwY6Dd2I0aFlZmkVzW4+sJxZCjJokWnEqFA5T29QZtY0wbmbQaygtNFHb5KG1oyfS4QghhBgju8uL1ZwUsx0H76Q4J4WCbCMnLrXS3NYd6XCEEDFEEq045XB10+MLxkSiBf37aukS1By70CqbRAohJk1NTQ0bN26krKyMjRs3Ultbe9uYYDDI1q1bWb16NWvWrGH79u23jamurmbRokW88sorUxB17Ij1joN3oigKq+ZnkZyYwKcnHPT2SfMmIcTISKIVp6rtHQDMiNKOg7fSatQsLDbT5O7m1BVpjCGEmBxbtmxh06ZNfPjhh2zatInNmzffNmbHjh3U19eze/du3nnnHV599VUaGhoGbg8Gg2zZsoXVq1dPZehRr7s3QJvHhy3DEOlQJpxWo+bhxTZ8/iD7TjrkgqAQYkQk0YpTNQ4PiToN2ebYecGbk59GWrKWX31yRa4YCiEmnMvloqqqivLycgDKy8upqqrC7R689mbnzp1s2LABlUqFyWRi9erV7Nq1a+D2n/70pzzyyCMUFhZOZfhRzzHQCCN6GzCNhylFz8qSTByubk5cah3UoXC4D+lQKMT0JO1y4lS1vYMZVmNM1cmrVAr3zsti18GrvP9ZLU9/aWakQxJCxBGHw0FWVhZqdX+DILVaTWZmJg6HA5PJNGiczWYb+NpqtdLU1L+X0vnz59m/fz9vv/02r7322pjiMJvHlohYLMYxHTdVjlf3J6zzZ2diuVY+GHZ3Y0weWWOMhATNwNjhjrl57Gjud7xjl5Zm0dHt53S1myPnnSQlJozofpfOycRiuv3CZ7T/n04keazxSR7r0CTRikM+f5CGFi+Pr8qPdCijlplu4N55Wfzu8FXum59NriU+r4wKIWKP3+/nBz/4AT/60Y8GkrWxcLm6CIVGAardFgAAIABJREFUV3pmsRhxOj1jPudUuFDjIkGjQhUMDsTa7Qvg6eod0fF+f/9YY7J+2GOujx3N/U7U2KWzM3B19PB/PjzP2hV5I+qw2N3twxkMDvpeLPyfThR5rPFJHms/lUq56wU0KR2MQ3VNHkLhMEXW6N2oeCgVDxah16p5a9d5giGptxBCTAyr1UpzczPBa294g8EgLS0tWK3W28bZ7faBrx0OB9nZ2TidTurr63nuued49NFHeeutt/jlL3/JD37wgyl9HNHK3urFajagUsVOJcVYqFUKDy+2kaRP4ONjjXT1+CMdkhAiSkmiFYeq7Z0AzIiRjoO3Sk5MYNOa2Vxp7OSDA3WRDkcIESfMZjOlpaVUVlYCUFlZSWlp6aCyQYB169axfft2QqEQbrebPXv2UFZWhs1m4+DBg+zdu5e9e/fyrW99i6effpqXX345Eg8n6thd8ddx8G4SdRr+4x/MxR8M8dGRBnr7gsMfJISYdiTRikM1jk7MKXpSk7SRDmXMVs3L5t65Wby/v5bLjR2RDkcIESdeeukltm3bRllZGdu2bWPr1q0APPvss5w+fRqAiooKcnNzWbt2LU8//TTPP/88eXl5kQw76vX4Arg7feRMk0QLwJaRxJeW5uDp8bP3aAOBoFRgCCEGkzVacajaHhsbFQ/nG2vncLmxg5++f5aX/nglBr38uAohxqe4uPiO+2K9/vrrA5+r1eqBBGwo3/3udyc0tlhmv9Zx0GaePokWQLbJwIMLrfz+hJ2PjzXypaU5aNRyDVsI0U/+GsSZDm8frs7euEi0DHoNz/3BPNydPv53ZdWoF48LIYSYGvbWa4nWNJrRuq4g28h987NxuLr5+FijzGwJIQZIohVnrm9UHA+JFsDMnFT+8LGZnLjcyi/2Xop0OEIIIe7A3upFo1ZhSUuMdCgRMTM3lfsX9Cdbe4824peNs4QQTECiVVNTw8aNGykrK2Pjxo3U1tbeNiYYDLJ161ZWr17NmjVr7li2UV1dzaJFi3jllVfGG9K0FAiB1xfgwtV2VApkpCUOu4FirEwQrV6ex5rleew50sDvDl+NdDhCCCFuYW/tnhYdB4dSnJPKAwuzaXZ3s/vwVXp8gUiHJISIsHEvetmyZQubNm2ioqKC9957j82bN/P2228PGrNjxw7q6+vZvXs37e3trF+/nlWrVpGbmwv0J2Jbtmxh9erV4w1n2vL5Axw+18zpKy7SjDpOXm4d9phFsy1TENnE2PjoTFydvfzio0votGoeWmQb/iAhhBBTwt7axazctEiHEXFFtlQSNGo+PWFn18F6HluWS0oMN6YSQozPuGa0XC4XVVVVlJeXA1BeXk5VVRVut3vQuJ07d7JhwwZUKhUmk4nVq1eza9eugdt/+tOf8sgjj1BYWDiecKa9cDhMa0cvGSPYPDHWqFQKzz45l/lFZt787Xk+OFBLOBwjU3JCCBHHenwBXJ0+rNNwfdad5GUms3ZlHn3+EL/9oh7HtUYhQojpZ1wzWg6Hg6ysLNRqNdDfqSkzMxOHwzFoXxKHw4HNdmMGwmq10tTUBMD58+fZv38/b7/9Nq+99tqY4rjbbsxjZbEYJ/T+pkLY3U0grOAPhMjNSsGYPHyylZCgGXbcrbeP5JixnOdmBoMOi8lwx9v++k/u4+//7Ti/+n01vmCYbz85nwTN2K4XeLr76OkdWWlHi7sb1GoS9RqMBrk6GYu/I5NFnosb5LmYnhyubmD6dRwciiUtkS/fm8/HxxrZc6QBo0HLE/cWoCjTt7RSiOkoov2y/X4/P/jBD/jRj340kKyNhcvVNWEd6SwWI06nZ0Luayp1+wLUXWuEkaxX4+nqHfYYvz8w5Dhjsv6224c7ZiznuVV3tw9n8O6bP35z7Sx0GoXK/TWcuujk2SfnkmMZfbLt9fWXW47E9ediRWkWvV7fqM8VT2L1d2QyyHNxw0Q+FyqVMuEX0MTkaWztAiDHIonWzVKStDy+qoD9pxz8+++rsTu9/NG6Oei1slWJENPFuH7brVYrzc3NBINB1Go1wWCQlpYWrFbrbePsdjsLFy4EbsxwOZ1O6uvree655wDo7OwkHA7T1dXFyy+/PJ7QpqXWjh4SNKqY3qh4JFSKwh8+Nos5+Wm8+dvzbH3zCE+sKmDN8jzZa0sIIaZYo9NLgkZF5jTtODiUBI2KR5bYcHt87DxQR12zh++snz+mi4NCiNgzrjVaZrOZ0tJSKisrAaisrKS0tHRQ2SDAunXr2L59O6FQCLfbzZ49eygrK8Nms3Hw4EH27t3L3r17+da3vsXTTz8tSdYYtXb0Yk7Vx3xpgqJShu2Y6PUFmJ2fzovfXMb8IhPv7a/hL/7pc97fX4O7c3QzbkIIIcausdWLzZw0rTsODkVRFNbdU8D/s3Ex3t4AL799hM9OOyIdlhBiCoz78v9LL73ECy+8wGuvvUZKSspAe/Znn32W733veyxYsICKigpOnjzJ2rVrAXj++efJy8sb76nFTfoCQdo8PubPMA0/OMr5/EFOXnSOePzCYjNrV+bzu0P1/GZ/Db/ZX8Os3FSWzrYwMyeV/CzjmNdxCSGEGFqjs4u5hbH/2jOZFJVCvjWFv9i0hDd/e55/+eAcFxs6WP/QDLSa25dO6BI0yMuWELFv3IlWcXHxHffFev311wc+V6vVbN26ddj7+u53vzvecKathuYuwuH+/bOmo7zMZL771YU0ubs5fK6ZQ+dbeGfvZQA0aoXMdAPmFD3mVD0ZqXrMKXqSEhPo7vWTqNPE/CygEEJEQlePn/auPlmfNYybLyDeOzeLRK2afSftnKl28fBi220t4FeUZqHRSSm8ELFOfovjRG1T/yL0eGztPhLXyw2NSVoeXZ7Ho8vz6OjyUdvkodbRSUtbD26Pjyv2Drpv6TSoUiDZoCU9WUuaUUdmeiKZaYmo1XI5UQghhmJv7W9dnpMha45GSqVSWDLbQoE1ld8dqqfy81pWzc9mhjUl0qEJISaYJFpxorbJQ5JeQ+I0vQI2VLmhLSMJ2037u/gDIbw9fjLNBk5fduHt8dPZ3Yfb46Ouub97lkqlkJWeSJEthfwsaVkthBB30ujs/5uZKzNao1ZgTaH8vgI+PWln30kHLW09LC+xoFbJRT4h4sX0fFceh+qaOqdt2eBoJWhUpBl1lBaa6Osb3EbeHwjR7O7G4ermaksXn51u4mBVMyUFJmbnpUYoYiGEiE4NrV4SdWrSjbpIhxKTkhITKFuZz7GLTqpq22ht7+GhxbbhDxRCxARJtOJAh7cPd6dPyg4mQIJGRW5mMrmZySwvsdDS1sPlxg6qat1U1bhocnXz1YeL5U2FEELQ39o9JyNZ1rmOg0qlsLwkk8z0RD4/3UTl53VkpCWyam52pEMTQoyTJFpxoMbeCUBG2vRcnzVZFEUhy2Qgy2Tg/kU5HDzj4NC5Zo5edPLUAzN4bHmulHgIIaatcDhMo7OL5SWZkQ4lLuRnGUk36vj0hIPX36+izuHha48Uo5H1wkLELEm04kC1owOVAuYUSbQmi9Gg5Z65WfyHNbP55UeX+MXey+w/7eCb60oGrf+6lbToFULEq/auPry9AXJl890JYzRoWXdvHldbvOw+fJUr9g6++5WFt3UlFELEBkm04kC1vRNbRpJc9ZoCxiQty+ZYsKQlcrCqmVf+zzGWzbFQkp92x9IZadErhIhXja39jTByhrjYJEZPrVKx4UszmVuQzs8+OMff/OsR/uzpxWSZDJEOTQgxSvLOPMaFwmFqHJ0UZMv6rKmiKAoF2UaevL8Qq9nA4XMtfHyskT5/cPiDhRAiTjQ6+1u726Tj4KRYWZrFn29aQo8vyN/861GuNHZEOiQhxChJohXjmt3d9PiCFFqlBflUS9RpeHRpDitKMmls9bLzi3o6vX2RDksIIaZEo9NLSpKWFIOUtU2WYlsq/+8fLcOg0/B3vzjBuVp3pEMSQoyCJFox7kpjfyOMgmxJtCJBURRKC9NZsyIPX1+QDw7UDewrI4QQ8ayxtUvKBqdAVrqBF7+xlIw0Pf9z+ylOXm6NdEhCiBGSRCvGXW5sx6DTSO12hGWbDDxxXwHJiQl8dLSRM9UuwuFwpMMSQohJEQqHaWz1SqI1RVKTdfy3TUvJsSTx438/zZHzLZEOSQgxApJoxbjLjZ3MzE1FJXuYRFxyYgLr7smnINvIsYut7D/loC8g67aEEPGn2d1Nnz9EXpZ0HJwqyYkJ/PkfLmGGNYV/eu8Mn59xRDokIcQwJNGKYV09fuytXopzUiMdirgmQaPioUVWlszKoMbh4dXtp+jo8kU6LCGEmFD1zf0l0gVZUrY+lQx6DX+2cREl+en8S+U5PjneGOmQhBBDkEQrhlXb+zsQzZJEK6ooisKCYjOPLLFhb/Xy8ttHqG/2RDosIYSYMHXNHjRqZch9BMXk0Gs1/JevLWRBsZm3P7zA7sNXIx2SEOIuJNGKYZcaOlApCjOs0to9GuVnGfm/Ny4mHIYfbTvGiUuygFkIER/qmz3kZCTL/o0Rok1Q86dfWcCyORZ+8dEldnxeG+mQhBB3IH8hY9jlhg7ys5LRadWRDkXcRV5mMn/1R8vJNht49Ven2HWwXppkCCFiWjgcpr65i3xZnxVRGrWKP6mYxz1zs/j1p9X8Yu8lunr9eH2Bu34EQpGOWojpRRPpAMTYBIIhahydPLTIFulQxDDSjTpe+PpS/qWyil9+fBmHy8s3y+bIlWAhRExq8/jo6vGTL+uzIk6tUrFp7Wzcnb3sPnSVq81dLC+xoNylQdaK0iw0OnnrJ8RUkXd6MepqSxd9gRAzc2V9VizQJaj5k/XzKb+vgH2nHPyPd07Q1eOPdFhCCDFqddfWnEojjOigUhTunZdFaUE65+ra+OJss1ROCBElJNGKUZcb+hthzJRGGDFDpSh85aFinikv5XJjBz98+wiNrd5IhyWEEKNS39yFAuRmSiOMaKEoCstLLCwoMnGpoYN9Jx0EQ1InKESkSaIVoy41dmBO0WFK0Uc6FDEERaXcViO/aJaF7351IT2+AC+/dZj9px1SQy+EiBn1zR6yTAb0WilBiyaKorBktoWlszOobfKw50gDfX7Zy1GISJK/kjEoHA5zpbGDWVI2GPV8/iAnLzrveFvZyjx+f8LOzz44x9yzTSydbUGlUqSGXggR1eqbPczMTYt0GOIu5heZMegT+Py0g10H63l0WS7JiQmRDkuIaUlmtGKQs72HNo+PWfJCF9MM+gTWrsxnTn4aVbVt/O7IVXp8gUiHJYQQd9XV48fV6ZOOg1GuyJbCo8ty8fYG2Hmgjpa27kiHJMS0JIlWDDpX1wZAaUF6hCMR46VWKdwzN4v7F2TT2t7LB5/XUePojHRYQsStmpoaNm7cSFlZGRs3bqS2tva2McFgkK1bt7J69WrWrFnD9u3bB277yU9+whNPPMGTTz7JV77yFfbt2zeF0Ufe9UYY0nEw+tkyknj83nwSNCp2H7rKpYb2SIckxLQj9Ukx6Hx9O6lJWqxmQ6RDEROkOCeVdKOOT47b+YdfnmTT6lk8siTnri16hRBjs2XLFjZt2kRFRQXvvfcemzdv5u233x40ZseOHdTX17N7927a29tZv349q1atIjc3l4ULF/Ltb3+bxMREzp8/zze+8Q3279+PXj891svWS8fBmJKarOPxVQV8esLOgTPNhMKw4Usz0SUMv/+mLkGDRi7HCzEukmjFmHA4zLm6NuYWpMub8DhjStHzxH0FnK5286+7L1Lt6OSba+egHcELohBieC6Xi6qqKt544w0AysvLefnll3G73ZhMpoFxO3fuZMOGDahUKkwmE6tXr2bXrl0888wzPPjggwPj5syZQzgcpr29nezs7Cl/PJFQ1+TBlKKTNT8xRJeg5rHluZy67OLQ2WbO17Xx8GIbacm6IY+T9cJCjJ/8BsUYu6ubTm8fJVI2GJd0CWr+U8U8Pjp8lfc/q6W+uYvvPDWfrHSZvRRivBwOB1lZWajV/Rcv1Go1mZmZOByOQYmWw+HAZruxGbzVaqWpqem2+/vNb35Dfn7+qJMss3ls65sslsjPItU2d1E6wzziWMLubozJI5vtS0jQDIwd7pibx47mfqNt7N2ONxh0WEwj+7s/0uf4wSW5PLA4hzcqq9h5oJ6Hl+ZQUmC66/jRxDAS0fDzO1XkscansTxWSbRizPlr67Mk0YpfKkVh/YNFFNlSeX3HWf76zcN8+/FSls3JjHRoQohrDh06xD/8wz/ws5/9bNTHulxdhEKj21DWYjHidHpGfa6J1Obx0eLu5tElOSOOpdsXwNPVO6Kxfn//WGOyfthjro8dzf1G29ihHmd3tw9ncGSt2UfzHC+abeGJVQXsO2nno8NXqbV3sLI0i4Q71AiOJobhRMPP71SRxxqfhnqsKpVy1wto466+lYXFU+t8XRvmFD2W1OmxHmA6W1hs5qU/Xkm2KYmf/PoMv/joEoGgbLIlxFhZrVaam5sJXnvzGAwGaWlpwWq13jbObrcPfO1wOAbNWh0/fpw///M/5yc/+QlFRUVTE3wUuNLYAcDMHNlaJJYZ9BrWrMhjQbGZK42d7DxQR5vHF+mwhIhL4060ri8s/vDDD9m0aRObN2++bczNC4vfeecdXn31VRoaGgBYuHAh7777Ljt27OBv//Zv+f73v09v78iuzEw3oXCY8/VtlMr6rGnDnKrnha8v5bGluew+fJX//m/H5QVRiDEym82UlpZSWVkJQGVlJaWlpYPKBgHWrVvH9u3bCYVCuN1u9uzZQ1lZGQCnTp3i+9//Pv/4j//IvHnzpvwxRNLlxg4SNCpp7R4HVCqFJbMyWL08l75AkA8O1HGhvp1weHQzrUKIoY0r0bq+sLi8vBzoX1hcVVWF2+0eNO5uC4sBHnzwQRITE4HBC4vF7RpauvD2BqSt+zSToFHx9bWz+ZOKeVxt6eKlNw5xttY9/IFCiNu89NJLbNu2jbKyMrZt28bWrVsBePbZZzl9+jQAFRUV5ObmsnbtWp5++mmef/558vLyANi6dSu9vb1s3ryZiooKKioquHDhQsQez1S60thBYbYRjVpa0cULW0YS5fcVkm1K5GBVM78/Ycfnn5hyQSHEONdoRcvC4uninKzPmtZWlmaRl5nMa78+w//4xQkqHpxB+X2FqGR2U4gRKy4uHlS+ft3rr78+8LlarR5IwG71q1/9atJii2b+QJDaJg9rV+RFOhQxwRJ1Gh5blktVbRvHLjpxfVbLQ4ttwx8ohBhW1DTDGM/C4rF2cLqbaO2gctneSY4lidlFGbfdNprOTteNpGPSrbePpsvSWI+ZinOM5Rhjsn5KHstQnZ4sFiN//2cZ/OTdk/xmXw31Ti//ddMyUpK0ozrHeEXr70gkyHNxgzwX8au2yUMwFJb1WXFKURTmzTCRlZ7IpycdfHiwHqNBS9mKPFmqIMQ4jCvRunlhsVqtHnZh8cKFC4HbZ7iuLyx+7bXXxrSweCwdnO4mWjuo+PqCnLzUypfu0u1pNF2HrhuuY9KdOiKNpsvSWI+ZinOM9pjrz8VUPJaRdHr65ppZ5FuS+Pmei3z37/byn9fPp9g2NW+AovV3JBLkubhhIp+LoTo4ici4fK0RRrEkWnEtIy2RJ+4rYP8pB7/ce5lGZ5fs5yjEOIyr0FoWFk+ds7VuAsEQi2eaIx2KiAKKovDIkhz+8pvLUCkK/9+2Y+w5clUWMgshJsXlhg4y0xOnfPZcTD1dgppHl+bw5Xvz+ex0Ez/adozW9p5IhyVETBr3ilZZWDw1TlxuJVGnYVZeWqRDEVGkMDuFLX+8gvkzTPx8zyX+1/tn6fEFIh2WECKOhMNhrjR2SNngFFJUCl5fYEQfE1TQM/j8isLjqwr53tcW0tLew9Y3D3OmxjXxJxIizo17jZYsLJ58oXCYU1dcLCgySbcncZskfQL/+SsL2Xmglh2f1VDb5OGZ8rlYM5KGPE6XoOEOe1QKIcQgzvYeOrv9kmhNIZ8/yMmLzhGNXTTbMikxKCqFWXlp/PmmJfzvHVX8z1+epOKBGTy6LPe2dVvyeiLEnUVNMwxxdzWOTjq9fSyaeXsTDCGgvyNYWrKWNcvz+PSknf/+82PcMzdryPUUK0qz0OjkT4AQYmgXrvZvuSKJ1vRyc7L38GIbn59p4jf7ajh5uZVV87MHXfiV1xMh7kyuP8SAk5dbUSkKC4pkfZYYWrbZQPl9hZhT9Hx2uokvzjYRDIYiHZYQIoadrXGTmqQlxzL0LLmIXwkaFQ8tsrJkVgY1Dg8fHqzH2+OPdFhCRD25/BADTlxyMTM3leTEhEiHImKAQa9hzYo8jl9q5WyNm9aOXh5ebMNokEXsQojRCYXCnK1xs3hmxkC5WCAEPv/I1oJOxvohERmKorCg2EyaUcf+kw4+OFDHw0tsZKXfeTsSIYQkWlGvtaOHBmcXT39pZqRDETFEpVJYNsdCZnoin51yUPl5HctLMpmZkyJ7ogghRqzG0Ym3N8CC4hsVFT5/gMPnmkd0/GStHxKRk5eZzJdX5fPxsUZ+d+gqK+dmsaI0K9JhCRGVpHQwyh272ArA4lmyPkuMXl5mMk/cV4DJqOPAmSb2HGmgq1vKPYQQI3O62oWiwNxC0/CDxbSRlqzj8VUFZJsNfHG2mV/uvURAytSFuI0kWlHuwJkmCrKNZJtkal6MjdGgZe3KPFbOzcTZ3sN7+2s4dbkVf0BeFIUQQztT46bImiKl6+I2ugQ1jy7LZd6MdPaddPB3/3acNo8v0mEJEVUk0YpiDc4u6po93Dc/O9KhiBinKAol+en8wQMzyM1M5sRlF3/z1hEOnWsmJJscCyHuoKvHT429k/nSiEnchUpRWDYnk299uYTaZg9bfnaIU1dkvy0hrpNEK4odONOEWqVwj9Q+iwmSnJjAw4ttrFmRizZBxT+/d5a/fvMwp660EpaESwhxk7M1bsLA/CIpGxRDW16SyZb/awVpyTr+fvtJ/m3PJXz+YKTDEiLipBlGlAqFwhw428SCIjMpSdItTkwsqzmJ8vtmcOaKi1/vq+bvt58iPzOZL99bwPISC2qVXIMRYro7Xe0iOTGBGdkpkQ5FxACrOYm/+qNl/PLjy/zuyFVOXmnlj79cwpz89EiHJkTESKIVpc7VtdHe1Sdlg9OQolLw+kbWOvm6sbRQVqkUVs3PZkVpJgfONrHrYD3/6/2zvPuJjocW2XhwkY20ZN3o71gIEfNC4TBnatzMm2FCpZJOpWJktAlqvrF2DsvmZPLmb8/xys+Pc/+CbJ59amGkQxMiIiTRilKfn3Fg0GlYNFNq46cbnz/IyYvOUR0znhbKGrWKBxfauH+BlROXWtl7rIFf76vh/c9qWTwzg0eW5FBamI5K2sILMW1cqG+n09vH4pnS8VYM79YLhPnZRv7bN5bx2wN1/P5EI4fP7+GxZXk8siSHdKMejRRNiGlCEq0o1N3r5+hFJ/fNyyZBo450OCJO3WnmbE5BOnMK0mlp6+Hz0w6+ONvE0YtOMlL1rJpvZensDMJqNd0jnHHTJWjkBVWIGPT5aQeJOjVLZGsRMQJ3u0CYY0niyfsLOXXFzW+/qON3h+t5aHEOX16ZjzlVH4FIhZhakmhFoY+PN9LnD/Hw4pxIhyLi2HAzZzmWJNY/OIO65i4uNbSz47MadnxWg9WcREF2MoXZRrQJQ18IWFGahUYnf2aEiCW9fQGOXHCysjRz2N9xIYZjNGhZt6qQOns7Z2vcfHKsgY+PNlBSkM6qedksnpUh2weIuCXvgKKMPxDkd0camDfDREG2MdLhiGlOrVZRZEuhyJZCvtXIB/trqW3y8MXZZg6dayEvM5liWwq2jCRZxyFEnDh6wYnPH+T+BdZIhyLiiClFz4OLbHzry6kcv+jkwJkmfrbzHIoCRdYU5s0wUWRLoSA7hdRrTcACIfD5pYJCxC5JtKLMZ2ea6PT28fg9+ZEORYhB0o16FhSbWbXQRq29nerGTmocHuqaPOi1amZY+xMyU4oORdZzCRGzPj/ThCVNz6zc1EiHIuKQOVVPxQMz+IP7C6l2dHL6ioszNW52fFbL9b5O6UYdhdlGbBlJ9PgCpBl1JOk1Q762SAWFiEbyExlFQqEwuw7WU5htpKRA2qGK6KQoChmpiWSkJrKsJBN7q5fqxg4u1Ldzrq6N1CQtBdlGCmVGVoiY4+ro5XxdG3/wwAy5YCImlaIoFNtSKbalsv7BInp8Aa62dFHr6KS22UOtw8OJS60DyZdWoyLdqCPNqMOcoiczPRGjIUF+TkVUk0Qrihy76KSlrYfvrJ8vfzhETFCrFPIyk8nLTMbnD1Ln8FDT1MmpKy5OXXFx6FwLK0szWVGSiS0jSX6uhYhyB842EQZWydYiYpIMtYVJTmYyOZnJ3H/t625fgI+PNtDm6aXN46PN4+PKtQt7AIk6NZnpBjLTE7FmJDE7J1VeZ0RUkUQrSgSCId77rIbM9ESWjqNVtxCRoktQMzs/jdn5afT4AtQ1eXB7fOz4rJb3P6vFnKJn3gwT82eYKC1MJ0kvi5+FiCb+QJCPjzdSkp9GZlpipMMRcWo0W5gsmm0hMz2RzPQbP4/hcJiOrj6a23poaeumua2HuiYPh8+1kG7UsWRWBktmW5iTl4ZGLYu2RGRJohUl9hxpoNHp5U+/skCaCoiYl6jTUFKQzorSLPz+IMcvOjlT4+bQuWY+PWnvX/xsS2F2XhozslMozDZiTtXLlUghIuiT43baPD6eKZ8b6VCEuCtFUUi7VkI4Jz8NgK4eP4n6BE5fbmXfKQd7jzWSqFMzb4aJRTMzmDvDhPam7XKkcYaYKpJoRQF3Zy/v7a9hYbFZ9iwRcSctWceXlubypaW5BIIhqu2dnK1xc7bWze5DVwmG+ivwk/QaCq0p5Gclk20ykG0ykGUyYEyUGnwhJpuvL8gHB2opLUinVNbAC06MAAAgAElEQVQIixiTnJjAotkW1ArMLzLhcHVT3+zh9BU3R8470aj7y9wLrSnYMgzcO88qjTPElJCfsijwbx9dIhQOs2nNbHlDKeKaRq1idl4as/PSeOqhIvyBEA3OLmqbPNQ1dVLr8AxKvgAMOg1ZJgNZpkQy0xLJulaPn2UyDNuFSggxMnuPNdDZ7edPHyyKdChCjItGrRpYOxwKhWlyd1Pb5KG+2UONw4NWo+JKYyf3zcumpCBdygvFpJJEK8KOX3Ry9IKTpx4qkpp4EXeGWvR8XabJQKbJwMq5WQCoVCqa3V5artXf9//bw4X6dg6ebSZ807GJOg2WND2Z6QbMqXosaf3JmDXDgF579z9vUjYixA09vgA7v6hjQZGZmdLSXcQRlUrBlpGELSOJe+Zm4XB5B7oZfnG2meTEBJbPsbCiNIs5eWmydENMOEm0IqjR2cXrlVXkZyWzbqXsmyXiz2gWPV+3aLaFWkcn0F8OkpyYQJEtBYBgMISnx4+n24+nuw9Pt59Obx819k6Onm8ZlIQZDQmkG3WYjDrSU/SYU3QYrjXgkP1WhLjh3d9fwdsbYP2DMyIdihCTRq1SyLUkk2tJZvEsC9WNHRw618znZ5v45ISd1CQty0syWVmaSXFOKiqplhATQN5pRIinu49/ePcUugQ13/vqQhLk8roQw1KrVaQl60hL1g36/qLZFo6db8Hb46fD20ebx4e7s78dcH1z18C4RJ0ac4qe1o5eZuemUpidQkqSdqofhhBR4/D5Fj4+1kjZyjxmWFMiHY4QUyJBo2LpbAtLZ1vw9QU5eaWVw+da+P0JOx8dbcCUomNFSSYrS7MozDZKiboYM0m0IsDXF+Qn/36aDm8f/23TUkwp+kiHJETMU6sUUpK0pCRpyctMHvi+PxCizdOLq9OHq6MXV0cvvz1Qx85rt5tTdBRmp1BoNVJo7e+AKK3nxXTQ3NbNGzvPUWxL4asPF0c6HCEiQqdVs7I0i5WlWfT4Apy41Mqhc83sOdLAh4eukm7UsaDIxPwZZuYWpg9URggxEpJoTbE2j49/ePckV1u6eO7JeQMlUUKIyZGgUV3b0NIw8L0FxWZa23qocXiobeqktsnD0ZtKHDPTEvsTr+wUZliN5GcZSZRSQxFHunv9/NNvzqBWKfyninnSEEAI+tf9rpqfzar52Xh7/Ry76OTEZReHzrXw6UkHKgUKrSmUFqRTnJtKQbbxtrbxQtxMfiKmUG1TJ//47il6+oL8l68tZGGxtHIXIhL0Wg1z8tOZk3+jjbW3109tk4daR3/idaWxg0PnWgBQgGyzYWDma4Y1ZdCsmRCxpLW9h79/9xTN7m7+9CsLyEiVRkxC3CpJn8CDC20snZPJwbNNODt6sDu92Fu9fHCgDgCVAqYU/cCmyo8tzyNbXhvETSTRmgJdPX7e21fDx8cbSTNq+ctvLBv2TVogBD7/0N3abhYKDz9GCNHvjt0QFaW/dPCmdSqe7j7qm7uob/bQ0OLlbI2bA2ebgP4X2OyMpP6W8yYDWenX/zWg0/Zf4ZTuhiLaXG7s4Mf/fppAIMR/3biYEtkzS4hhqVQKWen9f9+XXFvX5Wzv74jb0t7D+bp2qmrb+OS4HaNBS64libzMZPKzksnJSCbLlDhkJ1wRv8b9v15TU8MLL7xAe3s7aWlpvPLKKxQWFg4aEwwG+eEPf8i+fftQFIXnnnuODRs2DHtbrHO29/DZaQcfHW2g2xfgkSU5PPVgEcmJw9f3+vwBDp9rHvG5Fs22jCdUIaaV0XZDzEjV89iKPE5edNLdG6C1owdXpw9Pt58rjR2cvNxK+KaLHQa9htQkLTNzU7Gaksi41no+I00v678ibLq+ZrW0dfPrfTUcrGomI1XPX/yHJdgykiIdlhAxSadVk5uZTO61i+bBYAhXpw+jQUtHt5+LdW4+Pt6IPxAaOCbdqBt0QS4jVY/pWkdcY5JWuhzGqXEnWlu2bGHTpk1UVFTw3nvvsXnzZt5+++1BY3bs2EF9fT27d++mvb2d9evXs2rVKnJzc4e8Ldb4A0FqHB4uXG3nbI2bi1fbUehfD/K1h4sHfiGFELHLoNeQr+9ft2VM1uPp6iUYCg20mu/o6qPD20ent48j5530+ByDjk/UaUhL1pKWrCM1SUtqspbUJB1GQwIGnQaDXkPitX8NOg0JGjUatSJdrybIdHrN6uzu4+TlVo5fbOV0tQu1SuGJVQV8+Z58WdAvprWR7PF43UgqhtRqFZnpiaycl02iXkt3t49gKExLWzdNrm5ars1+Odt6OHrBSVePf9DxGrVCWrIOU4qedKOOdGN/8mVMTMBoSMCckki6UUuiTiMJWYwZV6LlcrmoqqrijTfeAKC8vJyXX34Zt9uNyWQaGLdz5042bNiASqXCZDKxevVqdu3axTPPPDPkbVMhFA7jbOshEAoTDoXp9AVxub2EQmFC4XD/v6EwwWuf9/YF8fb48fYG8Pb68fYEaO/y0eTuxtXZO3BVO8eSxFMPzuD+BVbpKihEnFOrbmo7n3Xj+ytKs1DCYZztvbR29NDa0UtrRy/tXT46uvq4Yu+go6uPvpuuet6JovQ39dBq1CRoVIM+dAlqdAn939eoVWhUCpr/n717j4uq2vsH/mGGAURALg4wKOKlo4wihaJoihdSsSMIYoaPZRfvecu0klMdb0WleawkLydTz+PT75w6ZmoCeczUg1iZqKWJ5g28wDAgAwpyGZhZvz/MyYkBBhiu83m/XujMXmuvvfZiz3xZe6+9tq0EthIJpFKbe8sM/5te9vs6NpBKJZBIbGADwMbGBjY297Z/b8lvr23up9/7xwZAuQAKCkpgY3PvCqBU0vLGTLaFmFVaXgl1QQm0FXpUVN770VbqUKbV4c5vHXx1QSlu5hWjoKgcwL2ZNR/r3xljQ7pUeTQCkTWqy6iGuowYKq/Q4fy1XBQVlxktd//tmY7+XVwBANoKHTp5OeP0hVwU//a35N3SChQUleFmXjFKyyphqn9nA8DBXgoHu3sn4hzspXCQSSGVSiAz+n7/w3e89N53u63EBpLffqQSG0hsqnlt8v29734hBPQCEELARVOKwsIS6MW9v6eFEBC/pQnDMuP3f1z2YHkCuLfd37b1YCySSH77/7e4dO//3/I+UD/j9AfKsblftnH6/QmzGkuDOloqlQpeXl6QSu/djyCVSuHp6QmVSmUUtFQqFXx8fAzvFQoFcnJyak0zV0Oe5H3o5E0c+PFGvdaVySRwtJfByUGGoJ5yyF0d4NPRCd0VLnB0aNjFQluppE5nHOua35x12tnbQldpnN4Y22lo/qZY535btIV9qe869/ObOi6as171WcdS26itLWylErSzk8LJ0Q7dqplhVAiBMq0eJeUVKCvXoeBuOX69VoAKnR4VFXrohIBOp4deL1B5/8SP7v5rPRzsbKHTC1RU6lFaXmlI0+n0hv91TXwT55BABaKG1P/htw35Tq9Ja49ZEokN/nnwIi7dvF1tHgc7KdycHRDS2wte7o7o2dkVPh0dLXpFtC6fn/rkNec7prHr0BR5a9rPllrn+uZ9cF9bQ30bktec49fRQYYuXs64W1JhMl2vF9BW6lBece8kipebI7QVOpSWV6KsQody7f2fSpRX6lGpE6jQ61FaIaCr1KPige9+nV4PnY438tfkqdE98fBDtU9QV913d03f6W3izjw3t/qPM48do0TsGKUFa2M5nRUd6pS/e+e639TcUtdpqfWqzzottV71Wael1qs+6zRVvepjeP8uTbIdah71jVkeHk6Iey7EwrWpn7rEp7p8bpi3ZdWDeVtOXmp+Hh51vwWoQWM7FAoF1Go1dDodgHs3Cefm5kKhUFTJl52dbXivUqng7e1daxoREZGlMGYREVFTalBHy8PDA0qlEomJiQCAxMREKJVKoyEYADB27Fjs3LkTer0eGo0GBw8eRHh4eK1pRERElsKYRURETclGCNGggZtXrlxBXFwc7ty5AxcXF6xevRrdu3fHzJkzsXDhQvTt2xc6nQ6rVq3CsWPHAAAzZ85EbGwsANSYRkREZEmMWURE1FQa3NEiIiIiIiIiYy1v/l0iIiIiIqJWjh0tIiIiIiIiC2NHi4iIiIiIyMLY0SIiIiIiIrIwdrSIiIiIiIgsjB2t32RkZCA2Nhbh4eGIjY1FZmZmc1epyaxevRphYWHo1asXLl68aFhujW1SUFCAmTNnIjw8HJGRkZg/fz40Gg0A4KeffsL48eMRHh6OadOmIT8/v5lr2/jmzp2L8ePHIzo6GlOmTMH58+cBWOexcd9HH31k9FmxxuMiLCwMY8eORVRUFKKionD06FEA1tkWja0tf9asJfZYW1yxxrhhDXHBmr73y8vLsXz5cowZMwaRkZH461//CqCex7AgIYQQU6dOFXv27BFCCLFnzx4xderUZq5R0zlx4oTIzs4WI0eOFL/++qthuTW2SUFBgfjhhx8M7999913xl7/8Reh0OjFq1Chx4sQJIYQQGzZsEHFxcc1VzSZz584dw+tvvvlGREdHCyGs89gQQohffvlFTJ8+3fBZsdbj4o/fFUIIq22LxtaWP2vWEnusLa5YW9ywlrhgTd/7b775poiPjxd6vV4IIUReXp4Qon7HMK9oAcjPz0d6ejoiIiIAABEREUhPTzeccWrrgoODoVAojJZZa5u4uroiJCTE8P6RRx5BdnY2fvnlF9jb2yM4OBgAMHnyZOzfv7+5qtlknJ2dDa+Li4thY2NjtceGVqvFqlWrsGLFCsMyaz0uTGFbWF5b/6xZS+yxtrhiTXHD2uNCW9zXu3fvYs+ePXjxxRdhY2MDAOjYsWO9j2HbRq9xK6BSqeDl5QWpVAoAkEql8PT0hEqlgru7ezPXrnmwTQC9Xo9//etfCAsLg0qlgo+PjyHN3d0der0ehYWFcHV1bcZaNr7XX38dx44dgxACn3zyidUeGx9++CHGjx+Pzp07G5ZZ83Hx8ssvQwiB/v37Y/HixVbdFo3FGj9rbX2frSWuWEvcsLa4YA3f+zdu3ICrqys++ugjHD9+HO3bt8eLL74IBweHeh3DvKJFVI0333wTjo6OePrpp5u7Ks0qPj4eR44cwUsvvYQ1a9Y0d3WaxenTp/HLL79gypQpzV2VFuH//b//h6+++gq7du2CEAKrVq1q7ioRtQrWElesIW5YW1ywlu99nU6HGzduoHfv3vjyyy/x8ssvY8GCBSgpKalXeexoAVAoFFCr1dDpdADuNXJubm6VIQ3WxNrbZPXq1bh27Ro++OADSCQSKBQKZGdnG9I1Gg0kEkmrPWNTH9HR0Th+/Di8vb2t7tg4ceIErly5gsceewxhYWHIycnB9OnTce3aNas8Lu7/ru3s7DBlyhScOnWKn5FGYI3fw215n60xrrTluGFtccFavvcVCgVsbW0NQwQffvhhuLm5wcHBoV7HMDtaADw8PKBUKpGYmAgASExMhFKpbNWXsxvKmttk3bp1+OWXX7BhwwbY2dkBAAICAlBWVoa0tDQAwGeffYaxY8c2ZzUb3d27d6FSqQzvDx06hA4dOljlsTFr1iykpqbi0KFDOHToELy9vbF161bMmDHD6o6LkpISFBUVAQCEEEhOToZSqbTKz0hjs8bPWlvdZ2uJK9YUN6wpLljT9767uztCQkJw7NgxAPdmGszPz0fXrl3rdQzbCCFEo9e6Fbhy5Qri4uJw584duLi4YPXq1ejevXtzV6tJvPXWWzhw4ABu3boFNzc3uLq6IikpySrb5NKlS4iIiEDXrl3h4OAAAOjcuTM2bNiAU6dOYfny5SgvL0enTp3w3nvvoWPHjs1c48Zz69YtzJ07F6WlpZBIJOjQoQOWLl2KPn36WOWx8aCwsDBs3rwZPXv2tLrj4saNG1iwYAF0Oh30ej169OiBN954A56enlbXFk2hLX/WrCX2WFNcsea40ZbjgrV979+4cQOvvfYaCgsLYWtri0WLFmH48OH1OobZ0SIiIiIiIrIwDh0kIiIiIiKyMHa0iIiIiIiILIwdLSIiIiIiIgtjR4uIiIiIiMjC2NEiIiIiIiKyMHa0iBrg+PHjGDZsWHNXg4iIqFaMWURNy7a5K0DUUgQFBRlel5aWws7ODlKpFACwcuVKjB8/vrmqVqPc3Fx88MEHSElJwd27d+Hl5YU///nPmDFjBhwdHRttuwkJCbh27RrWrl3baNsgIiLTGLPqhjGLmgM7WkS/OX36tOF1WFgY3nrrLTz66KPNWKPaFRYWYvLkyQgKCsJnn32Gzp07Q6VSYevWrbh+/Tr8/f2bu4pERNQIGLOIWj4OHSSqhVarRXx8PIYOHYqhQ4ciPj4eWq3WZN4dO3bgz3/+M3JycqDVarF69WqMGDECjz76KJYtW4aysjIAvw/f2LZtGwYPHoyhQ4di165dhnL++9//4s9//jOCgoIQGhqKrVu3mtze9u3b0b59e7z33nvo3LkzAEChUOCNN94wBKxTp05h4sSJ6N+/PyZOnIhTp04Z1g8LC8N3331neJ+QkICXX34ZAHDz5k306tULu3fvxogRIxASEoJNmzYBAFJSUvD3v/8dX3/9NYKCglrsmVMiImvDmMWYRS0HO1pEtdi0aRN+/vln7N27F1999RXOnj2LjRs3Vsn30UcfYffu3fj000/h7e2NtWvXIiMjA3v27MGBAweQm5uLDRs2GPLfunULRUVFSElJQXx8PFatWoXbt28DAF5//XWsWrUKp0+fRmJiIgYNGmSybt9//z1Gjx4NicT0R7mwsBCzZ8/G1KlTcfz4cTz//POYPXs2CgoKzN7/kydPYv/+/fjf//1fbNiwAVeuXMGwYcMwe/ZsPP744zh9+jS++uors8sjIqLGw5jFmEUtBztaRLXYt28f5s2bBw8PD7i7u2PevHlGX9JCCLzzzjs4duwYduzYAXd3dwgh8O9//xuvvfYaXF1d4eTkhNmzZyMpKcmwnq2tLebNmweZTIbhw4fD0dERGRkZhrTLly+juLgYHTp0QJ8+fUzWrbCwEHK5vNq6HzlyBH5+foiOjoatrS0iIiLQvXt3HD582Oz9nz9/PhwcHODv7w9/f39cuHDB7HWJiKhpMWYxZlHLwXu0iGqRm5sLHx8fw3sfHx/k5uYa3hcVFeHf//433n//fTg7OwMANBoNSktLERMTY8gnhIBerze8d3V1ha3t7x/Bdu3aoaSkBACwfv16bNq0CX/729/Qq1cvLFmyxOjG5wfLyMvLM7vu9+uvVqvN3X107NjRZB2JiKjlYcxizKKWg1e0iGrh6emJ7Oxsw3uVSgVPT0/DexcXF2zevBl/+ctfcPLkSQCAm5sbHBwckJSUhLS0NKSlpeHkyZNGNy/XJDAwEJs2bcJ3332HUaNGYdGiRSbzDR48GN98841RMKyp7vfr7+XlBeBeECotLTWk1RQA/8jGxsbsvERE1DQYs0xjzKLmwI4WUS3GjRuHTZs2QaPRQKPRYMOGDYiMjDTKExISgrVr12LBggU4c+YMJBIJJk2ahLfffhv5+fkAALVajaNHj9a6Pa1Wi6+++gpFRUWQyWRo3759tePZn3/+edy9exdLly5FVlaWYTvvvPMOLly4gOHDhyMzMxP79u1DZWUlkpOTcfnyZYwYMQIA4O/vj+TkZFRUVODs2bP4z3/+Y3a7eHh4ICsrq9qASURETY8xyzTGLGoO7GgR1WLu3LkICAjA+PHjMX78ePTp0wdz586tkm/IkCF4++23MWfOHJw7dw6vvPIK/Pz88OSTT6Jfv3547rnnDOPZa7N3716EhYWhX79++Oyzz/Dee++ZzOfq6op//etfsLW1xZNPPomgoCA8++yzcHZ2hp+fH9zc3LB582Zs374dISEh+OSTT7B582a4u7sDABYtWoTr169j4MCBSEhIqBKMazJ27FgA9wL2hAkTzF6PiIgaD2OWaYxZ1BxshBCiuStBRERERETUlvCKFhERERERkYWxo0VERERERGRh7GgRERERERFZGDtaREREREREFsaOFhERERERkYWxo0VERERERGRh7GgRERERERFZGDtaREREREREFsaOFhERERERkYWxo0VERERERGRh7GgRERERERFZGDtaREREREREFsaOFhERERERkYWxo0VERERERGRh7GgRERERERFZGDtaREREREREFsaOVisQFxeH999/3/D+n//8Jx599FEEBQWhoKCgGWtGDXH58mXExMRACFGv9cPCwvDdd99ZuFbUmixYsAD//e9/m7saRAaMV20T4xU1lLXGK3a0WpmKigq8++672LZtG06fPg03N7dq8/bq1QvXrl1r8DYTEhLw8ssvN7icuvpjwG5pPvjgA0RGRqJ3795ISEio8/offvghpk+fDhsbGwAMRK1JY3wmMjMz0bdvX6Nyf/jhB0RGRiI4OBghISGYN28e1Gq1IX3mzJn48MMPLVoPIkthvGoZ8vPzsXjxYgwdOhT9+/fH5MmT8fPPP9epDMar1qup4hUAaDQaLFmyBP3798eAAQOwZMkSQ5q1xit2tFqZ/Px8lJeX46GHHmruqlg9Pz8/vPzyyxg+fHid183NzcXx48cxatSoRqgZtUarVq1C3759jZY99NBD+OSTT5CWloajR4/Cz88Py5cvN6QHBgaiuLgYZ8+eberqEtWK8aplKCkpQd++ffHll1/ixx9/xIQJEzBr1izcvXvXrPUZr+iPTMUrAJg/fz46duyII0eO4LvvvsP06dMNadYar9jRaoHS09MxYcIEBAUFYdGiRSgvLwcAZGRkYOzYsQCAAQMG4Jlnnqm2jKeeegoAEBUVhaCgICQnJwMADh8+jKioKAQHB2Py5Mm4cOGCYZ2PP/4YoaGhCAoKQnh4OL7//nukpKTg73//O77++msEBQVh/PjxVbYlhMDbb7+NwYMHo1+/foiMjMTFixcBAFqtFqtXr8aIESPw6KOPYtmyZSgrKwMAHD9+HMOGDcO2bdswePBgDB06FLt27QIAfP7559i3bx+2bt2KoKAgzJkzBwCgVquxYMECDBo0CGFhYdixY4ehHgkJCXjxxRfx6quvIigoCOPGjTP6QKtUKsyfPx+DBg1CSEgIVq1aZUj74osv8Pjjj2PAgAGYPn06srKyav09TZgwAcOHD0f79u1rzftH3333HXr37g17e3sAwCuvvILs7GzMmTMHQUFB2LJlCwDg22+/xbhx4xAcHIypU6fiypUrJsu7cuUKwsLCkJiYCKDm33NYWBi2bt2KyMhI9O/f3+gY02g0mD17NoKDgzFw4EBMmTIFer2+xn0JCwvDJ598gsjISDzyyCN47bXXcOvWLcyYMQNBQUF47rnncPv2bUP+hQsXYsiQIejfvz+eeuopXLp0CcC9YyUqKgr/93//BwDQ6XSYPHkyPvrooxq3r9PpsHnzZowaNQpBQUGIiYmBSqUCAJw6dQoTJ05E//79MXHiRJw6dcqo3g+ekX3wrN/NmzfRq1cv7N69GyNGjEBISAg2bdoEAGZ9JuoqKSkJzs7OGDx4sNHyjh07wsvLy/BeKpXi+vXrRnkGDhxolcMxqGVgvGr58crX1xfPP/88PD09IZVKERsbi4qKCmRkZNS43n2MV4xXD6ouXqWmpiInJwevvvoqnJ2dIZPJ0Lt3b6M8VhmvBLUo5eXlYsSIEWL79u1Cq9WKr7/+WvTu3VusW7dOCCHEjRs3RM+ePUVFRUWtZfXs2VNkZmYa3p87d04MGjRI/PTTT6KyslJ8+eWXYuTIkaK8vFxcuXJFDBs2TOTk5Bi2c+3aNSGEEOvXrxdLliypdjspKSliwoQJ4vbt20Kv14vLly8LtVothBAiPj5ezJ49WxQUFIiioiIxe/ZssXbtWiGEED/88INQKpXigw8+EFqtVhw5ckQEBgaKwsJCIYQQS5cuNey3EELodDoxYcIEkZCQIMrLy8X169dFWFiYSElJMdQzICBAHDlyRFRWVoq1a9eKSZMmCSGEqKysFJGRkSI+Pl7cvXtXlJWViRMnTgghhPjmm2/EqFGjxOXLl0VFRYXYsGGDiI2NNeO3dc+SJUvE+vXrjZZlZWWJ/v37i6ysLJPrvPvuu2LFihVGy0aOHCmOHTtmeH/16lXx8MMPi9TUVKHVasXHH38sRo0aJcrLy43y//LLL2L48OHi0KFDQoiaf8/315s4caLIyckRBQUFYuzYseKf//ynEEKItWvXir/+9a9Cq9UKrVYrTpw4IfR6fY37P3LkSDFp0iSRl5cncnJyxKBBg0R0dLQ4d+6cKCsrE1OnThUJCQmG/Dt37hRFRUWivLxcvPXWW2L8+PGGtF9//VUEBweLy5cvi40bN4pJkyaJysrKGre/ZcsWERERIa5cuSL0er04f/680Gg0oqCgQAQHB4vdu3eLiooKsW/fPhEcHCw0Go3J9n7wOL//OXv99ddFaWmpOH/+vOjTp4+4fPlylbz3LV++XPTv39/kT0RERLX1LyoqEmPGjBEqlcpkufePpV69eonevXuLXbt2GaVv27ZNzJs3r8Y2ImoMjFetL14JIUR6eroICAgQd+7cEUIwXjFeWSZeJSQkiGnTpoklS5aIgQMHipiYGHH8+HGj9a0xXvGKVgvz888/o6KiAs8++yxkMhnGjh1r8vJsfXz++eeIjY3Fww8/DKlUigkTJkAmk+Gnn36CVCqFVqvFlStXUFFRgc6dO6NLly5mlWtra4u7d+/i6tWrEEKgR48e8PT0hBAC//73v/Haa6/B1dUVTk5OmD17NpKSkozWnTdvHmQyGYYPHw5HR8dqz7KdPXsWGo0G8+fPh52dHXx9ffHkk08azn4CQP/+/TF8+HBIpVJERUUZzoydOXMGubm5ePXVV+Ho6Ah7e3sEBwcDAD777DPMmjULPXr0gK2tLebMmYPz58+bdVWrOj4+PkhLS4OPj4/J9MyA700AACAASURBVKKiolqvhCUnJ2P48OEYMmQIZDIZpk+fjrKyMpw+fdqQJy0tDS+88AJWr16NkSNHAqj593zf1KlT4eXlBVdXV4wcORLnz58HcO/3kZeXh+zsbMhkMgQHBxvG5Nfk6aefNlx9CQ4ORmBgoOEM6OjRo5Genm7I+8QTT8DJyQl2dnZYsGABLly4gKKiIgBAz5498cILL2Du3LnYtm0b1qxZA6lUWuO2d+7ciRdffBHdu3eHjY0N/P394ebmhiNHjsDPzw/R0dGwtbVFREQEunfvjsOHD9e6P/fNnz8fDg4O8Pf3h7+/v9GZ1j9asWIF0tLSTP7s27ev2vU++OADTJw4Ed7e3ibT7x9LP/zwg2E/H9S+fXvcuXPH7H0ishTGq9YXr4qLi/Hqq69i/vz5cHZ2BsB4xXhlmXilVquRmpqKkJAQpKamYtq0aZg7dy40Go0hjzXGK9vmrgAZy83NhZeXl9GXRXVffnWVnZ2NPXv24NNPPzUsq6ioQG5uLgYOHIjXXnsNCQkJuHz5MoYOHYq4uDijYUvVGTx4MJ566imsWrUKWVlZGDNmDJYuXYry8nKUlpYiJibGkFcIYXRp39XVFba2vx+G7dq1Q0lJicntZGVlITc31xBwgHuX4R9837FjR8NrBwcHlJeXo7KyEiqVCj4+PkbberBd3n77baxevdqonmq1Gp06dap1/+vDxcWl1vHxubm5Rr97iUQChUJhNBnCZ599hgEDBiAkJMSwrKbf831yudzwul27doa06dOn46OPPsK0adMAALGxsZg1a1at+/Ngu9vb21f5Pdz/nep0Orz//vvYv38/NBoNJJJ753oKCgoMQT86Ohrvv/8+xowZg65du9a67ZycHJN/ZP2x/YB7n6UH268u+1XTsVlf58+fx/fff4/du3fXmtfV1RUTJkxAVFQUUlJSDMfy3bt34eLiYtF6EZmD8ap1xauysjLMmTMHDz/8MGbPnl1j3gcxXjFeAbXHK3t7e3Tq1AmTJk0CAIwbNw6bN2/GqVOnDPf3WWO8YkerhZHL5VCr1RBCGIJXdnY2fH19G1y2QqHAnDlz8MILL5hMj4yMRGRkJIqLi7Fs2TKsXbsW7733nllniJ555hk888wzyM/Px6JFi/DJJ59g4cKFcHBwQFJSklkB8I/+uF2FQoHOnTvjwIEDdS5LoVBApVKhsrKySvC63y6WGLtsrl69emHPnj015vH09DTcOwDcC6YqlcqoLVeuXIktW7bg7bffxmuvvQag9t9zTZycnBAXF4e4uDhcvHgRzz77LPr27VtlLHZ97du3D99++y22b9+Ozp07o6ioCAMGDDCaMnjlypUYOXIkUlNTkZaWZvSHiSne3t64fv06evbsabTc09MT2dnZRstUKhVCQ0MB3AtEpaWlhrS8vDyz98PUZ2LZsmXVngn08fExOjN+3/Hjx5GVlWU4u1tSUgKdTocJEyaYDGY6nQ75+fkoLi6Gq6srgHv3O/j7+5tddyJLYbz6XUuPV1qtFvPmzYOXl5fR/V7mYLxivAJqj1e9evWq9QqcNcYrDh1sYR555BHY2tpix44dqKiowIEDB+o9Q0vHjh1x48YNw/tJkybhs88+w88//wwhBEpKSnDkyBEUFxfj6tWr+P7776HVamFnZwd7e3vD2RsPDw9kZWVVe5PpmTNnDENI2rVrBzs7O0gkEkgkEkyaNAlvv/028vPzAdy7tHz06FGz6u/h4YGbN28a3gcGBqJ9+/b4+OOPUVZWBp1Oh4sXL+LMmTO1lhUYGAi5XI6//e1vKCkpQXl5OU6ePAkAmDx5Mj7++GPDTa5FRUX4+uuvay2zoqIC5eXlEEKgsrIS5eXl0Ol0Zu3bkCFDkJ6ebripF6j6+3r88cfx3//+F99//z0qKiqwbds22NnZISgoyJCnffv2hlnp1q5dC6Dm33NtDh8+jGvXrkEIAWdnZ0ilUrP+cDHX3bt3YWdnBzc3N5SWlmLdunVG6Xv27MG5c+fwzjvv4I033kBcXFytZ1InTZqEDz/8EJmZmRBC4MKFCygoKMDw4cORmZmJffv2obKyEsnJybh8+TJGjBgBAPD390dycjIqKipw9uxZ/Oc//zF7P0x9JlatWoXTp0+b/DEVtIB7Z2C/+eYb7NmzB3v27MHkyZMxYsQIbN26FQBw4MABXL16FXq9HhqNBu+88w569+5t6GQBwIkTJzBs2DCz605kKYxXv2vJ8aqiogILFy6Evb09Vq9ebWgrczFeMV4Btcer0aNH486dO9i9ezd0Oh32798PtVqNfv36GcqwxnjFjlYLY2dnh4SEBOzevRsDBw5EcnIyRo8eXa+y5s+fj7i4OAQHByM5ORl9+/bFm2++iVWrVmHAgAEYM2YMvvzySwD3znb97W9/Q0hICIYOHQqNRoPFixcDgGHmqJCQEEyYMAHAvbMhy5YtA3Dvy+iNN97AwIEDMXLkSLi6uhqm9HzllVfg5+eHJ598Ev369cNzzz1n9kxHTzzxBC5fvozg4GDMnTsXUqkUmzdvxoULF/DYY49h0KBBeOONN8z6Qr6/7rVr1zBy5EgMGzbMEJxGjx6NGTNmYPHixejXrx8iIiKQkpJSa5l//etfERgYiMTERGzevBmBgYHYu3cvgHtndYOCgqqcobqvY8eOCAkJwbfffmtYNmvWLGzatAnBwcHYunUrunfvjvfeew9vvvkmBg0ahMOHD2Pz5s2ws7MzKsvFxQXbtm1DSkoKPvjggxp/z7W5du0ann/+eQQFBSE2Nhb/8z//g0GDBpm1rjmio6Ph4+OD0NBQjBs3Do888oghLTs7G++88w5Wr16N9u3bIzIyEgEBAXjnnXdqLPP555/H448/jmnTpqFfv354/fXXUV5eDjc3N2zevBnbt29HSEgIPvnkE2zevBnu7u4AgEWLFuH69esYOHAgEhISEBkZafZ+mPpM1Ee7du0gl8sNP46OjrCzszPUUa1WY8aMGYbZ0SQSidGsVmfOnIGjoyMCAwPrXQei+mK8+l1LjlenT5/G4cOHcezYMQwYMABBQUEICgpCWloaAMar6jBeGastXrm6umLTpk3Ytm0bgoOD8fHHH2Pjxo2GdGuNVzZC1PMx30TUIJcvX8bSpUvxxRdfWPQsHFmPBQsW4IknnqjXs9yIiMzFeEUNZa3xih0tIiIiIiIiC+NkGK1YWloaZs6caTLtwSlVqX7YvvdkZ2dj3LhxJtOSkpIsNstYTWbMmGG4R+FBs2fPNjwclIhaLn6fNi627z2MV9TS8IoWERERERGRhXEyDCIialMyMjIQGxuL8PBwxMbGIjMzs0qe1NRUxMTEICAgwOiZRADw6quvIioqyvDj7+9vmAggISEBgwcPNqStXLmyKXaJiIhaoTZxRaug4C70+up3w8PDCfn5tc/0Y+3YTuZjW5mH7WQ+ttXvJBIbuLm1r/f6zzzzDCZOnIioqCjs3bsXu3btwo4dO4zyXLt2DSUlJdi/fz+0Wi2WLl1qsqwLFy7g2WefxdGjRw2z7JWUlFSb3xyMWfXDdjGN7WIa28U0totpDWmXmmJWm7hHS68XNQat+3modmwn87GtzMN2Mh/bquHy8/ORnp6O7du3AwAiIiLw5ptvQqPRGKYZBgA/Pz8AwMGDB6HVaqst74svvkBkZGSVaaobgjGr/tguprFdTGO7mMZ2Ma0x2oVDB4mIqM1QqVTw8vKCVCoFcO+ZRJ6enlCpVHUuS6vVYt++fZg4caLR8qSkJERGRmLatGlWNdEAERHVTZu4okVERGRpBw8ehI+PD5RKpWHZ5MmTMWfOHMhkMhw7dgxz585FcnIy3NzczC7Xw8Op1jxyuXO96tzWsV1MY7uYxnYxje1iWmO0CztaRETUZigUCqjVauh0OkilUuh0OuTm5kKhUNS5rF27dlW5miWXyw2vhwwZAoVCgUuXLmHgwIFml5ufX1zjEBW53Bl5eUV1rm9bx3Yxje1iGtvFNLaLaQ1pF4nEptoTaBw6SEREbYaHhweUSiUSExMBAImJiVAqlUb3Z5kjJycHJ0+eRGRkpNFytVpteH3+/HlkZWWhW7duDa84ERG1ObyiRUREbcqKFSsQFxeHjRs3wsXFxTB9+8yZM7Fw4UL07dsXaWlpWLx4MYqLiyGEQFJSEuLj4xEaGgoA2L17N0aOHIkOHToYlb1u3TqcO3cOEokEMpkMa9asMbrKRUREdF+bmN6dwzAsg+1kPraVedhO5mNb/a6mYRhtAWNW/bBdTGO7mMZ2MY3tYlpjDR3kFS0yKCrR4m55ZY157GW2sOWAUyIiamK1xSjGJyJqadjRIoPSskqcOK+uMc8ApRds7XnYEBFR06otRjE+EVFLw3M/REREREREFsaOFhERERERkYWxo0VERERERGRh7GgRERERERFZmFkdrYyMDMTGxiI8PByxsbHIzMyskic1NRUxMTEICAgwPLPkvldffRVRUVGGH39/f3z77bcAgISEBAwePNiQtnLlyobvFRERERERUTMya3qe5cuXY8qUKYiKisLevXuxbNky7NixwyiPr68v4uPjsX//fmi1WqO0NWvWGF5fuHABzz77rOGhkAAQHR2NpUuXNmQ/iIiIiIiIWoxar2jl5+cjPT0dERERAICIiAikp6dDo9EY5fPz84NSqYStbc19ty+++AKRkZGws7NrQLWJiIiIiIharlqvaKlUKnh5eUEqlQIApFIpPD09oVKp4O7uXqeNabVa7Nu3D//4xz+MliclJSE1NRVyuRwLFixAUFBQncqt7mnMD5LLnetUpjXK1ZTA2cmhxjyOjvaQuzs2UY1aNh5T5mE7mY9tRURE1HY06ZP9Dh48CB8fHyiVSsOyyZMnY86cOZDJZDh27Bjmzp2L5ORkuLm5mV1ufn4x9HpRbbpc7oy8vKIG1d0qSKUoKi6rMUtJSTnydLomqlDLxWPKPGwn87GtfieR2Jh1Ao2IiKglq3XooEKhgFqthu63P651Oh1yc3OhUCjqvLFdu3Zh4sSJRsvkcjlkMhkAYMiQIVAoFLh06VKdyyYiIiIiImopau1oeXh4QKlUIjExEQCQmJgIpVJZ52GDOTk5OHnyJCIjI42Wq9Vqw+vz588jKysL3bp1q1PZRERERERELYlZQwdXrFiBuLg4bNy4ES4uLobp22fOnImFCxeib9++SEtLw+LFi1FcXAwhBJKSkhAfH2+YXXD37t0YOXIkOnToYFT2unXrcO7cOUgkEshkMqxZswZyudzCu0lERERERNR0zOpo9ejRAzt37qyyfMuWLYbXwcHBSElJqbaMF154weTyPz5zi4iIiIiIqLUz64HFREREREREZD52tIiIiIiIiCyMHS0iIiIiIiILY0eLiIjalIyMDMTGxiI8PByxsbHIzMyskic1NRUxMTEICAiocq9wQkICBg8ejKioKERFRWHlypWGtNLSUixatAijR4/G2LFjcfjw4cbeHSIiaqWa9IHFREREjW358uWYMmUKoqKisHfvXixbtgw7duwwyuPr64v4+Hjs378fWq22ShnR0dFYunRpleVbt26Fk5MTvvnmG2RmZuKpp57CgQMH0L59+0bbHyIiap14RYuIiNqM/Px8pKenIyIiAgAQERGB9PR0aDQao3x+fn5QKpWwta3b+cavv/4asbGxAICuXbsiICCgxhl3iYjIerGjRUREbYZKpYKXlxekUikAQCqVwtPTEyqVqk7lJCUlITIyEtOmTcPp06cNy7Ozs9GpUyfDe4VCgZycHMtUnoiI2hQOHSQiInrA5MmTMWfOHMhkMhw7dgxz585FcnIy3NzcLFK+h4dTrXnkcmeLbKstydWUwNnJodp0R0d7yN0dm7BGLQePF9PYLqaxXUxrjHZhR4uIiNoMhUIBtVoNnU4HqVQKnU6H3NxcKBQKs8uQy+WG10OGDIFCocClS5cwcOBA+Pj4ICsrC+7u7gDuXUELCQmpUx3z84uh14satu+MvLyiOpVpFaRSFBWXVZtcUlKOPJ2uCSvUMvB4MY3tYhrbxbSGtItEYlPtCTQOHSQiojbDw8MDSqUSiYmJAIDExEQolUpDx8gcarXa8Pr8+fPIyspCt27dAABjx47F559/DgDIzMzE2bNnERoaasE9ICKitoJXtIiIqE1ZsWIF4uLisHHjRri4uBimb585cyYWLlyIvn37Ii0tDYsXL0ZxcTGEEEhKSkJ8fDxCQ0Oxbt06nDt3DhKJBDKZDGvWrDFc5Zo+fTri4uIwevRoSCQSrFq1Ck5OtQ8FJCIi68OOFhERtSk9evTAzp07qyzfsmWL4XVwcHC1swX+8blaD3J0dMT69esbXkkiImrzOHSQiIiIiIjIwtjRIiIiIiIisjB2tIiIiIiIiCyMHS0iIiIiIiILM6ujlZGRgdjYWISHhyM2NhaZmZlV8qSmpiImJgYBAQFVbiROSEjA4MGDERUVhaioKKxcudKQVlpaikWLFmH06NEYO3YsDh8+3LA9IiIiIiIiamZmzTq4fPlyTJkyBVFRUdi7dy+WLVuGHTt2GOXx9fVFfHw89u/fD61WW6WM6OhoLF26tMryrVu3wsnJCd988w0yMzPx1FNP4cCBA2jfvn09d4mIiIiIiKh51XpFKz8/H+np6YiIiAAAREREID09HRqNxiifn58flEolbG3rNmP8119/jdjYWABA165dERAQUO2Uu0RERERERK1Brb0ilUoFLy8vSKVSAIBUKoWnpydUKhXc3d3N3lBSUhJSU1Mhl8uxYMECBAUFAQCys7PRqVMnQz6FQoGcnJw67YSHR+0Pi5TLnetUpjXK1ZTA2cmhxjyOjvaQuzs2UY1aNh5T5mE7mY9tRURE1HY0yQOLJ0+ejDlz5kAmk+HYsWOYO3cukpOT4ebmZpHy8/OLodeLatPlcmfk5RVZZFttmlSKouKyGrOUlJQjT6drogq1XDymzMN2Mh/b6ncSiY1ZJ9CIiIhaslqHDioUCqjVauh+++Nap9MhNzcXCoXC7I3I5XLIZDIAwJAhQ6BQKHDp0iUAgI+PD7Kysgx5VSoVvL2967QTRERERERELUmtHS0PDw8olUokJiYCABITE6FUKus0bFCtVhtenz9/HllZWejWrRsAYOzYsfj8888BAJmZmTh79ixCQ0PrtBNEREREREQtiVlDB1esWIG4uDhs3LgRLi4uhunbZ86ciYULF6Jv375IS0vD4sWLUVxcDCEEkpKSEB8fj9DQUKxbtw7nzp2DRCKBTCbDmjVrIJfLAQDTp09HXFwcRo8eDYlEglWrVsHJiUNGiIiIiIio9TKro9WjRw/s3LmzyvItW7YYXgcHB1c7W+Afn6v1IEdHR6xfv96cahAREREREbUKZj2wmIiIiIiIiMzHjhYREREREZGFsaNFRERERERkYexoERERERERWRg7WkRERERERBbGjhYREREREZGFsaNFRERtSkZGBmJjYxEeHo7Y2FhkZmZWyZOamoqYmBgEBARUeQTJhg0bMG7cOERGRiImJgZHjx41pMXFxWHYsGGIiopCVFQUNm3a1Ni7Q0RErZRZz9EiIiJqLZYvX44pU6YgKioKe/fuxbJly7Bjxw6jPL6+voiPj8f+/fuh1WqN0gIDAzFt2jS0a9cOFy5cwNNPP43U1FQ4ODgAAGbNmoWnn366yfaHiIhaJ17RIiKiNiM/Px/p6emIiIgAAERERCA9PR0ajcYon5+fH5RKJWxtq55vDA0NRbt27QAAvXr1ghAChYWFjV95IiJqU3hFi4iI2gyVSgUvLy9IpVIAgFQqhaenJ1QqFdzd3etc3p49e9ClSxd4e3sblm3fvh2ff/45fH19sWTJEvTo0aNOZXp4ONWaRy53rnNd27pcTQmcnRyqTXd0tIfc3bEJa9Ry8Hgxje1iGtvFtMZoF3a0iIiITPjxxx/x4YcfYtu2bYZlL730EuRyOSQSCfbs2YMZM2bg4MGDho6dOfLzi6HXi2rT5XJn5OUVNajubZJUiqLismqTS0rKkafTNWGFWgYeL6axXUxju5jWkHaRSGyqPYHGoYNERNRmKBQKqNVq6H77g1un0yE3NxcKhaJO5Zw+fRqvvPIKNmzYgO7duxuWe3l5QSK5Fzqjo6NRUlKCnJwcy+0AERG1GexoERFRm+Hh4QGlUonExEQAQGJiIpRKZZ2GDZ45cwYvvfQS1q9fjz59+hilqdVqw+ujR49CIpHAy8vLMpW3YpV64G55ZY0/5RXWd7WKiFo3Dh0kIqI2ZcWKFYiLi8PGjRvh4uJimL595syZWLhwIfr27Yu0tDQsXrwYxcXFEEIgKSkJ8fHxCA0NxcqVK1FWVoZly5YZylyzZg169eqFpUuXIj8/HzY2NnBycsKmTZtMTqhBdVNeUYkT59U15gnuU7erkkREzY3RgYiI2pQePXpg586dVZZv2bLF8Do4OBgpKSkm19+1a1e1Zf/jH/9ocP2IiMg6cOggERERERGRhZnV0crIyEBsbCzCw8MRGxuLzMzMKnlSU1MRExODgIAAwzCN+zZs2IBx48YhMjISMTExOHr0qCEtLi4Ow4YNQ1RUFKKiorBp06aG7REREREREVEzM2vo4PLlyzFlyhRERUVh7969WLZsGXbs2GGUx9fXF/Hx8di/fz+0Wq1RWmBgIKZNm4Z27drhwoULePrpp5GamgoHh3vPw5g1axaefvppC+0SERERERFR86r1ilZ+fj7S09MREREBAIiIiEB6ejo0Go1RPj8/PyiVSpM3BYeGhqJdu3YAgF69ekEIgcLCQkvUn4iIiIiIqMWptaOlUqng5eVleBijVCqFp6cnVCpVvTa4Z88edOnSBd7e3oZl27dvR2RkJObOnYsrV67Uq1wiIiIiIqKWoklnHfzxxx/x4YcfYtu2bYZlL730EuRyOSQSCfbs2YMZM2bg4MGDho6dOap7GvOD5HLnetXZmuRqSuDs5FBjHkdHe8jdHZuoRi0bjynzsJ3Mx7YiIiJqO2rtaCkUCqjVauh0OkilUuh0OuTm5kKhqNvzLE6fPo1XXnkFGzduRPfu3Q3LH3zQY3R0NN555x3k5OSgU6dOZpedn18MvV5Umy6XOyMvr6hO9bVKUimKistqzFJSUo48HR8ayWPKPGwn87GtfieR2Jh1Ao2IiKglq3XooIeHB5RKJRITEwEAiYmJUCqVcHd3N3sjZ86cwUsvvYT169ejT58+Rmlq9e8PKDx69CgkEolR54uIiIiIiKi1MWvo4IoVKxAXF4eNGzfCxcXFMH37zJkzsXDhQvTt2xdpaWlYvHgxiouLIYRAUlIS4uPjERoaipUrV6KsrAzLli0zlLlmzRr06tULS5cuRX5+PmxsbODk5IRNmzaZnFCDiIiIrINeL5D2ay5uF2vh7uKAjh0cEOTPk7BE1LqY1aPp0aMHdu7cWWX5li1bDK+Dg4ORkpJicv1du3ZVW/Y//vEPc6pAREREVkAIge9+ycHV7DtwdbLD+UwN9AIo0erg79sBNjY2zV1FIiKz8NIRERERtQhCCJy4kIur2XfwyEMeCHyoI3R6gbNX8nEiXY3KCh369vBo7moSEZmFHS0iIiJqES7dvI0L1wqh9HMzdKikEhs8/JAHZDIpTl7IhUt7O/h5c4ZOImr5ap0Mg4iIiKixCSFwLkODjh0cEOwvNxoiaGNjg/8Z0wsdOzjg2FkVSsoqmrGmRETmYUeLiIiImp0qvwRFJRXw93MzeR+WnUyK0IcV0OsFfr6c3ww1JCKqG3a0iIiIqNldvFEIe5kUft7VP0PN2dEOPX1dcTnrNm4Xa5uwdkREdceOFhERETWrwuJy3MgtxkOdO0AqqflPk749PCCV2OCnS3lNVDsiovphR4uIiIia1XdnVRAC6Onboda87ext0burO66pi3HrdmkT1I6IqH7Y0SIiIqJmU6nT47uzOfDp2B7OjnZmrdOnmzsc7KT46dKtRq4dEVH9saNFRERtSkZGBmJjYxEeHo7Y2FhkZmZWyZOamoqYmBgEBARg9erVRmk6nQ4rV67EqFGjMHr0aOzcudOsNKqfizcKcfuu1qyrWffJbCVQ+rkh+1YJCorKG7F2RET1x44WERG1KcuXL8eUKVPwn//8B1OmTMGyZcuq5PH19UV8fDymT59eJW3fvn24fv06Dhw4gM8//xwJCQm4efNmrWlUP+cyNZBKbKDwaF+n9Xr6usJWaoPzmQWNVDMiooZhR8vKlZRVYv/x69j3XSa+/j4T13KKIIRo7moREdVLfn4+0tPTERERAQCIiIhAeno6NBqNUT4/Pz8olUrY2tpWKSM5ORmTJk2CRCKBu7s7Ro0ahf3799eaRvWTnlGAbgoXyGzr9ieJvZ0UPTp1wNXsOygtr2yk2hER1V/VCENW40r2bfx97zncul1mtFzh4YjBAd5waidrppoREdWPSqWCl5cXpFIpAEAqlcLT0xMqlQru7u5ml+Hj42N4r1AokJOTU2uauTw8qp++/D653LlOZbZWt4vLcU1dhOjhPeDs5FBr/j/mCVZ649frhcjIKcZYR3vI3R0bq6otmrUcL3XFdjGN7WJaY7QLO1pW6mDaDXx+6DJcnezx2tT+6OrtjEpIsCPpHNJ+zcW+1Ew82tcbft78MBIRWVJ+fjH0+upHDsjlzsjLK2rCGjWf4+lqAEAPhTPyCmufQbCo2PjEoNQG6OzphLOXb6HwTglsdLpGqWdLZk3HS12wXUxju5jWkHaRSGyqPYHGoYNW6OKNQvzz4CX07e6BldMG4KFOHWArlcDeToqeXVwxfkg3uDrb4ejP2VBrSpq7ukREZlMoFFCr1dD99ge3TqdDbm4uFApFncrIzs42vFepVPD29q41jeruXKYGjva26OJV/5N6fbq6obxChxPpuRasGRFRw7GjZWUqKnX43/0X0LGDA2aP7wNHh6rDA50cZQjr9dHyCQAAIABJREFU3xlO7WQ4cjobRSXaZqgpEVHdeXh4QKlUIjExEQCQmJgIpVJp9rBBABg7dix27twJvV4PjUaDgwcPIjw8vNY0qhshBNIzNVD6uUEisal3OZ5u7eDmbI+Un7N5jzERtSjsaFmBSj1wt7wSd8sr8eXRDKjyS/DkYw+hUgjD8rvllSiv+H3Ihb1MirD+nSEgcOhkFrQV1jccg4hapxUrVuDTTz9FeHg4Pv30U6xcuRIAMHPmTJw9exYAkJaWhmHDhmH79u347LPPMGzYMBw9ehQAEBUVhc6dO2PMmDF48sknMW/ePPj6+taaRnWToymB5k45enczvxNsio2NDXp1cUX2rbu4dPO2hWpHRNRwZt2jlZGRgbi4OBQWFsLV1RWrV69G165djfKkpqZi3bp1uHjxIqZOnYqlS5ca0nQ6Hd566y0cPXoUNjY2mDVrFiZNmlRrGllGeUUlTpxXo6CoHAd+vI7uPi4oLqnAifNqo3zBfYyH1ri0t8OIRzrhmxM3cOpiHgb14fAYImr5evToYfL5Vlu2bDG8Dg4ORkpKisn1pVKpoXNWlzSqm/TfpmXv09WtwWV1U7jg58v5OHTqJnr6uja4PCIiSzDrihafSdI2nL6YBztbKYL9Pc1ex9vDEf5+brh44zZyC2q/UZmIiMgc5zI06NjBAZ5uDZ8pUGYrwaA+Xjj5ax4Ki/kAYyJqGWrtaPGZJG1DUYkWN/PuomcXVzjYSeu07iN/6ghHB1v8cC4HlTp9I9WQiIishV4IXLxRiN4WuJp139BAH+j0Av/9Kbv2zERETaDWjlZNzyQxV2M/k4Rqd/FGIWxsgJ6+Heq8rsxWgpDeXigs1uLQSV5tJCKihlFrSlBSXokePnWPSdXxdGuHgO7uOPJTFk8KElGL0Caeo8WHP9asXF2Ey1l30N2nA7w71twO1T0wsreTA66ri7H/+HXEhPVER9d2jVHVVsWaj6m6YDuZj21F1uJq9h0AQDcfF4uWG9avM9Z/cQanL93CgDoMkyciagy1drQefCaJVCpt0DNJAgMDARhfxaopzVx8+GPNUn+6iXKtDj18XKo87PGPakp/uIcHMlV3sP2rX/Dc4/6WrmarYu3HlLnYTuZjW/2upoc/UtuQoboDezspfDzaW7TcwO4e6NjBAd+evMmOFhE1u1qHDvKZJK2bEAIpP2Wjg5MdvNwbdhXKyVGGoYEKpJ5RIYcPMiYionq6mn0H3bydG/T8LFMkEhuM7NcJF28U4mZusUXLJiKqK7NmHeQzSVqvzJwi3MgtRq8urrCxaXhAGzOwC2S2EuxOuWqB2hERkbWpqNThRm6xxYcN3hca6AOZrQSHTvGeYiJqXmbdo8VnkrReJy7kQiKxQXeFZQKaS3s7jB7gi8TvMvHnnCL4efOeEiIiMt91dTF0eoHuCstNhPEgp3YyhCi98P05NZ4Y8RAcHdrE7ehE1AqZdUWLWichBE79modevq6wk9VtSveajB3YBe0dbLH7KK9qERFR3VxV3ZsIo3sjXdECgLD+nVBeocOxX8yfIZmIyNLY0WrDbubdRW5hKR5+qKNFy3V0sMWYgV1w5ko+rqt58z4REZkvI/sO3Jzt4eZs32jb6Ortgu4+Ljh0Kgt6Uf1kWUREjYkdrTbs5K+5sAHQt4eHxct+rF8nONhJkfzDNYuXTUREbddV1R2LDWevyWP9OkOtKcH5zIJG3xYRkSnsaLVhpy7m4U++rnBpb2fxsh0dZAjr1xknLuRCzRkIiYjIDMWlFcgtKG20iTAeFOzvCWdHGSfFIKJmw45WG6XWlOBm3l307ylvtG2MHuALW6kEXx/nVS0iIqpdxv37s5rgipbMVoJhD/vgp8u3cOt2aaNvj4joj9jRaqNOXcwDAPRrxI5Wh/Z2CA1U4NjZHGju1PwgZCIioqvZd2ADNNmMtSMe6QQAOHI6u0m2R0T0IHa02qiTF/PQ1dsZHh0cGnU7Y0O6QAjg4EkOzSAioppdVxfB28MR7eybZsp1jw4OeOShjkj5ORsVlbom2SYR0X3saLVBd0q0uJp9B0F/suxsg6Z07NAO/XvJkfJTNsq0lY2+PSIiar2uq4vQxatpn78Y1r8ziksr8OP53CbdLhERO1pt0P0Zlvp0s/xsg6aMGeCLkvJKHDub0yTbIyKi1qe4tAL5d8rRxdOpSbfb288N3u6OOHQqq0m3S0TEjlYbdC5Tg/YOtujaRGPge3TqgO4+LjiYdoPPKyEiIpNu5BYDQJNf0bKxsUFYv07IUN0xTMZBRNQU2NFqY4QQSM/UwN/PDRKJTZNtd3SwL9QFpThzJb/JtklEZEpGRgZiY2MRHh6O2NhYZGZmVsmj0+mwcuVKjBo1CqNHj8bOnTsNaa+++iqioqIMP/7+/vj2228BAAkJCRg8eLAhbeXKlU21W63ejd8ecO/bxFe0AODRAAXsZVIc4v3ERNSEmuZuVGoyOZoSaO6UI2Kwe5Nut38vOdyc7fHNiRt45KHGvzeMiKg6y5cvx5QpUxAVFYW9e/di2bJl2LFjh1Geffv24fr16zhw4AAKCwsRHR2NwYMHo3PnzlizZo0h34ULF/Dss88iNDTUsCw6OhpLly5tsv1pK67nFsPVya5Rnu1YG0cHWzwa4I2jZ1R4MuwhODs2fR2IyPrwilYbk/7b/Vm9uzVtR8tWKsFj/Tvj/LUC3PxteAgRUVPLz89Heno6IiIiAAARERFIT0+HRqMxypecnIxJkyZBIpHA3d0do0aNwv79+6uU98UXXyAyMhJ2dvzDvKGuq4ubfNjgg8L6dUKlTo+jZ1TNVgcisi7saLUx5zI0kLs6wNO1XZNve9jDPrCzleBA2o0m3zYREQCoVCp4eXlBKpUCAKRSKTw9PaFSqark8/HxMbxXKBTIyTGe0Eer1WLfvn2YOHGi0fKkpCRERkZi2rRpOH36dCPtSdtSUamHKv9uswwbvK+T3An+XVxx+FQW9HreT0xEjY9DB9uQSp0eF64XYFBvr2bZvlM7GR7tq0DqGRWeGN6jWYaHEBFZysGDB+Hj4wOlUmlYNnnyZMyZMwcymQzHjh3D3LlzkZycDDc3N7PL9fCovbMhlzfflZ/GcPlmIXR6gYA/yU3um9CUwNmp9uc+1pTH0dEecnfHGtePHvEnvLvjBK7dKsHAPt61V7yVaGvHi6WwXUxju5jWGO3CjlYbkqG6gzKtDr27Nu2wwQeNDu6MI6ezcOSnLIwf0q3Z6kFE1kmhUECtVkOn00EqlUKn0yE3NxcKhaJKvuzsbAQGBgKoeoULAHbt2lXlapZcLje8HjJkCBQKBS5duoSBAweaXcf8/9/efUfHVd6J/3/f6SNp1Eddlmy5CbkXjGkuuAVsy4EYs16ymwQMCQ78wiY5mA2HTrImWVKIMXsS4l2+yQbCEmwwxhiH4ga4G/eqrlHv0mjq/f0xkmzZKmMsaWakz+scnWnP3PnM1Z155vPcp1Q39XhGxWq1UFnZ6Pf2QsFXp8oBiDbpunxvLQ43jU2tvW6npzItLQ4qPT0vSjwiMZzoCAN//+QswxPCe329UDAYj5e+IPula7JfunYt+0WjUbptQPOr66DM4BQajufVoCiQnel/y2pfS44LZ/yIOD4+WILL7Q1YHEKIoSkuLo7s7Gw2b94MwObNm8nOziY2tnMD1KJFi3jrrbfwer3U1NSwfft2Fi5c2PF4WVkZBw4cYMmSJZ2eV15e3nH95MmTlJSUMHy4NCr1prCiCaNBizWm/7q1KxqFZoe7xz8UDbMnp3I8r4aympZ+i0UIIcDPM1oyg1NoOFVYR0aihXCTPqBxzJ+exktvHmHvyXJuGp/c+xOEEKIPPf3006xZs4ZXXnmFyMhI1q5dC8CqVat45JFHGD9+PLm5uRw5coQFCxYAsHr1atLT0zu28c477zBnzhyioqI6bfull17i+PHjaDQa9Ho9L774YqezXKJrReWNpCdEoFH6b9kRh8vDkTOVPZaZnp3IrIkpvLc7n08OlvBP80b1WzxCCNFrotU+g9OGDRsA3wxOzz33HDU1NZ1aCLubwen+++/vtD2Zwal/uNweLpQ2cNvU1ECHQk5mLCnx4Xy0v4gbxyWh9GPFKoQQl8vKyurUq6LdH/7wh47rWq22xx4UP/jBD7q8vz1pE/7zqiqFFU3cOC44xkRFRRiZOsbKrqM27rx1BEaDNtAhCSEGqV67DsoMTqEhz9aI2+NldFp0oENBURTmTUujsLyJs8X1gQ5HCCFEAFXV2Wl1egI6tfvlbpuaht3h5vMTZb0XFkKIr2lAJ8OQGZz6zydHfInvDZPSrpjtz9/ZnKDnGZ3Av1mdAJbMGsnfP7vAjqM2bpqS3mv5UDTYj6m+IvvJf7KvxGBUWO5bWzGQU7tfbmRqFMMSIvj4QDGzJqZIzwshRL/oNdGSGZxCw6FT5aRaw3G0OKhscXR6zN/ZnKDnGZ3Av1md2t0yIZmtews5ea6C+KiBX9erPw2FY6ovyH7yn+yri3qawUmEnsKKJjSKQmp88MzypygKt01NY8MHpzhZUBvQ2XqFEINXr10HZQan4OfxejlbUs/o9P7vNujPrE7tkw3OnZKGgsLHB0v6PS4hhBDBqai8keS4MAz64BoLdUNOIpFherbtKwp0KEKIQcqvroMyg1NwKyxvwuH0MGYAEi1/Z3XSGXXERZmYMjqeHYdLyb1puAw4FkKIIaiwoomxwwI/fvhyep2WOVPS2LQrD1t1M8lxwXPGTQgxOPiVaMkMTsHtTFEdAKOCYCKMy82bls7+05V8fryM2ZMDPyOiEEKIgdPY4qS20UF6QnCOP5wzOZX3Py/go/3F/MvCMYEORwgxyPi1YLEIbmeK6kiINhNjMQY6lCuMSosiI9HC9gPFqGr34+iEEEIMPoUVvokwEmLNPXY572GYdb+KDDcwMyeRPUdtNNldgQlCCDFoDeisg6LveVWVs8X1TBoZH+hQutQ+1ftr75/kRH4tOcNlwLEQQgwVRW0zDlbUttDQ7Oy23MTRgRsyMH96Oju/svHpoRIW35gZsDiEEIOPnNEKccWVzTTZXWQkW4KupbDd9dm+Accf7ZcBx0IIMZQUVjQSHWHAZAjedt00awQ5w2P5x8Fi3B5voMMRQgwiwfvNJ/xyIr8GgJZWF/tOlndZJpAthQB6nYbZk1N5d3c+5TUtJPqxDpcQQojQV1TeRJo1+KfqXzA9nV//7Qh7T5Zz47jk3p8ghBB+kDNaIe5cST1hRh0RZn2gQ+nRnMmpaDUK2w8UBzoUIYQQA8Dp8mCrbiEtiBYq7s644bEkx4WxbV+RjCcWQvQZSbRCmKqqnC+uJyHWHPSr2kdFGLk+O4FdR220tLoDHY4QQoh+VlLVjFdVSQ2BM1qKorBgejqF5U0dM/kKIcS1kkQrhFXWt1Lf7CQxxhzoUPyyYPowHE4PO46UBjoUIYQQ/aywvBGANGtorE81MyeJCLMsYCyE6DuSaIWwM4W+VrfEmNAY85SRZGHssGg+2l8kA46FEGKQK6xowmzUEhtlCnQofjHotcyenMrhs1WU1bQEOhwhxCAgiVYIO1NUR7hJR1SEIdCh+G3RjGHUNjrY283EHUIIIQaHovIm0q0RaIK8a/ulbpuahlarYeuXBYEORQgxCEiiFcLOFNWRlRoV9OOzLjV+RBwp8eFs/VIGHAshxGDl9aoUVTSRnmgJdChXJSrcwC0Tktl9tIzaRkegwxFChDhJtEJUbaODijo7WWlRgQ7lqiiKwsLr0ymubOJEfm2gwxFCCNEPymtbcLg8ZIRYogW+nheqCh/uLezycbeXbtetbP9zS+94IQSyjlbIap8VaWRqFOUh1pf8huuS+PuOC3zwZQE5w2MDHY4QQog+VlDmmwgjMyn0Ei1rtJnrr0vgs8OlLL4x84rlUxwud7frVrabnp2Izig/sYQY6uSMVog6U1yH0aANiWlzL6fXaVgwLZ0T+bVcKG0IdDhCiEEmLy+PFStWsHDhQlasWEF+fv4VZTweD8888wzz5s1j/vz5vPXWWx2Pvfzyy8ycOZPc3Fxyc3N55plnOh6z2+386Ec/Yv78+SxatIhPPvlkIN5SyMkva0Sv05AcHxqTNV3u9hsycLg8/EPWfhRCXANpbglRZ4rqGJUahVYTOuOzLjV7cipbvihg8558HvnWhECHI4QYRJ566ilWrlxJbm4umzZt4sknn+T111/vVOa9996jsLCQbdu2UVdXx7Jly5g5cyZpaWkALFu2jMcee+yKbb/22mtERETw0UcfkZ+fzz//8z+zbds2wsNDYwrzgVJY3kiaNQKtRgMETz86RaPQ7Oh5LUejXkeaNYJJI+PZvr+IBdPTMcvZKSHE1yBntEJQk91FSWUzo9OjAx1Kl9orsp7+9Hod86elc/hcFUUVTYEOWQgxSFRXV3PixAkWL14MwOLFizlx4gQ1NTWdym3ZsoXly5ej0WiIjY1l3rx5bN26tdftf/DBB6xYsQKAzMxMxo0bx44dO/r+jYQwr6pSUN5IRhB2G3S4POw7Wd7jn8PlS8SW3JRJc6ub7XJWSwjxNUkTTQg62zY+K1gTLYfLw5EzlT2WmZ6dyG3T0ti6t5D3P8/n+7njBiY4IcSgZrPZSExMRKvVAqDVaklISMBmsxEbG9upXEpKSsft5ORkysrKOm6///777Nq1C6vVysMPP8zkyZMBKC0tJTU1tdvnCaiqs2N3eEJyfNalhidHMjErjm17C7ltShphJvnJJIS4OvKtEYJOF9Wh02oYnhyJM4QX/g036Zk7JY0Pvigg9+ZmkuM6d71xe+loWeyOUa9DJ+dlhRB96J577uH73/8+er2e3bt389BDD7FlyxZiYmL6ZPtxcb2PrbVaQzdJOVXiG3s7cWwiVqsFtaYFS0TPixbr9bpeywA9lvFnG/6UCQszYo31jS37zpJxPPqbz/j8VAX3zB8D4Nf7uXQbAyGUj5f+JPula7JfutYf+8WvRCsvL481a9ZQV1dHdHQ0a9euJTMzs1MZj8fD888/z86dO1EUhQceeIDly5cDvoHF//u//0tCQgIAU6ZM4amnngJ8A4sff/xxjh8/jlar5bHHHmPOnDl9+BYHnzNFdWSlRKLXaUI60QJYMD2d7QeKeG93Pg8szen0mMzsJIS4WsnJyZSXl+PxeNBqtXg8HioqKkhOTr6iXGlpKRMm+MaIXnqGy2q1dpS76aabSE5O5uzZs1x//fWkpKRQUlLScXbMZrMxY8aMq4qxuroJr7f7dQStVguVlY1Xtc1gcvRsBVqNQphWobKykRaHm8am1h6f43L1XgbosYw/2/CnTEuLg0qPB4Aok5ZJI+N555NzzBxrJcyk9+v9XLqN/hbqx0t/kf3SNdkvXbuW/aLRKN02oPl1LqB9YPGHH37IypUrefLJJ68oc+nA4jfffJOXX36Z4uKL/ZqXLVvGpk2b2LRpU0eSBZ0HFr/66qs88cQTNDc3X+17HDLsDjeF5U2MCtJug1crMtzAbVPT+PJEOcWVMlZLCHFt4uLiyM7OZvPmzQBs3ryZ7OzsTt0GARYtWsRbb72F1+ulpqaG7du3s3DhQgDKyy828Jw8eZKSkhKGDx/e8bw333wTgPz8fI4ePcott9wyEG8tZBSW+SbC0A+S7ga5Nw+nxeFm276iQIcihAgxvX4LysDi4HK+tB6vqjJmkCRaAN+YkYHJqOWdHRcCHYoQYhB4+umn+fOf/8zChQv585//3DE9+6pVqzh69CgAubm5pKWlsWDBAu6++25Wr15Neno6AC+99BKLFy9m6dKlPPHEE7z44osdZ7nuu+8+GhoamD9/Pg8++CDPPvssERGht8xGf1FVlfyyRjKSBs8+yUiyMHWMlQ/3FlHf7Ax0OEKIENJrn6tQGFg82Pu7X6p4fzEajcKMiamYjbo+7fsOPfd/93dbV9sH3grcOWcUf9l6ilq7m9HDfOMggrEf/KUGyzHV32Q/+U/2Vd/IysrqtC5Wuz/84Q8d17Vabaf1sS61du3abrcdFhbG7373u2sPcpCqbmiludVNRuLgOpbvmpXF4bNVvLsrjztnZwU6HCFEiBiQwS39PbB4sPd3v9ThU+VkJFpoarDTBH3a9x167v/u77autg88wI3ZCWz67DyvbTrKT+7xJeHB1g/+UoPpmOpPsp/8J/vqop76u4vgVlDm6wKekRQZ4Ej6VlJsGLdOSuGzQ6XcNCG59ycIIQR+dB28dGAx0OvA4nY2m42kpCTAN7BYr9cDnQcWAx0Di7t6nujM5fZwwdYwqLoNtjMbdSyemcGJ/FqO59X0/gQhhBBBp6C8AY2ikGYdfAs4L71pOHq9hvd25wc6FCFEiOg10ZKBxcEjz9aI26MG7fpZ12rOlDSs0Sbe+MdZPN7Qnk1RCCGGorzSBlKt4Rj02kCH0ueiwg184/phHDlXRXlNS6DDEUKEAL+6Dj799NOsWbOGV155hcjIyI7+66tWreKRRx5h/Pjx5ObmcuTIERYsWABwxcDi48ePo9Fo0Ov1VwwsXrNmDfPnz0ej0cjA4h6cbluoeGRaVIAj6R96nYa754xk3TvH2HG4lOtz5MymEEKECq+qcsHWwIzrQvu7W9EoNDu6XsPx5kkpfHqklL0nK7hjZgYajTLA0QkhQolfiZYMLA4OpwtrSbOGE2HWBzqUfjNltJWxw6J5Z2ce47LiAh2OEEIIP5VWNWN3eMhKCe3xWQ6XhyNnKrt9PPeW4fzPllOcLqwjO7NvxpoLIQanwbHIxRDgcns4W1zP2IzB/aWuKAr33DaKZruLD74oDHQ4Qggh/HS+pB6AkamDs9dFu/FZcSTHhXH4XBX2bs58CSEESKIVMs6VNOBye7kuM7b3wiFuWKKFWyamsONwCTUN/s2WKIQQIrDOlzQQYdaTEGMOdCj9SlEUrs9OxOPxcuB092e+hBBCEq0QcSK/Bo2iDMoZB7vyrdlZhJv1fH6sHK/a/dT9QgghgsP50npGpESiKIN/3FJUhIGc4bFcKG2gpLIp0OEIIYKUJFoh4mRBLcNTLJiNA7L0Wb9rH2zc3Z+iUbhzVhbVDa2cKqgNdLhCCCF60GR3YatuIWuQdxu81ISsOKIiDOw5Vo7DNfDrOQohgt/g+NU+yLW0usmzNbB4ZmagQ+kzvQ02BpgwKp5UaziHz1YxLNEyqCcBEUKIUHahtAGAkSE+EcbV0Go13Dw+mS1fFLDvZAU3y0LGQojLyBmtEHC6sBZVheuG2OxGiqIw47pEAPYcLUOVLoRCCBGUzpfUoygwfAglWgBxUSbGj4jjQmkDheWNgQ5HCBFkJNEKAScKajHoNIxIGTpdMtpFmPVMH5tAWU0Lx/OlC6EQQgSj86X1pFkjMBmGXkeZ8VlxxEYa2XO0jMYWZ6DDEUIEEUm0QsDJglpGpUej1w3Nf9fItCiGJUZw+Ewl1fUyC6EQQgQTr1flQmnDkBqfdSmtRmHWpBQAPj1UitvjDXBEQohgMTR/uYcItxdKq5sprWpmZFpUl5NGeIdAbzpFUZiZk4TJoGPnVzZcbqnEhBAiWJRWNdPqDP2Fiq+FJczAzROSqW108OWJcunqLoQAJNEKag6Xmw++KADA7fay72T5FX9u79BIOowGLTdPSKax2cmeozapxIQQIkicLqoDYNQQWX6kO2kJEUzIiuN8SQOfHCwJdDhCiCAgiVaQK65sxmzUEhtpDHQoAZcUF8aUMVYKyps4eqEm0OEIIYTAt85jfJSJhOjBvVCxPyaMjCMjycI7Oy6w+6gt0OEIIQJMEq0g5vZ4Ka1qJtUaMSQWgPTHdZkxjEiJ5PDZKpnhSQghAszj9XKqsJbrMmMDHUpQ0CgKN09IYsywaDZsOcWhsz0vYyKEGNwk0Qpi50vqcbm9pFnDAx1K0FAUhRtyEomLNLHrKxsXSuoDHZIQQgxZ+bZG7A7PkFt+pCdajYZVS3LISLKwfuMx9p+qCHRIQogAkUQriB27UINGo5AcJ4nWpXRaDXOnphJm1LF+4zHyyxoCHZIQQgxJx/NrUIDsDEm0LmU0aHn07olkJkeyfuMx/nGgONAhCSECQBKtIKWqKscuVJMcFzZkp3XvidmoY/70dMJMOv7zjcMUVTQFOiQhRJDIy8tjxYoVLFy4kBUrVpCfn39FGY/HwzPPPMO8efOYP38+b731Vsdj69at44477mDJkiXceeed7Ny5s+OxNWvWcOutt5Kbm0tubi7r168fiLcUtE7k1zIs0YIlzBDoUIJOhFnPT1ZMYtKoeP7y0Rne+MfZPp/63e2FipqWLmclbv+TiXqFCJyht7JgiCitbqGqvpUZ1yUEOpSgFW7W8/BdE/jd/33Ff/zlIKu/OS4oxgm4vb4ZI3ti1OuQ/FmI/vHUU0+xcuVKcnNz2bRpE08++SSvv/56pzLvvfcehYWFbNu2jbq6OpYtW8bMmTNJS0tjwoQJfO9738NsNnPq1Cnuvfdedu3ahclkAuCBBx7g3nvvDcRbCyqtTjfnS+qZNy2dZkf333lDYRmS7hj0Wh765jje2H6ObfuKOF9Sz4O5OcRH9c3EIQ6Xm1MXqmls6n6NyenZieiM8nNPiEDw65OXl5fHmjVrqKurIzo6mrVr15KZmdmpjMfj4fnnn2fnzp0oisIDDzzA8uXLAV/r4JYtW9BoNOj1eh599FFuueUWwNc6uGfPHmJifN0OFi1axA9+8IM+fIuh6atzVQCkWSMCHElwi4828/i9U/jtW1/x678d4V8WjeGWCSkBjcnhcrPvZHmPZaTiE6J/VFdXc+LECTZs2ADA4sWLee6556ipqSE29mJDzJYtW1i+fDkajYbY2FjmzZvH1q1buf/++zvqJ4AxY8agqip1dXUkJSUN+PsJZmeK6vF4VUamRfX4nTdxtHUAowo+Wo2Gf14wmlHpUfz3B6d4+k/7WDl/FDNzkmSiKyHEf7psAAAgAElEQVQGOb9+6Unr4MA7fK6KNGs44WZ9oEMJevFRZh6/dyrrNx5lw5ZTnC2u5565owgzSSIjxFBjs9lITExEq9UCoNVqSUhIwGazdUq0bDYbKSkXG2WSk5MpKyu7YnsbN25k2LBhnZKsDRs28Oabb5Kens6Pf/xjsrKyrirGuLjeG9CsVstVbTMQ8vYUoNdpGDfSyrEL1d2W0+t1WCJMPW7LnzJAj2X66nV6K+PPNsLCjFhjwzrdd4fVwpTrknnpfw/wx80n2Xuqkoe+NZHUa2hQVWtagJ73S1exDBWh8DkKBNkvXeuP/dLrL1FpHRx4Dc1OzpXUs/D6YYEOJWSEmXT8f8snsmlXHlu+KOBEfg3fWTSWcSPiAh2aECJE7d27l9/+9rf86U9/6rjv0UcfxWq1otFo2LhxI/fffz/bt2/vSOz8UV3dhLeH/nRWq4XKyuBfvuLAyTJGpkbhdrl77Lrm6uVxf8sAA/I6vZXxZxstLQ4qPZ4r7tcBP7lnEp8dLuX/Pj3P6hc/Zs6UVBbfmEnk1xjn1tLWZbOneLqLZbALlc/RQJP90rVr2S8ajdJtA1qviZa0Dg68z0+dR1XhxomplLe1VnWnL1sKoedWsb58vb4qc3lL3fe/NYk504fx678e4qW/HSFnRBz/tGAME0bG93kXje6OKbWm5Wu1dg5WofTZCzTZV9cuOTmZ8vJyPB4PWq0Wj8dDRUUFycnJV5QrLS1lwoQJwJV12KFDh/jpT3/KK6+8wogRIzruT0xM7Li+bNkyfvGLX1BWVkZqamo/v7PgUllnp7iymeVzpMH0amkUhTmTU5kyKp53dubxjwPF7PrKxoLp6cyblk6E9GQRYtAY0L5V0jron4++KGBYYgQx4XrOFfZ/69ylBqI1sC/LdNVSFxum58l/ncpnh0vZ8kUBT7y6hzRrONPGJjBtTALJcWHXnHT1dEy1OL5+a+dgE2qfvUCSfXVRT62DvYmLiyM7O5vNmzeTm5vL5s2byc7O7tQwCL7xwG+99RYLFiygrq6O7du385e//AWAr776ikcffZTf/e535OTkdHpeeXl5R7K1c+dONBpNp+RrqNjXtjbU9DEyYdPXFRVh5DvfGMvC69P5+2cXeHd3Ph/uLWLWpBTmTUvrswkzhBCB02uiJa2DA8tW3Ux+WSMr5o4MdCghTa/TMm9aOrMmpbDraBmfHy9j4848Nu7MI8KsJyPJQro1ghiLkRiLEUuYnjCTnjCjjjCTDpNBK4OUhQhRTz/9NGvWrOGVV14hMjKStWvXArBq1SoeeeQRxo8fT25uLkeOHGHBggUArF69mvT0dACeeeYZWltbefLJJzu2+eKLLzJmzBgee+wxqqurURSFiIgI1q9fj0439MaD7j1ZzoiUSOKjzT3OOCh6lxwXzuo7x1Nc2cQHXxSwfX8xH+0vYmJWPLMnp5AzPBatRqapFSIU9Vo7SOvgwPr8eDmKAjOuG7r7oC/pdVrmTE5lzuRUahsdHD5XRb6tgYKyRrYXFuH2dH0mVKMohJl0HYlXVLgBa7QZa7SZtIQIzOHGAX4nQgh/ZWVldVoXq90f/vCHjutarZZnnnmmy+e//fbb3W77v//7v685vlBXXtNCYXkT90iDYJ9Ks0awakkOd83K4tPDJew4XMrhc1VEmPVMH5vAlDFWRqdFodf53+NHCBFYfjXDSevgwPCqKl8cL+O6zFiiI4zSStjHYixG5kxOhcm+s6WqqtJkd1Hb6KDJ7qKl1U2Lw01Lq5vmVhctDjf2VjfNrW5qGh2cKqrD4bzY3S8h2szo9GiyM2LIzowhOkKSLyHE4Le3bSr3aWOl2+C16G7NRaNRx8IZGdw2LZ0zhfXsP1XO7qM2PjlUgl6nYWRqFCNSIkm1hhMbaaLV6UZVVemFIUQQ8iujkdbBgXGuuJ6q+laW3TI80KEMCYqiYAkzYPFzpidVVWlocVFU0UhVo5OjZys5dLaSXUdtAIxIiWTyqHiZ6VAIMajtPVXByLQoYiP9m2RpKFI0Sq+NpV4VDpzqfc3F6WOttDrdnC6s42RBLScLatn6ZSGeS8amazUKRoMWvU6DXqtB13ap12m4UNpAhFmPxawnLsqENdpMSlw4RoOcGROivw3NU0dB6vPjZRj0GqYM8cUdA6G7lsXLmU0GhqdEkRNmZPoYK16vSklVMyfyavjqfDVvf3aBtz+7QGKMmazUKDKSLOh10rdeCDE4lFQ1U1LZzMp5owIdSlBzuDwcOVPZY5mrWcjZZNAxcWQ8E0fGA+D2eLFVt5BX1sDZ4gZqG+w4XB5cbi9ujxeX20urw43bo1JRa6fV6cHt8XZsT6MopFnDyUqLYmJWPNdlxqDTSl0lRF+TRCtINNldfH68jOljEzAZ5N8y0BwuN/tO9tyyCL6K8ciZSiwRpk4zC8ZGGpk9OYVmuwu708OuI6XsOVbG3pPlDEu0MCotioQYs3TtEEKEtH0ny1GQboOBptNqSE+IIDbKRHiYsceZbqdnJxJu1GF3uKmqb6Wyzk5+WSMXSuvZc6yMTw6WYDbqmDI6njmT0xiREjmA70SIwU1+0QeJjw8W43R5ZZHiEBdu1nPjxBTiIo1U1bVyrqS+rUJrIDbSSHZGDJnJUokJIUKP2+PlsyOlXDc8VsakhpBLuzHGRpmIjTIxJiMGAJfby+nCWo6er2b/6Up2Hy0jM8nC/OnpXJ+dILMdCnGNJNEKAk6Xh38cKGZCVhxp1q+3dsxQ5U8/eL1Oh8vde1/5vqQoCtYYM9YYM9OzE7hQ2sDJ/Fp2Hy3j4JlK6pocLJiW7vf4MCGECLQvT5RT3+TkvjvSAx2KuAr+dGP8p/mj+ef5o9lzrIyPDxbzh/dO8M6OC9x+QwY3jU+SmQ6F+Jok0QoCu4+V0dji4hsz5GzW1fK3H3xf9pW/WjqthtHp0YxKi6K0qoWTBTW8v6eAbXuLmJmTyPxp6aRKgi2ECGKqqrJ1byFp1nByMmN7f4LoE701JvZlI6HZqOO2qWnMmZLKkbNVbP68gNc/PM2mXXksvH4YsyalYDbKz0YhroZ8YgLM61X58MtChidHMjo9OtDhiH6kKAqp1nBSreGkJUSw64iNz4+XseOIjZzMGOZMSWPiyLg+76rhz0QfRr0OmbNDCNGdY3k1lFQ2c98d2TLWdAD11pjYV42Elyd0ozNieHRYNGeK6ti2t4i/fXKO9z/P57apacyblk6EWd8nryvEYCeJVoDtP11BRZ2dh2ZnSeU1hCTHhfOdb4zlrlkj+PRwKZ8eKuH3fz9KjMXIrEkp3Doxpc/GQPgz0cf07ER00lIphOjG1i8LibEYmXFdYqBDEf2gp4TuhpxEltyUyccHinl3dz4f7i1i9uQUFkwfRoxFxuoJ0RP5ZRVAdoebNz8+R5o1QqZ0H6IsYQaW3JjJ7TcM4/DZaj49VMzGnXm8tzufyaOtzMxJZNzwWOkfL4QImDxbAycLalk+J0umAB+iMpMjefiuCRRXNvHBFwV8tK+Yfxwo5sZxydwyMZkRyZF4VCVoek+4vVBR00JLN90upReHGCiSaAXQpl151DY6+MGycWg0cjZrKNNqNEwdY2XqGCvlNS18cqiE3Udt7D9VgdmoZWJWPDnDY8nOiJFFQoUQA8brVfl/H54mMkzPrImpgQ5HBFiaNYJVS3LIvWUEW78sZNdXNnYcKSU20sj4EXEAWKPNhJm6/nk5UL0nHC43py5UdzvtvfTiEANFjrIAKSxvZPv+YmZNSmFkalSgwxFBJDE2jHtuG8W3ZmdxqqCWvScrOHK+ii9O+Lr/xUeZyEi0MCwxgsTYMOKiTMRaTISbdBj0PZ/58qoqHo+Kx+ttu/Rdz7c1oFUUXB4vXq+KRqOgUZS2S9BqNZiNOsLa/gx6Tb93dZWxZUIE3vb9ReSXNfL93JxufzyLoSch2sy/LBzDXbNGcPhsFftPVbDnaBmutkWRzUYtljADljA9YUYdJqMOs0FLtMVIYrSZqHADZqNOhkyIQU++NQOgvYUw3KzjrllZgQ5HBIA/09Ib9TrGjYhj3Ig4vKpKcUUTJ/JrybM1UFDeyIEu+tPrtBoMl2UeKuB2e/F4vT3MUFVwVfFrNQpmo47IcAMxFiMxEUbfpcVIbKSR+CgzcVHXduZNxpYJEViVdXb+vvMCE7PimC4LFA9p3dZZisKk0VYmjbbidHvZvq+Qylo7dU1OGlqclFY10+rw0F717Dhi63iqTqt0JGOWMAOR4QaiI4xERxiItRhJiAkjIcaMsZcGRH+pqord4aG51cWhM5U0211UN7TS0Oykye6iye7C5fbicnvxeFW0WgWdRoPJqCWyLb6EGDPJceEkx4URGW7AZNBLY5/okfxCGWCqqvKX7Wc4X9rAqiXXycw9Q5Q/09JfmkRoFIVhiRaGJVo6Hm91uqmqb6WqrpXaJgctrS6aW9243d5O23F5vFTU2tFqFLRaBa1Wg1ajoNNcvJ6dGYvFrEev06BRFLyqiteroqq+s15uj0qr001Lqxu7w02Lw3e9vtlJbWMrxZVNNDQ5uTyPi7YYibMYiY82Ex9l8v21XY+LNMl4DyGClFdVeX3rKRRF4dsLx8iZhyHO36VUrNFmrNHmTvd7VRWH00Or00OKNYLjF6ppdbixOz0dl6VVzVworcfh9FzRIBgdYfAlXdFmoi1GYiIMWMIMmAxajAYtiqKgqr56qtnuotHuoqaxlaKKZhqbHbS0umluddPS6rpi20a9lqgIAxaznugIIwa9Fn1bvdjq8lBZ20Kr00NdYyMtDjdO18X61aDXkJ4QQUaihYwkC5lJkaTEh8kiz6ITSbQG2Na9hXxysIRFM4YxMycp0OGIEGYy6EizRvS6yHWzo/czQ+Oy4lB7WZClt4WfPR4vDS1OGprdVNQ2U93QSn2zi4qaFs4V17PvZHmnSk5RIMZiJD7SRFyUGUuYngiznnCznnCTDp1OQ3VDK0a9FqNei06ryI89IQaAqvp6XRzPr+VfFo6RcaHimmgUXw8Is1HH6GHR2Ftd3ZadOjYBvCo1DQ7Ka1uoqLV3XB7Nq6ah2Ynq59phep0Go16L2agjPtpEuMlCuFlHuEnPjJwkYiJ67r7oVeHAqc51p93hpr7JSW2Tg/omB26Pyu5jZXx8sKTjNdMTIshIspCRaCE9wUJslLHHRkXpAj+4SaI1gL44XsZbn5xn+tgEvjVbugyKnvnTvbC35Af8W9CyrxZ+bi9XUdtMXKSJzJTojsHIXq9Ki8NNU4sLa4yZxmYnlXWtVNXbOV1US5Pd1am1sCs6rYJep0Gn1aDXadhztIwwkw6TQYv20nFll1wHBRW17bav0lfaruu0Gkx6LRFhBsKNvu2YjbqOriz6S2o/f8aMgVSaIrSpqsqbH5/js8Ol3DEzg9mTZQIMMXA0ikJ42/dvRpLlise9XpWGFieNLS7fWTKXG1Wl4/s+3KTDEmZA0SictzV2OxmGNcbsV513ufaEMSkuDPD1PDEbtJTXtFBQ1kh+WSOF5Y18cbyMT9qSLwUIN+uJCNNjMeuxhPvOoJmNWkwGHTdPSCFWpskftCTRGgAer5d3duSx5YsCRqdFcf/ibDTSMi960VfJT18taHmtNBqFCLPvrNX07ETCuxhb5XJ7aG5109TWd/7IuSocLi9Ol6ej77zb47t0ebygQH2Tk3KXB4/Hi6qqeFXweC/v+ugbn6aqqt+toXBxQHdkmIEwkw67w42pLSEzGbSYDW3XjVoMei0aRZFxYyJkudxe/u/T83y0v4jbpqZx560j/Gpg8KcxRwh/+NPAGBFm7HWdyd620Zc0itI2biucG9p6KnlVlcpaO6eL6zh4upLGtuSwsLwJh8vT6fkbd+Zh0GuIDDMQFe4bC9Z+6bvuG/scazFiCTfI78cQI78G+llFnZ0/vX+SM0V1zJqUwj/dNkrWRBKiG3qdlugILdERRmIiTdQ2Onos313Cdqmuuk62J1wer4rL7WVsRgwK0Or0YHe4aWxx0tDiorHZ2dF6Wllnp7bRgcPpuWIsGvhaLQ16Ldv2FhEZfnGAt8Wsv3i97dJs9J05852Jk9NfIvAKyhp57f0TFFc2M3dKKv80bxSKouBw9t71OFgac0To86eB8fqcJByunrP7gUr+e0oMI8INTB6dgPeyYJwuD012F3aHh1anm/goM3aHm4YWJ/VNTirq7JwrqaepxXVFXaPVKJdMPGUixmIkLtJEXPsY6CgTJoP8tA8mfv038vLyWLNmDXV1dURHR7N27VoyMzM7lfF4PDz//PPs3LkTRVF44IEHWL58+TU9FsraF/X78kQFOp3CqsXXMXPcxTFZ0koohjJ/Wi378/hXFAVF8Z1l0+s0JMWH9zpGrb2/vldVcbo8tDp8g7vtTrfvusuDw+nGbNRjb3VRWtVMY0sdzfYrK8tLGXQaTEYd4WY9Bq0Gs9E3Js1ouHhpartuMvim1jcZdBj1WvQ6LYqitpXREWbSXTEWYCh2ZZQ6yz+qqpJna+STQ8V8cbycCLOeR741gUkj4wMdmhDd8re3RzDE0lUcBr2W2EtmUrw+J6nL+sfj9U3uUd/koNHupqqtsa+uyUFdo4PzJfXUtY0Tu1SEWX9x8qm2GYDbJ6KKiTCgXk23DnHN/Eq0nnrqKVauXElubi6bNm3iySef5PXXX+9U5r333qOwsJBt27ZRV1fHsmXLmDlzJmlpaV/7sVDh9apU1tsprWrmdGEdRy9UY6tuwaDXMG9aGguvH0bMZf1v/Zm6WloJxWAVTBUlXF08GkXBZNB122p4+Vk2r1elye7q6DrSaHdhd7hp7Zhxy3ddRaGusZVWh5vaJt+ZM4fL99fq9Pjd5VGnVTC0TSBi1GtJjA0jKlxPuOniRCPhprZLc/v9vsRtsEw2InVW17yqSn2Tk3xbA2eK6zieV0txZRNGg5ZZk1JYdssImQlXiAHmb/3TbHdiNoaREh/Wcb+qqrQ6PQxLiqS63k51fSs1DQ5qGloprGji8LmqKxIxbXs3/rYxY+EmfccMjh2Ne+2NfQYtJr0Oo16D0aDr/JhMUuWXXhOt6upqTpw4wYYNGwBYvHgxzz33HDU1NcTGxnaU27JlC8uXL0ej0RAbG8u8efPYunUr999//9d+zF++Ae9fr0xNQytfna9uG9vh607kbRvnQdt11QtefI973F4cbi8Op28thsYWJ/XNLjxti/RptRqyUqO4fWYmU0bFE95NpaXTaggz9VyhDWQZXzklqGIKtjKXljMbdXjcofG/DdQ+ArrdT30Wk16Lw93zBBoazQC//y5iMhp1GI064mPayui0uN2d++mbzAZa7c6L27mkjKqquLxeXC4vTpcXp9uDy+Wh1eXlbFEtbo+Kq33smsuD032xnMfrpaS6hZZWFx5P99maVqNgNukJM2oxG9svdW3LAvimO9ZoFLSKgkbbtpC1RoOCbwZJ8F1RgOzMWBJjzN2+Vm/8+U7vzmCvs/LLGsizNcJldZaq+tbMU73g9HjaEnUvDqcbh8tLS6uLmkZHp7pqWEIEt8/MYMpoKyZD113a++Kz0XffQT3XUQP1XRdM37u6tsXku6uP+vJ1Qm3fBkM9PRCvE26GYUkWmu0uUq0RpF4yE7GqqjhcHlrsbuJjzDS2OHG5VeqbWml2uLG3+pZqabS7cLo8ONzeju8If2g1Cnqdtm2SqrbJqjSajts6nS8Zu1xPDYeKAgq+XicovrqmvReKAvh62196f+fritI+6ZXvEgU0V9znq6toq8emtjekfs26p6fn9Zpo2Ww2EhMT0Wp9X8JarZaEhARsNlunSstms5GSktJxOzk5mbKysmt6zF8xMeG9lomL63oK7Li4CEYND0w3ibTkqF7LjEiLGbAyAOmJkUEVU7CVCcaYgq1MIF6vN8NSBu6zdk2irz45mT1tWD8EErqGQp01NeeqXu6a9UVd1Vefr97qqIH67gmm7115HXkdf7chetbd9+61GGI994UQQgghhBCi//WaaCUnJ1NeXo7H4+vC4vF4qKioIDk5+YpypaWlHbdtNhtJSUnX9JgQQghxNaTOEkIIESx6TbTi4uLIzs5m8+bNAGzevJns7OxOXTAAFi1axFtvvYXX66Wmpobt27ezcOHCa3pMCCGEuBpSZwkhhAgWiurHPI/nz59nzZo1NDQ0EBkZydq1axkxYgSrVq3ikUceYfz48Xg8Hp599ll2794NwKpVq1ixYgXA135MCCGEuFpSZwkhhAgGfiVaQgghhBBCCCH8J5NhCCGEEEIIIUQfk0RLCCGEEEIIIfqYJFpCCCGEEEII0cck0RJCCCGEEEKIPjaoE628vDxWrFjBwoULWbFiBfn5+YEOKSisXbuWuXPnMmbMGM6cOdNxv+yvK9XW1rJq1SoWLlzIkiVL+OEPf0hNTQ0Ahw8fZunSpSxcuJDvfe97VFdXBzjawHrooYdYunQpy5YtY+XKlZw8eRKQ46o7v//97zt9BuV4EvJZ8ZE6qmtSH3VP6p/uSV3T2dy5c1m0aBG5ubnk5uayc+dOoB/3izqIffvb31Y3btyoqqqqbty4Uf32t78d4IiCw759+9TS0lJ1zpw56unTpzvul/11pdraWvWLL77ouP0f//Ef6uOPP656PB513rx56r59+1RVVdV169apa9asCVSYQaGhoaHj+kcffaQuW7ZMVVU5rrpy7Ngx9b777uv4DMrxJFRVPivtpI7qmtRH3ZP6p2tS11zp8u8VVVX7db8M2jNa1dXVnDhxgsWLFwOwePFiTpw40dH6M5RNmzaN5OTkTvfJ/upadHQ0M2bM6Lg9adIkSktLOXbsGEajkWnTpgFwzz33sHXr1kCFGRQsFkvH9aamJhRFkeOqC06nk2effZann3664z45noR8Vi6SOqprUh91T+qfK0ld47/+3C+6PtlKELLZbCQmJqLVagHQarUkJCRgs9mIjY0NcHTBR/ZX77xeL3/961+ZO3cuNpuNlJSUjsdiY2Pxer3U1dURHR0dwCgD62c/+xm7d+9GVVX++Mc/ynHVhd/+9rcsXbqUtLS0jvvkeBLyWemZ7J/OpD66ktQ/nUld072f/OQnqKrK1KlT+bd/+7d+3S+D9oyWEH3tueeeIywsjHvvvTfQoQStF154gU8//ZRHH32UF198MdDhBJ1Dhw5x7NgxVq5cGehQhBAhTOqjK0n9c5HUNd37y1/+wrvvvsvbb7+Nqqo8++yz/fp6gzbRSk5Opry8HI/HA4DH46GiouKK7gjCR/ZXz9auXUtBQQG/+c1v0Gg0JCcnU1pa2vF4TU0NGo1mSLUI9WTZsmV8+eWXJCUlyXF1iX379nH+/Hluu+025s6dS1lZGffddx8FBQVyPA1x8h3cM9k/F0l91DOpf6Su6Un7/99gMLBy5UoOHjzYr5+hQZtoxcXFkZ2dzebNmwHYvHkz2dnZQ+J08dch+6t7L730EseOHWPdunUYDAYAxo0bR2trK/v37wfgjTfeYNGiRYEMM6Cam5ux2Wwdtz/++GOioqLkuLrMAw88wK5du/j444/5+OOPSUpK4rXXXuP++++X42mIk89Kz2T/+Eh9dCWpf64kdU3XWlpaaGxsBEBVVbZs2UJ2dna/foYUVVXVPtlSEDp//jxr1qyhoaGByMhI1q5dy4gRIwIdVsA9//zzbNu2jaqqKmJiYoiOjub999+X/dWFs2fPsnjxYjIzMzGZTACkpaWxbt06Dh48yFNPPYXD4SA1NZVf/vKXxMfHBzjiwKiqquKhhx7Cbrej0WiIioriscceIycnR46rHsydO5dXX32V0aNHy/Ek5LPSRuqorkl91DWpf3ondY1PUVERDz/8MB6PB6/XS1ZWFk888QQJCQn9tl8GdaIlhBBCCCGEEIEwaLsOCiGEEEIIIUSgSKIlhBBCCCGEEH1MEi0hhBBCCCGE6GOSaAkhhBBCCCFEH5NESwghhBBCCCH6mCRaQlyDL7/8kltvvTXQYQghhBC9kjpLiIGlC3QAQgSLyZMnd1y32+0YDAa0Wi0AzzzzDEuXLg1UaD2qqKjgN7/5DTt27KC5uZnExERuv/127r//fsLCwvrtdV9++WUKCgr41a9+1W+vIYQQomtSZ10dqbNEIEiiJUSbQ4cOdVyfO3cuzz//PDfeeGMAI+pdXV0d99xzD5MnT+aNN94gLS0Nm83Ga6+9RmFhIWPHjg10iEIIIfqB1FlCBD/pOihEL5xOJy+88AI333wzN998My+88AJOp7PLsq+//jq33347ZWVlOJ1O1q5dy+zZs7nxxht58sknaW1tBS523/jTn/7EzJkzufnmm3n77bc7tvPZZ59x++23M3nyZG655RZee+21Ll9vw4YNhIeH88tf/pK0tDQAkpOTeeKJJzoqrIMHD3LXXXcxdepU7rrrLg4ePNjx/Llz57Jnz56O2y+//DI/+clPACguLmbMmDG88847zJ49mxkzZrB+/XoAduzYwX/913/xwQcfMHny5KBtORVCiKFG6iyps0TwkERLiF6sX7+eI0eOsGnTJt59912OHj3KK6+8ckW53//+97zzzjv8+c9/JikpiV/96lfk5eWxceNGtm3bRkVFBevWresoX1VVRWNjIzt27OCFF17g2Wefpb6+HoCf/exnPPvssxw6dIjNmzdzww03dBnb559/zvz589Fouv4o19XV8eCDD/Ltb3+bL7/8ku9+97s8+OCD1NbW+v3+Dxw4wNatW/mf//kf1q1bx/nz57n11lt58MEH+cY3vsGhQ4d49913/d6eEEKI/iN1ltRZInhIoiVEL9577z1Wr15NXFwcsbGxrF69utOXtKqq/OIXv2D37t28/vrrxMbGoqoqf/vb3/j3f/93oqOjiYiI4MEHH+T999/veJ5Op2P16tXo9XpmzZpFWFgYeXl5HY+dO3eOpqYmoqKiyMnJ6TK2uro6rFZrt7F/+jynE4QAAAMySURBVOmnZGRksGzZMnQ6HYsXL2bEiBF88sknfr//H/7wh5hMJsaOHcvYsWM5deqU388VQggxsKTOkjpLBA8ZoyVELyoqKkhJSem4nZKSQkVFRcftxsZG/va3v/HrX/8ai8UCQE1NDXa7nTvvvLOjnKqqeL3ejtvR0dHodBc/gmazmZaWFgB+97vfsX79ev7zP/+TMWPG8OMf/7jTwOdLt1FZWel37O3xl5eX+/v2iY+P7zJGIYQQwUfqLKmzRPCQM1pC9CIhIYHS0tKO2zabjYSEhI7bkZGRvPrqqzz++OMcOHAAgJiYGEwmE++//z779+9n//79HDhwoNPg5Z5MmDCB9evXs2fPHubNm8ePfvSjLsvNnDmTjz76qFNl2FPs7fEnJiYCvkrIbrd3PNZTBXg5RVH8LiuEEGJgSJ3VNamzRCBIoiVEL+644w7Wr19PTU0NNTU1rFu3jiVLlnQqM2PGDH71q1/x8MMP89VXX6HRaFi+fDk///nPqa6uBqC8vJydO3f2+npOp5N3332XxsZG9Ho94eHh3fZn/+53v0tzczOPPfYYJSUlHa/zi1/8glOnTjFr1izy8/N57733cLvdbNmyhXPnzjF79mwAxo4dy5YtW3C5XBw9epQPP/zQ7/0SFxdHSUlJtxWmEEKIgSd1VtekzhKBIImWEL146KGHGDduHEuXLmXp0qXk5OTw0EMPXVHupptu4uc//znf//73OX78OD/96U/JyMjg7rvvZsqUKXznO9/p6M/em02bNjF37lymTJnCG2+8wS9/+csuy0VHR/PXv/4VnU7H3XffzeTJk/nXf/1XLBYLGRkZxMTE8Oqrr7JhwwZmzJjBH//4R1599VViY2MB+NGPfkRhYSHXX389L7/88hWVcU8WLVoE+Crsb37zm34/TwghRP+ROqtrUmeJQFBUVVUDHYQQQgghhBBCDCZyRksIIYQQQggh+pgkWkIIIYQQQgjRxyTREkIIIYQQQog+JomWEEIIIYQQQvQxSbSEEEIIIYQQoo9JoiWEEEIIIYQQfUwSLSGEEEIIIYToY5JoCSGEEEIIIUQfk0RLCCGEEEIIIfrY/w+pcvEekVsgVwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x792 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qerS7dkL-88l",
        "colab_type": "text"
      },
      "source": [
        "**Observation**:  The max tokens count  in a sequence is 70 in all dataset. Among all dataset, the train dataset(both in sentence1 and sentence 2) have higer tokens count of 70 and 63 respectively. But, the majority of tokens length are about 40 counts. \n",
        "\n",
        "So, this gives us the good idea for selecting the max-length of tokens in a sequence.  We can put our max-length of 60, so that we will not truncate more tokens(loss information) and also don't have to do more padding.\n",
        "\n",
        "But, we will use 120 tokens length (double of a single sequence) for the model architecure where a pair of sentence are required to feed together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ublMhtvmGQsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-6qnrXwZ91",
        "colab_type": "text"
      },
      "source": [
        "# Part-1: Sentence Level Embedding Extraction\n",
        "For Sentence Embedding Extraction, last 4 hidden layers are concatenated and mean pooling of words in a sequence/sentence is done. So, the final dimension of a sentence vector representation is 768*4 =3072."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNtl2Tj-OOS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to extract sentence level embedding \n",
        "def sent_embedding(sent, model):\n",
        "  \"\"\"\n",
        "   Concatenate from last 4 layers and Mean pooling of words in  a sequence\n",
        "\n",
        "  Args:\n",
        "     sent: list of sentences\n",
        "     model: BERT pre-trained model\n",
        "\n",
        "  Returns:\n",
        "     sent_embedding: 2d tensor \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #tokenize sequence into fix-length size of 60 tokens \n",
        "  #add ['CLS'], ['SEP'] and return 2d 'input_ids', 'attention_masks' and 'token_type_ids'\n",
        "  tokens = tokenizer( sent,  #list of strings\n",
        "                      padding = 'max_length', # padding max-length\n",
        "                      truncation = True, # truncate max-length\n",
        "                      max_length = 60, # max tokens size\n",
        "                      return_tensors='pt',  #return pytorch tensor\n",
        "                      verbose = False # stop throwing warnings\n",
        "                     )\n",
        "    \n",
        "\n",
        "\n",
        "  #feed input_ids to BERT model\n",
        "  with torch.no_grad():  # reduces memory consumption\n",
        "        outputs = model(tokens['input_ids'], tokens['attention_mask']) # feed input_ids, attention_mask\n",
        "        hidden_states = outputs[2] # all hidden layers\n",
        "\n",
        "   \n",
        "  '''\n",
        "  #sent_embeding = torch.cat(tuple([hidden_states[i].mean(dim=1) for i in [-4,-3,-2,-1]]), dim = 1)\n",
        "  #sent_embeding = hidden_states[-2].mean(dim=1) '''\n",
        "\n",
        "  concat_embeding = torch.cat(tuple([hidden_states[i] for i in [-4,-3,-2,-1]]), dim = 2) #concatenation last 4 hidden layers\n",
        "  print('Concatenated Vector Dimension: ',concat_embeding.shape )\n",
        "  padded = tokens['attention_mask'].unsqueeze(2) # insert size one at 2 position to make 3dimesion\n",
        "  mul_out = torch.mul(concat_embeding, padded)# make zeros vector for paddding tokens\n",
        "  sent_embeding = mul_out.mean(dim=1) # average pooling  of tokens\n",
        "  print('Sentence Embedding Dimension: ',sent_embeding.shape )\n",
        "   \n",
        "  return sent_embeding\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5AASUSgMJi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab1be158-cbef-41c3-d714-e8dd3c86a61e"
      },
      "source": [
        "#initiate the pre-trained model\n",
        "model = BertModel.from_pretrained(  'bert-base-uncased',\n",
        "                                     output_hidden_states = True # return all hidden-states\n",
        "                                    )\n",
        "#put model in evaluation\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLA8lPX5EN2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H_0awHH2Ucr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOn7EXoMRNTb",
        "colab_type": "text"
      },
      "source": [
        "#Extract Sentence level embedding\n",
        "Now, lets extract sentence level vector representation for all 3 datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG0mK0m0zJHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f8b815b8-1bcb-4a51-df79-a9ddd4190139"
      },
      "source": [
        "%%time\n",
        "#dev dataset\n",
        "df_dev_sentence_1 = sent_embedding(df_dev.sentence_1.tolist(), model)\n",
        "df_dev_sentence_2 = sent_embedding(df_dev.sentence_2.tolist(), model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "CPU times: user 8min 40s, sys: 11.1 s, total: 8min 51s\n",
            "Wall time: 8min 52s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj3ks6x98cQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37r03c58pruB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "467f595c-de1e-45bc-9094-3ac15775e30b"
      },
      "source": [
        "%%time\n",
        "#test dataset\n",
        "df_test_sentence_1 = sent_embedding(df_test.sentence_1.tolist(), model)\n",
        "df_test_sentence_2 = sent_embedding(df_test.sentence_2.tolist(), model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Concatenated Vector Dimension:  torch.Size([1379, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1379, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1379, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1379, 3072])\n",
            "CPU times: user 7min 59s, sys: 2.55 s, total: 8min 1s\n",
            "Wall time: 8min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPfxsq_BSvY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNbpHF-US11U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "af6954b3-7d94-4469-e120-1dbb69ebeb0d"
      },
      "source": [
        "%%time\n",
        "#train dataset\n",
        "train_embed_sentence_1 =  []\n",
        "train_embed_sentence_2 =  []\n",
        "\n",
        "#make batches to avoid crash of RAM\n",
        "for x in range(0, df_train.shape[0], 1500):\n",
        "  embed_1= sent_embedding(df_train.sentence_1.tolist()[x: x+1500], model)\n",
        "  embed_2= sent_embedding(df_train.sentence_2.tolist()[x: x+1500], model)\n",
        "  train_embed_sentence_1.extend(embed_1)\n",
        "  train_embed_sentence_2.extend(embed_2)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1500, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1500, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1249, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1249, 3072])\n",
            "Concatenated Vector Dimension:  torch.Size([1249, 60, 3072])\n",
            "Sentence Embedding Dimension:  torch.Size([1249, 3072])\n",
            "CPU times: user 33min 14s, sys: 10.1 s, total: 33min 24s\n",
            "Wall time: 33min 27s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1NAwk-PelS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentence level vector represetnation in train dataset\n",
        "df_train_sentence_1 = torch.stack(train_embed_sentence_1) # change list of tensors >>> 2d tensor\n",
        "df_train_sentence_2 = torch.stack(train_embed_sentence_2) # change list of tensors >>> 2d tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_RkbrOCvr3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W9QSzRMREQM",
        "colab_type": "text"
      },
      "source": [
        "# Calculate Cosine Similarity\n",
        "Now, sentence level vector representation of each pair sentence is used to compute cosine similarity. Cosine similarity is calculated on all 3 datasets just to know how the extracted sentence level embedding perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwMvRGPRC5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine #cosine distance\n",
        "from sklearn.preprocessing import MinMaxScaler # minmax scaling\n",
        "from scipy.stats import pearsonr # pearson correlation\n",
        "from scipy.stats import spearmanr # spearman correlation\n",
        "from collections import defaultdict #store scores\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gEbjpc5obHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to calculate similarity score \n",
        "def calc_similarity_score(embed_1, embed_2):\n",
        "\n",
        "  \"\"\"\n",
        "  Calcualte cosine simialrity between a pair of sentences.\n",
        "  \n",
        "    Args:\n",
        "      embed_1(numpy array): first sentence embedding\n",
        "      embed_2(numpy array): second sentence embedding\n",
        "\n",
        "    Return:\n",
        "      scaled_cosine_score(list) : cosine score between embed_1 \n",
        "      and embed_2 with range (0,5)\n",
        "  \"\"\"\n",
        "  \n",
        "  #collect similarity score\n",
        "  cosine_sim_score = []\n",
        "  \n",
        "  for x,y in zip(embed_1, embed_2):\n",
        "\n",
        "    distance = cosine(x, y) # cosine distance\n",
        "    cosine_sim = 1 - distance # cosine similarity\n",
        "    cosine_sim_score.append(cosine_sim) \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  #scaled the similarity score to (0,5)\n",
        "  scaler = MinMaxScaler(feature_range=(0,5)) # initiate minmaxscaler\n",
        "  scaled_cosine_score = scaler.fit_transform(np.transpose([cosine_sim_score])) #scaled to (0,5)\n",
        "  \n",
        "  return scaled_cosine_score.round(3)\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b53f69trJD8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPnbqdPFGcNo",
        "colab_type": "text"
      },
      "source": [
        "#Pearson and Spearman score\n",
        "Calculate the Pearson and Spearman correlation by comparing with computed cosine similarity score with gold score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfcMwCp33USf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_pearson_spearman_score(x, y):\n",
        "  '''\n",
        "  Calculate Pearson and Spearman correlation between x, y.\n",
        "  Agrs:\n",
        "    x: array_like\n",
        "    y: array_like\n",
        "\n",
        "  Returns:\n",
        "    Pearson and Spearman score\n",
        "  '''\n",
        "\n",
        "  pearson_score, _ = pearsonr(x, y)#pearson score\n",
        "  sperman_score, _ = spearmanr(x, y)#spearman score\n",
        "\n",
        "  return pearson_score , sperman_score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-ozSu0GDmy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9d990f5d-59a4-49fa-e459-15d9cda32ebd"
      },
      "source": [
        "#Pearson and Spearman score\n",
        "first_sent_embd = [df_train_sentence_1, df_dev_sentence_1, df_test_sentence_1] #first sentence embedding\n",
        "second_sent_embd = [df_train_sentence_2, df_dev_sentence_2, df_test_sentence_2] #second sentence embedding\n",
        "gold_labels = [df_train.loc[:,'score(0-5)'].values, df_dev.loc[:,'score(0-5)'].values, df_test.loc[:,'score(0-5)'].values ] #gold scores\n",
        "data_type = ['Train', 'Validation', 'Test'] #datasets names\n",
        "\n",
        "pear_spear_score = defaultdict(list) #store scores\n",
        "\n",
        "#zip and enumerate 'first_sent_embd' and 'second_sent_embd'\n",
        "for i, (first_sent, second_sent) in enumerate(zip(first_sent_embd,second_sent_embd)):\n",
        "  cosine_sim_score = calc_similarity_score(first_sent, second_sent) #cosine similarity score\n",
        "  cosine_sim_score = cosine_sim_score.flatten()# change to 1d\n",
        "\n",
        "  person_score, _ = pearsonr(gold_labels[i], cosine_sim_score) #pearson score\n",
        "  pear_spear_score['pearson'].append(person_score)\n",
        "  \n",
        "\n",
        "  sperman_score, _ = spearmanr(gold_labels[i], cosine_sim_score)#spearman score\n",
        "  pear_spear_score['spearman'].append(sperman_score)\n",
        "\n",
        "  print(data_type[i], 'data: ')\n",
        "  print('=='*6)\n",
        "  print(\"Pearson score: {} \\nSpearman score: {}\".format(person_score, sperman_score))\n",
        "  print()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data: \n",
            "============\n",
            "Pearson score: 0.5090061328623349 \n",
            "Spearman score: 0.4820996486643903\n",
            "\n",
            "Validation data: \n",
            "============\n",
            "Pearson score: 0.5929747726822713 \n",
            "Spearman score: 0.5956354804822067\n",
            "\n",
            "Test data: \n",
            "============\n",
            "Pearson score: 0.4767255590987412 \n",
            "Spearman score: 0.4695436549422946\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E0Z4ffWPEQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "1a34647a-441f-4032-fbcc-cb75d1fd3f65"
      },
      "source": [
        "#show in panda dataframe\n",
        "pd.DataFrame.from_dict(pear_spear_score,\n",
        "                          orient='index',\n",
        "                          columns=['Train', 'Dev', 'Test']\n",
        "                          )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train</th>\n",
              "      <th>Dev</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pearson</th>\n",
              "      <td>0.509006</td>\n",
              "      <td>0.592975</td>\n",
              "      <td>0.476726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spearman</th>\n",
              "      <td>0.482100</td>\n",
              "      <td>0.595635</td>\n",
              "      <td>0.469544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Train       Dev      Test\n",
              "pearson   0.509006  0.592975  0.476726\n",
              "spearman  0.482100  0.595635  0.469544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGYhyjojTLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fL8zk1xCduJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4K0cpKC03t3",
        "colab_type": "text"
      },
      "source": [
        "# Part-2: Fine-tune Bert\n",
        "The custom model is defined using pytorch nn.module. And, The pre-trained BERT(bert-base-uncased) model is fine-tunned with STS-b train dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBO8LnzX5T8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import necessary libraries\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader #pytorch dataset and dataloader\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup #optimizer and learning rate scheduler\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0mqIXgTpOcv",
        "colab_type": "text"
      },
      "source": [
        "# Custom Model\n",
        "The two different network stuctures are tried  for fine-tuning the pre-trained model. They are as follows:\n",
        "\n",
        "1. Siamese Network: In this Siamese Network structure, two different sentences(a pair of sentences) are encoded separetely meaning they are encoded one by one, and their final vecotors are used to calculate the cosine similarity.  The output layer is just the cosine similairty. No additonal layers.  The vector representation of a sentence is the concatenation of the last 4 hidden layers, and mean pooling of the tokens in a inputed sequence. \n",
        "\n",
        "2. Regression Model: In this fine-tuning, the Linear Regression output layer is added on the top of Bert. The two sentences(a pair of sentences) are feeded and encoded together. And, the final output is the regression ouput.\n",
        "\n",
        "The Mean Square Error(MSE) Loss is used as loss function  for both the Netowrk structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSIIZmvz02nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define custom model\n",
        "class SentenceSimilarityModel(nn.Module):\n",
        "  \"\"\"Bert model to calculate cosine similarity between a pair of sentences\"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(SentenceSimilarityModel, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(  'bert-base-uncased',\n",
        "                                          output_hidden_states = True # return all hidden-states\n",
        "                                          ) #initiate pretrained bert model\n",
        "    self.drop = nn.Dropout(p=0.3) #dropout 30 percent\n",
        "    self.regression = nn.Linear(768,1) #Regression layer with one output\n",
        "    self.cosine_sim = nn.CosineSimilarity(dim=1, eps=1e-6) # cosine similarity\n",
        "\n",
        "    \n",
        "  #forward pass\n",
        "  def forward(self, input_ids, attention_masks,  model_type, token_type_ids=None):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "          input_ids_1(tensor) : input_ids of first sentence\n",
        "          input_ids_2(tensor) : input_ids of second sentence\n",
        "          attention_mask_1(tensor): attention_mask of first sentence\n",
        "          attention_mask_2(tensor): attention_mask of second sentence\n",
        "          model_type(str): type of model architecture to calculate ouput\n",
        "          (for example, 'regression' or 'cosine')\n",
        "\n",
        "       Returns:\n",
        "          cosine_score(1d tensor): Cosine similarity between a pair of sentences\n",
        "          or\n",
        "          regre_output(1d tensor) : regression output\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    #if model type is 'Siamese Network' i.e. cosine output\n",
        "    if  model_type == 'cosine':\n",
        "      \n",
        "      #first sentence embedding\n",
        "      input_1 = input_ids['first_sent'].to(device)\n",
        "      atten_1 = attention_masks['first_sent'].to(device)\n",
        "      output_1 = self.bert(\n",
        "          input_ids = input_1,\n",
        "          attention_mask = atten_1\n",
        "      )\n",
        "      hidden_states_1 = output_1[2] #hidden states only\n",
        "      #hidden_states_1 = self.drop(hidden_states_1)\n",
        "      #sent_embeding_1 = torch.cat(tuple([hidden_states_1[i].mean(dim=1) for i in [-4,-3,-2,-1]]), dim = 1) #concat last four hidden layers\n",
        "      concat_embeding_1 = torch.cat(tuple([self.drop(hidden_states_1[i]) for i in [-4,-3,-2,-1]]), dim = 2) # last four hidden layers\n",
        "      padded_1 = atten_1.to(device).unsqueeze(2) #insert size one at position 2\n",
        "      mul_out_1 = torch.mul(concat_embeding_1, padded_1)#make zeros vector for paddding tokens\n",
        "      sent_embeding_1 = mul_out_1.mean(dim=1) # pooling average of tokens\n",
        "      #out_1 = mul_out_1[:, 0, :] #cls token\n",
        "  \n",
        "    \n",
        "\n",
        "      #second sentence embedding\n",
        "      input_2 = input_ids['second_sent'].to(device)\n",
        "      atten_2 = attention_masks['second_sent'].to(device)\n",
        "      output_2 = self.bert(\n",
        "          input_ids = input_2,\n",
        "          attention_mask = atten_2\n",
        "      )\n",
        "      hidden_states_2 = output_2[2]\n",
        "      #hidden_states_2 = self.drop(hidden_states_2)\n",
        "      #sent_embeding_2 = torch.cat(tuple([hidden_states_2[i].mean(dim=1) for i in [-4,-3,-2,-1]]), dim = 1)\n",
        "      concat_embeding_2 = torch.cat(tuple([self.drop(hidden_states_2[i]) for i in [-4,-3,-2,-1]]), dim = 2) # last four layers\n",
        "      concat_embeding_2 = self.drop(concat_embeding_2)\n",
        "      padded_2 = atten_2.unsqueeze(2) #insert size one at position 2\n",
        "      mul_out_2 = torch.mul(concat_embeding_2, padded_2)#make zeros vector for paddding tokens\n",
        "      sent_embeding_2 = mul_out_2.mean(dim=1) # pooling average of tokens\n",
        "      #out_2 = mul_out_2[:, 0, :] #cls token\n",
        "  \n",
        "      #calculate cosine similarity\n",
        "      cosine_score = self.cosine_sim(sent_embeding_1, sent_embeding_2)\n",
        "      return cosine_score\n",
        "\n",
        "    \n",
        "    #Regression model\n",
        "    elif model_type == 'regression':\n",
        "      if token_type_ids is not None:\n",
        "        #initiate pre-trained model\n",
        "        bert_output = self.bert(  \n",
        "          input_ids = input_ids,\n",
        "          attention_mask = attention_masks,\n",
        "          token_type_ids = token_type_ids\n",
        "        )\n",
        "\n",
        "        #hidden_state = bert_output[0]# last_hidden_state\n",
        "        hidden_state = bert_output[1] #['CLS'] token of last hidden_state\n",
        "        #hidden_state = bert_output[2] #all hidden states\n",
        "\n",
        "        drop_out =  self.drop(hidden_state) #apply dropout\n",
        "        regre_output =  self.regression(drop_out)# apply linnear tranformation\n",
        "        return regre_output.flatten() #change to 1d tensor\n",
        "\n",
        "      else:\n",
        "        print('Regression model requires token_type_ids!!!')\n",
        "\n",
        "\n",
        "    else:\n",
        "      print(\"Please sepecify model's architecture :['cosine', 'regression']\")\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXtcA5sUhc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQsxqYZpQ11y",
        "colab_type": "text"
      },
      "source": [
        "# Create Pytorch Dataset\n",
        "Pytorch dataset is created that returns input_ids, attention_mask, token_type_ids, and gold_score from each input sequence.\n",
        "\n",
        "Since  we are trying two different model network architectures for fine-tuning, the way of feeding the pair of sentences are different from one architecture to another. For example, in a regression model, we feed a pair of sentences together while in Siamese network model, we feed one sentence and take sentence level embedding and same goes to its pair sentence. \n",
        "\n",
        "Thus, a function  is written that returns the pytorch dataset which fit for both the model architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEa5bH6zQH97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define function to pytorch Dataset\n",
        "class SimilarityDataset(Dataset):\n",
        "  \"\"\" Pytorch dataset that returns input_ids, attension_mask from first, second sentence,\n",
        "   and gold_score  of STS-B dataset.\n",
        "\n",
        "   Dataset is return based on the model type is provided i.e. either ('regresion' or 'cosine'). \n",
        "   Default is Siamese netowrk structure.\n",
        "   \n",
        "   \"\"\"\n",
        "\n",
        "  def __init__(self, sent_1s, sent_2s, scores, tokenizer, max_len, model_type = 'cosine'):\n",
        "    \"\"\"\n",
        "     Args:\n",
        "       sent_1s(numpy array): Array of first sentences\n",
        "       sent_2s(numpy array): Array of second sentences\n",
        "       scores(float): Gold score (0-5)\n",
        "       tokenizer: Higging Face Bert Tokenizer\n",
        "       max_len(int): Maximun token length\n",
        "       model_type(str): type of model architecture to calculate \n",
        "       ouput(for example, 'regression' or 'cosine')\n",
        "      \n",
        "    \"\"\"\n",
        "    self.sent_1s = sent_1s\n",
        "    self.sent_2s = sent_2s\n",
        "    self.scores = scores\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.model_type = model_type\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sent_1s) #total no. of samples\n",
        "\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    first_sent = self.sent_1s[item]  #first sentencs\n",
        "    second_sent = self.sent_2s[item] #second sentence\n",
        "    score = self.scores[item]\n",
        "\n",
        "    #print('model_type is: {}'.format(self.model_type))\n",
        "    # model type is 'Siamese Network'\n",
        "    if self.model_type == 'cosine':\n",
        "\n",
        "      #encode first sentence\n",
        "      first_sent_encoding = self.tokenizer( first_sent,\n",
        "                                            add_special_tokens=True, #add ['CLS'] and ['SEP']\n",
        "                                            max_length=self.max_len, # set max len for padding\n",
        "                                            return_token_type_ids=False, #set false to segment_ids\n",
        "                                            padding='max_length', # padding\n",
        "                                            return_attention_mask=True,  # return attention mask\n",
        "                                            return_tensors='pt', # pytorch tensor\n",
        "                                            )\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      #encode second sentence\n",
        "      second_sent_encoding = self.tokenizer( second_sent,\n",
        "                                            add_special_tokens=True,\n",
        "                                            max_length=self.max_len,\n",
        "                                            return_token_type_ids=False,\n",
        "                                            padding= 'max_length',\n",
        "                                            return_attention_mask=True,\n",
        "                                            return_tensors='pt',\n",
        "                                            )\n",
        "      \n",
        "      return {\n",
        "          'input_ids' :{ \n",
        "              'first_sent' : first_sent_encoding['input_ids'].flatten(),\n",
        "              'second_sent' : second_sent_encoding['input_ids'].flatten()\n",
        "              \n",
        "          },\n",
        "\n",
        "          'attention_ids' :{\n",
        "              'first_sent' :first_sent_encoding['attention_mask'].flatten(),\n",
        "              'second_sent' :second_sent_encoding['attention_mask'].flatten()\n",
        "          },\n",
        "\n",
        "          'score' : torch.tensor(score)\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "    #if model's out is regression type\n",
        "    else:\n",
        "\n",
        "      #encode input sequences\n",
        "      sent_encoding = self.tokenizer(   first_sent, second_sent,\n",
        "                                        add_special_tokens=True,\n",
        "                                        max_length=self.max_len,\n",
        "                                        return_token_type_ids=True,\n",
        "                                        padding= 'max_length',\n",
        "                                        truncation = True,\n",
        "                                        return_attention_mask=True,\n",
        "                                        return_tensors='pt',\n",
        "                                      )\n",
        "      return {\n",
        "            'input_ids' : sent_encoding['input_ids'].flatten(),\n",
        "            'attention_ids' : sent_encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids' : sent_encoding['token_type_ids'].flatten(),\n",
        "            'score' : torch.tensor(score),\n",
        "\n",
        "        }\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRhAVaDkwUIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffC_ZF0Me4bV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "eaa38be9-8f36-4063-8091-d5a2c5e11f35"
      },
      "source": [
        "#lets test our pytoch dataset for Siamese Network (cosine output)\n",
        "dt = SimilarityDataset(df_train.sentence_1[:5], df_train.sentence_2[:5], df.loc[:5,'score(0-5)'], tokenizer, 20, model_type='cosine' )\n",
        "input_id_1 = dt[0]['input_ids']['first_sent'] #first senquence\n",
        "input_id_2 = dt[0]['input_ids']['second_sent'] #second senquence\n",
        "print('Input_ids 1st sent :', input_id_1)\n",
        "print()\n",
        "print('Input_ids 2nd sent :', input_id_2)\n",
        "print()\n",
        "print('A pair of sentences are tokenized separetly:')\n",
        "print(tokenizer.convert_ids_to_tokens(input_id_1)) #convert ids to tokens\n",
        "print()\n",
        "print(tokenizer.convert_ids_to_tokens(input_id_2)) #for second sequence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input_ids 1st sent : tensor([ 101, 1037, 4946, 2003, 2635, 2125, 1012,  102,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "\n",
            "Input_ids 2nd sent : tensor([ 101, 2019, 2250, 4946, 2003, 2635, 2125, 1012,  102,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "\n",
            "A pair of sentences are tokenized separetly:\n",
            "['[CLS]', 'a', 'plane', 'is', 'taking', 'off', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "\n",
            "['[CLS]', 'an', 'air', 'plane', 'is', 'taking', 'off', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9td9DiXFiiM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCtAuHM_SUzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1c53a450-ce29-4836-dcc4-8de3bf863246"
      },
      "source": [
        "#lets test our pytoch dataset for regression model type \n",
        "dt = SimilarityDataset(df_train.sentence_1[:5], df_train.sentence_2[:5], df.loc[:5,'score(0-5)'], tokenizer, 20,model_type='regression' )\n",
        "input_id = dt[0]['input_ids'] # input_ids\n",
        "atten_id = dt[0]['attention_ids'] #attention mask\n",
        "token_type_id = dt[0]['token_type_ids'] #token type ids\n",
        "print('Input_ids :', input_id)\n",
        "print('Attention_mask :', atten_id)\n",
        "print('Token type ids :',token_type_id)\n",
        "print()\n",
        "print('A pair of sentences tokenized together:')\n",
        "print(tokenizer.convert_ids_to_tokens(input_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input_ids : tensor([ 101, 1037, 4946, 2003, 2635, 2125, 1012,  102, 2019, 2250, 4946, 2003,\n",
            "        2635, 2125, 1012,  102,    0,    0,    0,    0])\n",
            "Attention_mask : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
            "Token type ids : tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
            "\n",
            "A pair of sentences tokenized together:\n",
            "['[CLS]', 'a', 'plane', 'is', 'taking', 'off', '.', '[SEP]', 'an', 'air', 'plane', 'is', 'taking', 'off', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m2lYZ_YcjWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHN6hIpCVh20",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch Data Loader\n",
        "It provides an iterable over the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcSgz7WOgyjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define pytorch data loader\n",
        "def pytorch_data_loader(df, tokenizer, max_len, batch_size, model_type='cosine'):\n",
        "  \"\"\"\n",
        "    Agrs:\n",
        "       df(panda dataframe): dataset\n",
        "       tokenizer(Hugging face tokeniser): Bert Tokonizer\n",
        "       max_len(int): Maximun tokens length in a sequence/sentence\n",
        "       batch_size(int): Number of samples in a batch\n",
        "       model_type(str): type of model architecture to calculate \n",
        "       ouputs(for example, 'regression' or 'cosine')\n",
        "\n",
        "    Return:\n",
        "      Dataloader(pytorch dataloader): Dataset with given batch size\n",
        "  \"\"\"\n",
        "  dataset = SimilarityDataset(\n",
        "      sent_1s =df.sentence_1.to_numpy(), #first sequence\n",
        "      sent_2s =df.sentence_2.to_numpy(), #second sequence\n",
        "      scores =df.loc[:,'scaled_score(0-1)'].to_numpy(), #gold score\n",
        "      tokenizer=tokenizer, #hugging face tokenizer\n",
        "      max_len=max_len, #max length for padding\n",
        "      model_type = model_type # return dataset as per model type\n",
        "  )\n",
        "\n",
        "  return DataLoader( \n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle= True\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHaA7do6HwUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g9rvYYPHwtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define model training function\n",
        "def model_train(model, dataloader, device, loss_fun, optimizer, scheduler, model_type):\n",
        "  \"\"\"\n",
        "     Args:\n",
        "       model: Custom model\n",
        "       dataloader: Pytorh data loader\n",
        "       device: device to compute\n",
        "       loss_fun:  loss function( MES loss)\n",
        "       optimizer: optimizer (Adamw)\n",
        "       scheduler: learning rate schedule\n",
        "       model_type(str): type of model architecture('regression' or 'cosine')\n",
        "\n",
        "    Returns:\n",
        "       training loss, and Pearson correlation between gold_score and predicted score\n",
        "       \n",
        "  \"\"\"\n",
        "  print('Model is training based on {} ouput!!!'.format(model_type))\n",
        "\n",
        "  #put model in training mode\n",
        "  model.train()\n",
        "\n",
        "  losses = [] # store loss from each batch\n",
        "  pearson_score = [] #pearson score from each batch\n",
        "\n",
        "  #Siamese Model type(Cosine output)\n",
        "  if model_type == 'cosine':\n",
        "  \n",
        "    #iterate over each batch of training data\n",
        "    for batch in dataloader:\n",
        "\n",
        "      #input ids\n",
        "      input_ids = batch['input_ids']\n",
        "      atten_ids = batch['attention_ids']\n",
        "\n",
        "      #gold score\n",
        "      gold_score = batch['score'].to(device).float() # change to float dtype as same as model output\n",
        "      \n",
        "      #clear previously calculated gradients\n",
        "      model.zero_grad()\n",
        "\n",
        "      #model forward pass\n",
        "      output = model(input_ids, atten_ids, model_type) # output dtype --> float tenosr \n",
        "      \n",
        "      #Calculate Pearson Correlation score\n",
        "      p_score = pearsonr(output.detach().cpu(), gold_score.detach().cpu())\n",
        "      pearson_score.append(p_score[0])\n",
        "      \n",
        "      #perform loss\n",
        "      loss = loss_fun(output, gold_score)\n",
        "      losses.append(loss.item()) \n",
        "\n",
        "      loss.backward() # perform backward pass to calculate gradients\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)# clip the norm of the gradients to 1.0.\n",
        "      optimizer.step() #update parameters\n",
        "      scheduler.step() #update learning rate\n",
        "      \n",
        "    return np.mean(losses), np.mean(pearson_score)\n",
        "\n",
        "  #Regression model type\n",
        "  elif model_type == 'regression':\n",
        "\n",
        "    #iterate over each batch of training data\n",
        "    for batch in dataloader:\n",
        "      \n",
        "      #Input sequence \n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      atten_ids = batch['attention_ids'].to(device)\n",
        "      tok_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "      #gold score\n",
        "      gold_score = batch['score'].to(device).float() # change to float dtype as same as model output\n",
        "      \n",
        "      #clear previously calculated gradients\n",
        "      model.zero_grad()\n",
        "\n",
        "      #model forward pass\n",
        "      output = model(input_ids, atten_ids, model_type, tok_type_ids) # output dtype --> float tenosr \n",
        "      \n",
        "      #Calculate Pearson Correlation score\n",
        "      p_score = pearsonr(output.detach().cpu(), gold_score.detach().cpu())\n",
        "      pearson_score.append(p_score[0])\n",
        "      \n",
        "      #perform loss\n",
        "      loss = loss_fun(output, gold_score)\n",
        "      losses.append(loss.item()) \n",
        "\n",
        "      loss.backward() # perform backward pass to calculate gradients\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)# clip the norm of the gradients to 1.0.\n",
        "      optimizer.step() #update parameters\n",
        "      scheduler.step() #update learning rate\n",
        "    return np.mean(losses), np.mean(pearson_score)\n",
        "\n",
        "  else:\n",
        "    print('Pleae specify the model type for training the data!!!')\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA8wxPXfH5Rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define evaluation function\n",
        "def model_eval(model, dataloader, device, loss_fun, model_type):\n",
        "  \"\"\"\n",
        "     Args:\n",
        "       model: Custom model\n",
        "       dataloader: pytorch data loader\n",
        "       device: device to compute\n",
        "       loss_fun:  loss function( MES loss)\n",
        "       \n",
        "    Returns:\n",
        "       Evaluation loss, and Pearson correlation between gold_score and predicted score\n",
        "       \n",
        "  \"\"\"\n",
        "  #print('Model is validation based on {} ouput!!!'.format(model_type))\n",
        "\n",
        "  #put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  losses = []\n",
        "  pearson_score = []\n",
        "  \n",
        "\n",
        "  #tell model not to compute or store gradients\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    #Siamese model type(cosine)\n",
        "    if model_type == 'cosine':\n",
        "      \n",
        "      #each batch of evaluation data\n",
        "      for batch in dataloader:\n",
        "        #first sentence\n",
        "        input_ids = batch['input_ids']\n",
        "        atten_ids = batch['attention_ids']\n",
        "\n",
        "        #gold score\n",
        "        gold_score = batch['score'].to(device).float() # change to float dtype as same as model output\n",
        "\n",
        "        #model forward pass\n",
        "        model = model\n",
        "        output = model(input_ids, atten_ids, model_type) # output type --> float tensor \n",
        "\n",
        "        #perform loss\n",
        "        loss = loss_fun(output, gold_score)\n",
        "        losses.append(loss.item()) \n",
        "        \n",
        "        #Calculate Pearson Correlation score\n",
        "        p_score = pearsonr(output.detach().cpu(), gold_score.detach().cpu())\n",
        "        pearson_score.append(p_score[0])\n",
        "      return np.mean(losses), np.mean(pearson_score)\n",
        "    \n",
        "\n",
        "    #regression model type\n",
        "    elif model_type == 'regression':\n",
        "\n",
        "      #each batch of evaluation data\n",
        "      for batch in dataloader:\n",
        "        #first sentence\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        atten_ids = batch['attention_ids'].to(device)\n",
        "        tok_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "        #gold score\n",
        "        gold_score = batch['score'].to(device).float() # change to float dtype as same as model output\n",
        "\n",
        "        #model forward pass\n",
        "        model = model\n",
        "        output = model(input_ids, atten_ids, model_type, tok_type_ids) # output type --> float tensor \n",
        "\n",
        "        #perform loss\n",
        "        loss = loss_fun(output, gold_score)\n",
        "        losses.append(loss.item()) \n",
        "        \n",
        "        #Calculate Pearson Correlation score\n",
        "        p_score = pearsonr(output.detach().cpu(), gold_score.detach().cpu())\n",
        "        pearson_score.append(p_score[0])\n",
        "      return np.mean(losses), np.mean(pearson_score)\n",
        "\n",
        "    else:\n",
        "      print('Please specify the model type to evaluate the data!!!')\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWpK0RpoLpuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2Ftr_BMK8M",
        "colab_type": "text"
      },
      "source": [
        "# Re-scaled gold score from (0-5) to (0,1)\n",
        "\n",
        "Before making the Data to Pytorch Dataloader, lets re-scaled the 'gold score' to (0,1). This is done because the cosine similarity score is (0,1). So, it makes sense to calculate the loss during the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-0tO4XVL5il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#re-scale to all datasets\n",
        "df_train['scaled_score(0-1)'] = MinMaxScaler(feature_range=(0,1)).fit_transform(df_train[['score(0-5)']])\n",
        "df_dev['scaled_score(0-1)'] = MinMaxScaler(feature_range=(0,1)).fit_transform(df_dev[['score(0-5)']])\n",
        "df_test['scaled_score(0-1)'] = MinMaxScaler(feature_range=(0,1)).fit_transform(df_test[['score(0-5)']])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAzolCLNOXng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C87nbyp-kqTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create data loader for Regression model\n",
        "BATCH_SIZE = 16\n",
        "train_data_loader_reg = pytorch_data_loader( df_train, tokenizer, 120, BATCH_SIZE, 'regression') # train data loader\n",
        "dev_data_loader_reg = pytorch_data_loader(df_dev, tokenizer, 120, BATCH_SIZE , 'regression', )# dev data loader\n",
        "test_data_loader_reg = pytorch_data_loader(df_test, tokenizer, 120, BATCH_SIZE, 'regression' )# test data loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH9VNuPkjEay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L3ef6p_iJk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create data loader for Siamese model i.e. cosine similarity output\n",
        "train_data_loader_cos = pytorch_data_loader( df_train, tokenizer, 70, BATCH_SIZE) # train data loader\n",
        "dev_data_loader_cos= pytorch_data_loader(df_dev, tokenizer, 70, BATCH_SIZE )# dev data loader\n",
        "test_data_loader_cos = pytorch_data_loader(df_test, tokenizer, 70, BATCH_SIZE)# test data loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSK-Pz-xZ8tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c39132e7-d581-436c-ce9e-3df5e6c895c4"
      },
      "source": [
        "#print batch of dev_data_loader_reg\n",
        "data = next(iter(train_data_loader_cos))\n",
        "data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_ids': {'first_sent': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0]]),\n",
              "  'second_sent': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          ...,\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0],\n",
              "          [1, 1, 1,  ..., 0, 0, 0]])},\n",
              " 'input_ids': {'first_sent': tensor([[  101,  1037,  2450,  ...,     0,     0,     0],\n",
              "          [  101,  3516,  5721,  ...,     0,     0,     0],\n",
              "          [  101,  2611,  4832,  ...,     0,     0,     0],\n",
              "          ...,\n",
              "          [  101,  1037,  2177,  ...,     0,     0,     0],\n",
              "          [  101,  1996,  2158,  ...,     0,     0,     0],\n",
              "          [  101,  2446, 19918,  ...,     0,     0,     0]]),\n",
              "  'second_sent': tensor([[  101,  1037,  2711,  ...,     0,     0,     0],\n",
              "          [  101, 13109,  2050,  ...,     0,     0,     0],\n",
              "          [  101,  1037,  2611,  ...,     0,     0,     0],\n",
              "          ...,\n",
              "          [  101,  1037,  2177,  ...,     0,     0,     0],\n",
              "          [  101,  1037,  2158,  ...,     0,     0,     0],\n",
              "          [  101,  4238,  1005,  ...,     0,     0,     0]])},\n",
              " 'score': tensor([0.0800, 0.5600, 0.4000, 1.0000, 1.0000, 0.5000, 0.5500, 0.4000, 0.6400,\n",
              "         0.7200, 0.9200, 0.7200, 0.6800, 0.6800, 0.7000, 0.2800],\n",
              "        dtype=torch.float64)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYxIsuUb9gbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpfHcjS2j9sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now lets test dataloader with the custom model\n",
        "out_score = custom_model( data['input_ids'],\n",
        "                         data['attention_ids'],\n",
        "                         \n",
        "                         'cosine',\n",
        "                         \n",
        "                        )\n",
        "             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLmub3cy1lTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5db2cfd8-8d4a-49a3-f44f-2b54248ea1e0"
      },
      "source": [
        "out_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7277, 0.7249, 0.7469, 0.7783, 0.8608, 0.8753, 0.8644, 0.8105, 0.8925,\n",
              "        0.7494, 0.8413, 0.8269, 0.6999, 0.8244, 0.7620, 0.7985],\n",
              "       device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaY5s2uBcJN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjNtLyGbqlXE",
        "colab_type": "text"
      },
      "source": [
        "# Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V20oZVOTpzGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "176587eab3ad433ab7e0ed401cef1f3c",
            "f6d8b09fc2584e5582d4801eea771b1f",
            "fdca5cb17d5244708402b6eb6bbcf0e8",
            "7afd408a33334bf0b26c1bd1fa4e0cf1",
            "a02485564bbc4acc80066f6d33769cca",
            "34d7fcaf123c45198f3cdfffde971cb7",
            "f96b1a901abb42e88880a238eaab1866",
            "3ac1e33f014942169639157c7540bdee",
            "878fc8c2e17e44d1abf992e585ffa7a9",
            "be8bb46c2dc640678c65b9dc7ccde5f7",
            "693e7691852d404d81a1a4c14ac20b77",
            "3c044203a4d74a60bb197f4b467b842b",
            "13b62c0c3ede41f49b8218ef75f3fe69",
            "c8b811abb27a42c7b98864ca545d3fc5",
            "9debdbf399504f3882128223b8f9e026",
            "be38de922e0541008c8adca4155676e2"
          ]
        },
        "outputId": "bae4317d-1518-4345-ed64-b2e3c72d1540"
      },
      "source": [
        "#initiate model\n",
        "custom_model = SentenceSimilarityModel()\n",
        "custom_model = custom_model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "176587eab3ad433ab7e0ed401cef1f3c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "878fc8c2e17e44d1abf992e585ffa7a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YevjZFxRe-09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a09bb269-d21d-43ca-f8bb-3edac1567a69"
      },
      "source": [
        "#check list of parameter\n",
        "#for param_tensor in custom_model.state_dict():\n",
        "#   print(param_tensor, \"\\t\", custom_model.state_dict()[param_tensor].size())\n",
        "len(custom_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYAJ_33_fjm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvbYoLWoJyl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "54b67397-d401-4a80-fdb3-74477b8eb824"
      },
      "source": [
        "#check paramerts in model\n",
        "params = list(custom_model.named_parameters())\n",
        "print(len(params))\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print()\n",
        "\n",
        "for p in params[21:21+16]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print()\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "bert.encoder.layer.1.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.1.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.1.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.1.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.1.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.1.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.1.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.1.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.1.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.1.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.1.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.1.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.1.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.1.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "regression.weight                                           (1, 768)\n",
            "regression.bias                                                 (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF1DKIKahNjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl3wYrVccD7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#list of parameters\n",
        "max_length = 60 #max tokens size in a sequence\n",
        "EPOCHS = 4 # number of training epoch\n",
        "\n",
        "optimizer =AdamW(custom_model.parameters(), lr=3e-5, eps=1e-8) # Adam optimizer\n",
        "total_steps = len(train_data_loader_cos) * EPOCHS # total number training steps\n",
        "#learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(  \n",
        "  optimizer,  \n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")  \n",
        "\n",
        "#MSE loss function\n",
        "loss_fn = nn.MSELoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N9aKvyVjmCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7QsUASlg4wS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "9b18d468-0602-4e2b-9b5c-ae1e1957862e"
      },
      "source": [
        "%%time\n",
        "#Training process start\n",
        "epochs_score = defaultdict(list) #store score from each epochs\n",
        "val_score = 0 # validation score\n",
        "model_type = ['regression', 'cosine']\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch {:}/{:}'.format(epoch + 1, EPOCHS))\n",
        "  print('=='*5)\n",
        "  print()\n",
        "  \n",
        "  #model training initiate\n",
        "  train_loss, train_Pearson_score = model_train(custom_model, train_data_loader_cos, device, loss_fn, optimizer, scheduler, model_type[1])\n",
        "  print('Train loss is: {}, train_Pearson_score is {}'.format(train_loss,train_Pearson_score))\n",
        "\n",
        "  #store values\n",
        "  epochs_score['train_loss'].append(train_loss) #train losss\n",
        "  epochs_score['train_Pearson_score'].append(train_Pearson_score) # pearson correlation\n",
        "\n",
        "\n",
        "  #model validatiaon initiate\n",
        "  val_loss, val_Pearson_score = model_eval(custom_model, dev_data_loader_cos, device, loss_fn, model_type[1])\n",
        "  print('Validation loss is: {}, Validation_Pearson_score is {}'.format(val_loss,val_Pearson_score))\n",
        "  print()\n",
        "  \n",
        "  #store values\n",
        "  epochs_score['val_loss'].append(val_loss) #train losss\n",
        "  epochs_score['val_Pearson_score'].append(val_Pearson_score) # pearson correlation\n",
        "\n",
        "  #save model with high validaiton score\n",
        "  if val_Pearson_score >  val_score:\n",
        "    val_score = val_Pearson_score\n",
        "    torch.save(custom_model.state_dict(), 'cos_best_model.bin')\n",
        "\n",
        "print()\n",
        "print('Training completed......')\n",
        "epochs_score\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "==========\n",
            "\n",
            "Model is training based on cosine ouput!!!\n",
            "Train loss is: 0.03525991279424893, train_Pearson_score is 0.7737912409703589\n",
            "Validation loss is: 0.040995525166471584, Validation_Pearson_score is 0.8388022081369747\n",
            "Epoch 2/4\n",
            "==========\n",
            "\n",
            "Model is training based on cosine ouput!!!\n",
            "Train loss is: 0.01583862947170726, train_Pearson_score is 0.9057581402554787\n",
            "Validation loss is: 0.03573798307990457, Validation_Pearson_score is 0.8515228122437515\n",
            "Epoch 3/4\n",
            "==========\n",
            "\n",
            "Model is training based on cosine ouput!!!\n",
            "Train loss is: 0.009391826189433536, train_Pearson_score is 0.9478323482497297\n",
            "Validation loss is: 0.03578938943076324, Validation_Pearson_score is 0.8495049487285798\n",
            "Epoch 4/4\n",
            "==========\n",
            "\n",
            "Model is training based on cosine ouput!!!\n",
            "Train loss is: 0.007055937146974935, train_Pearson_score is 0.9618974993105434\n",
            "Validation loss is: 0.03679277084054465, Validation_Pearson_score is 0.8475719429010564\n",
            "\n",
            "Training completed......\n",
            "CPU times: user 4min 46s, sys: 1min 35s, total: 6min 22s\n",
            "Wall time: 6min 23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6-9vAYzqLFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQXjuYIRIFHe",
        "colab_type": "text"
      },
      "source": [
        "#Plot model learning Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1au3LlrwHE7e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "42c4562e-3ca8-4c51-ace0-f2058fdcc47f"
      },
      "source": [
        "#plot training and validation score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "train_score = epochs_score['train_Pearson_score'] # train_score\n",
        "valid_score = epochs_score['val_Pearson_score'] # valid_score\n",
        " \n",
        "\n",
        "sns.set(style='darkgrid') #set plot style\n",
        "sns.set(rc={'figure.figsize':(9,6)}) # set figure size\n",
        "plt.plot(train_score ,'-o', label= 'Train_score' )#plot train score\n",
        "plt.plot(valid_score, '-o', label='Valid_score') # plot valid score\n",
        "plt.xticks( range(0, EPOCHS, 1)) #set xlabel location\n",
        "plt.title('Model Training with Siamese Network') #  title\n",
        "plt.ylabel('Pearson Coefficient') # ylabel\n",
        "plt.xlabel('No. of epochs') # xlabel\n",
        "plt.ylim([0.6,1]) #set the ylabel limit\n",
        "plt.legend(loc=4) #label \n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfmfngOm4az8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b1bf30e9-7a97-4479-c90f-4692884f7d2a"
      },
      "source": [
        "epochs_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(list,\n",
              "            {'train_Pearson_score': [0.9095253426930726,\n",
              "              0.9507620315805678,\n",
              "              0.9658815727894156,\n",
              "              0.969138802258031],\n",
              "             'train_loss': [0.014850491575068897,\n",
              "              0.008308044855948538,\n",
              "              0.005935145596029341,\n",
              "              0.005526766769859629],\n",
              "             'val_Pearson_score': [0.8596345902946204,\n",
              "              0.8628200338581528,\n",
              "              0.8550283151967495,\n",
              "              0.8579226058427706],\n",
              "             'val_loss': [0.029453454192410758,\n",
              "              0.028082741582964327,\n",
              "              0.028189612146308447,\n",
              "              0.0281864658315131]})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2qKzz2u4g5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x26xOBtG2hLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "41efe8d1-5ef1-43e0-c7ab-c6e7a7cca341"
      },
      "source": [
        "#plot training and validation score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "train_score = epochs_score['train_Pearson_score'] # train_score\n",
        "valid_score = epochs_score['val_Pearson_score'] # valid_score\n",
        " \n",
        "sns.set(style='darkgrid') #set plot style\n",
        "sns.set(rc={'figure.figsize':(9,6)}) # set figure size\n",
        "plt.plot(train_score ,'-o', label= 'Train_score' )#plot train score\n",
        "plt.plot(valid_score, '-o', label='Valid_score') # plot valid score\n",
        "plt.title('Model Training with Regression output') #  title\n",
        "plt.ylabel('Pearson Coefficient') # ylabel\n",
        "plt.xlabel('No. of epochs') # xlabel\n",
        "plt.ylim([0.6,1]) #set the ylabel limit\n",
        "plt.legend(loc=4) #label \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH6c-4v6FmRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW7EMbm8_bLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWMFCXm1P_pj",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl8Vn_bVTF6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19025997-070b-4593-e9bf-fbdef00d0181"
      },
      "source": [
        "#evaluate on test data Siamese Network model\n",
        "test_loss, test_Pearson_score = model_eval(\n",
        "                                      custom_model, \n",
        "                                      test_data_loader_cos, \n",
        "                                      device, \n",
        "                                      loss_fn,\n",
        "                                      model_type[1] #Sieamese Network\n",
        "                                      )\n",
        "print(test_loss, test_Pearson_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03049802704146881 0.8359374742099637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRSkduiuXtsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hCGvi34Mx03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluate on test data on Regression model\n",
        "test_loss, test_Pearson_score = model_eval(\n",
        "                                      custom_model, \n",
        "                                      test_data_loader_reg, \n",
        "                                      device, \n",
        "                                      loss_fn,\n",
        "                                      model_type[0] # Linear Regression output\n",
        "                                      )\n",
        "print(test_loss, test_Pearson_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqMif_hPPveW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd2857e2-bca4-4472-e4b1-ac2f00b64cdd"
      },
      "source": [
        "print(test_loss, test_Pearson_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.025870686771626444 0.847667453528674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6cfDRC6iEan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9aAcR1qhayQ",
        "colab_type": "text"
      },
      "source": [
        "# Save and Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryrceZodhZQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model\n",
        "torch.save(custom_model.state_dict(), '/content/drive/My Drive/Google_Colab/cos_best_model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o52SHrJtQFhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cf41d3a-d0ee-41b8-e9f8-ac22d308ba43"
      },
      "source": [
        "#load model and evaluate on test data\n",
        "fine_tuned_model = SentenceSimilarityModel()\n",
        "fine_tuned_model.load_state_dict(torch.load('/content/drive/My Drive/Google_Colab/cos_best_model.bin'))\n",
        "test_loss, test_Pearson_score = model_eval(\n",
        "                                      fine_tuned_model.to(device),\n",
        "                                      test_data_loader_cos, \n",
        "                                      device, \n",
        "                                      loss_fn,\n",
        "                                      model_type[1]\n",
        "                                      )\n",
        "print(test_loss, test_Pearson_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.030774633539305336 0.8418700599177172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDkQqCXC3qJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy model\n",
        "!cp -r /content/reg_best_model.bin \"/content/drive/My Drive/Google_Colab/cos_best_model.bin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmaLPAeX32iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCfjOQ9LxAMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clear unused GPU\n",
        "with torch.cuda.device('cuda:0'):\n",
        "   torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXV9pXvpxGhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f9365595-c7f8-409b-deb7-a63777b20051"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul  3 16:16:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    58W / 149W |   7417MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1HQwbw7wQTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Taj_czZksJJL",
        "colab_type": "text"
      },
      "source": [
        "# References:\n",
        "1. Chris McCormick and Nick Ryan. (2019, May 14). BERT Word Embeddings Tutorial. Retrieved from http://www.mccormickml.com , https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
        "\n",
        "2. Sentiment Analysis with BERT  https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=1zhHoFNsxufs&forceEdit=true&sandboxMode=true\n",
        "\n",
        "3. Hugging Face Transformer, https://huggingface.co/transformers/\n",
        "\n",
        "4. The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning), https://jalammar.github.io/illustrated-bert/\n",
        "\n",
        "5. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805\n",
        "\n",
        "6. Eneko Agirre, Daniel Cer, Mona Diab, Iñigo Lopez-Gazpio, Lucia\n",
        "Specia. Semeval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. Proceedings of SemEval 2017\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7hEFPesSfPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLI1w0RfSgMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asAfP6C0Sg99",
        "colab_type": "text"
      },
      "source": [
        "#ELMo Embedding\n",
        "We will use allennlp to extract elmo embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in46LZ9HU424",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "e526873d-1bcf-45c6-94a7-c69d397a73bd"
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: spacy<2.3,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: torch<1.6.0,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.5.1+cu101)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: overrides==3.0.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.14.20)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.7)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: transformers<2.12,>=2.9 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.11.0)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (49.1.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (0.7.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.6.0,>=1.5.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.16.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.20 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.17.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.7.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<2.12,>=2.9->allennlp) (20.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers<2.12,>=2.9->allennlp) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers<2.12,>=2.9->allennlp) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<2.12,>=2.9->allennlp) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<2.12,>=2.9->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->allennlp) (0.15.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<2.12,>=2.9->allennlp) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<2.12,>=2.9->allennlp) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K12FdcJkU4zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Iterable, List\n",
        "from overrides import overrides\n",
        "from allennlp.data import DatasetReader, Instance, DataLoader, Vocabulary\n",
        "from allennlp.data.fields import LabelField, TextField, ArrayField\n",
        "from allennlp.data.fields.text_field import TextFieldTensors\n",
        "from allennlp.data.tokenizers import SpacyTokenizer, Token, Tokenizer, WhitespaceTokenizer\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
        "from allennlp.data.samplers import BasicBatchSampler, BucketBatchSampler, RandomSampler\n",
        "from allennlp.modules import Seq2VecEncoder\n",
        "from allennlp.training.metrics import PearsonCorrelation\n",
        "from allennlp.training.util import evaluate\n",
        "from allennlp.nn import util\n",
        "from allennlp.models import Model\n"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv3LHNwWfura",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiJZrgEdfwE3",
        "colab_type": "text"
      },
      "source": [
        "#Prepare Dataset\n",
        "\n",
        "Create dataset reader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9QJ6ILEVcSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SemanticTextDataReader(DatasetReader):\n",
        "  def __init__(self, \n",
        "               lazy: bool = False, \n",
        "               tokenizer = None, \n",
        "               token_indexers: Dict[str, TokenIndexer] = None, \n",
        "               max_tokens: int = None\n",
        "               ):\n",
        "    super().__init__(lazy)\n",
        "    self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "    self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
        "    self.max_tokens = max_tokens\n",
        "    \n",
        "\n",
        "  #convert given text into instance\n",
        "  @overrides\n",
        "  def text_to_instance(self, sent_1: str, sent_2: str, gold_score: float = None) -> Instance:\n",
        "    \n",
        "    #tokenize text\n",
        "    token_1 = self.tokenizer.tokenize(sent_1) \n",
        "    token_2 = self.tokenizer.tokenize(sent_2)\n",
        "    \n",
        "    #tokens\n",
        "    if self.max_tokens:\n",
        "            token_1 = token_1[:self.max_tokens]\n",
        "            token_2 = token_2[:self.max_tokens]\n",
        "    \n",
        "    #Textfield\n",
        "    text_field_1 = TextField(token_1, self.token_indexers) \n",
        "    text_field_2 = TextField(token_2, self.token_indexers)\n",
        "\n",
        "    #fields contain 'Textfield' and 'LabelField'\n",
        "    fields = {'first_sent': text_field_1, 'second_sent': text_field_2 } \n",
        "    \n",
        "    #check score/label is given or not\n",
        "    if gold_score is not None:\n",
        "      fields['score'] = ArrayField(np.array([gold_score])) # labelfield\n",
        "   \n",
        "    return Instance(fields) #instance with inputs and outputs fields \n",
        "\n",
        "    \n",
        "  #Read dataset and convert them to Iterable Instance\n",
        "  @overrides\n",
        "  def _read(self, file_path: str) -> Iterable[Instance]:\n",
        "    \n",
        "    #read data with pandas\n",
        "    df = pd.read_csv(file_path, \n",
        "              delimiter=',' , \n",
        "              header= None,\n",
        "              names= col_names\n",
        "              )\n",
        "    \n",
        "    #scale down gold-score(0-5) to (0-1) since cosine-simialrity score(0-1)\n",
        "    df['scaled_score(0-1)'] = MinMaxScaler(feature_range=(0,1)).fit_transform(df[['score(0-5)']])\n",
        "    print('Data reading started .....')\n",
        "    \n",
        "    \n",
        "    #iterate over rows in df\n",
        "    for index, row in df.iterrows():\n",
        "      sent_1 = row['sentence_1']#first sent\n",
        "      sent_2 = row['sentence_2']#second sent\n",
        "      gold_score = row['scaled_score(0-1)']#score\n",
        "      yield self.text_to_instance(sent_1, sent_2, gold_score) #iterable\n",
        "\n",
        "    print('...... Completed.')\n",
        "    print()\n",
        "  \n",
        "    "
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhlfJYT52imD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBxOBKkZlzDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "62094fc9-d3e1-41da-a367-f185ebf692df"
      },
      "source": [
        "#Read dataset\n",
        "file_path = \"/content/drive/My Drive/Google_Colab/stsbenchmark_dataset/\" #file path\n",
        "dataset_types = [ \"sts-train.csv\", \"sts-dev.csv\", \"sts-test.csv\",] # 3 datasets\n",
        "col_names = [\"genre\", \"file\", \"years\", \"_\", \"score(0-5)\", \"sentence_1\", \"sentence_2\"] #columns names\n",
        "\n",
        "text_tokenizer = SpacyTokenizer() #tokenizer\n",
        "elmo_indexer = ELMoTokenCharactersIndexer() #character Indexer\n",
        "\n",
        "dataset_reader = SemanticTextDataReader(tokenizer= text_tokenizer,  token_indexers={'tokens': elmo_indexer}, max_tokens=70) #initiate dataset reader\n",
        "train_instances, dev_instances, test_instances = (dataset_reader.read(os.path.join(file_path, file_name)) for file_name in dataset_types)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "143it [00:00, 1425.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data reading started .....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5749it [00:03, 1626.39it/s]\n",
            "251it [00:00, 2503.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...... Completed.\n",
            "\n",
            "Data reading started .....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1500it [00:00, 1705.45it/s]\n",
            "238it [00:00, 2370.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...... Completed.\n",
            "\n",
            "Data reading started .....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1379it [00:00, 1934.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...... Completed.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H__kk72G_MML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xiahAXd_L5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Big6eELKI4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "bcef8cd1-37a8-44aa-fc2b-913c6e640be8"
      },
      "source": [
        "#check the dataset instances\n",
        "for x in train_instances[:2]:\n",
        "  print(x)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance with fields:\n",
            " \t first_sent: TextField of length 6 with text: \n",
            " \t\t[A, plane, is, taking, off, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
            " \t second_sent: TextField of length 7 with text: \n",
            " \t\t[An, air, plane, is, taking, off, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
            " \t score: ArrayField with shape: (1,) and dtype: <class 'numpy.float32'>. \n",
            "\n",
            "Instance with fields:\n",
            " \t first_sent: TextField of length 8 with text: \n",
            " \t\t[A, man, is, playing, a, large, flute, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
            " \t second_sent: TextField of length 7 with text: \n",
            " \t\t[A, man, is, playing, a, flute, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
            " \t score: ArrayField with shape: (1,) and dtype: <class 'numpy.float32'>. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtbHTDPsCmXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-vUUHPbPG7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vars(train_instances[1].fields['text_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltOXFnyUPGr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb22027a-3863-4c2b-831b-f5024da24913"
      },
      "source": [
        "train_instances[1]['text_1'][:]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[A, man, is, playing, a, large, flute, .]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBZIwZXwV69V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO7-P317Ut3T",
        "colab_type": "text"
      },
      "source": [
        "#Prepare Vocabulary and Data iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkzy4ErF4Pvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare vocabulary and data iterator\n",
        "vocab = Vocabulary() #initiate vocabulary object\n",
        "train_instances.index_with(vocab) #index tokens \n",
        "dev_instances.index_with(vocab)\n",
        "test_instances.index_with(vocab)\n",
        "\n",
        "#random sample and batch train data\n",
        "sampler_tr = RandomSampler(data_source=train_instances) \n",
        "batch_sampler_tr = BasicBatchSampler(sampler_tr, batch_size=32, drop_last=False) \n",
        "train_data_loader = DataLoader(train_instances, batch_sampler= batch_sampler_tr)\n",
        "\n",
        "#random sample and batch dev data\n",
        "sampler_dev = RandomSampler(data_source=dev_instances) \n",
        "batch_sampler_dev = BasicBatchSampler(sampler_dev, batch_size=32, drop_last=False)\n",
        "dev_data_loader = DataLoader(dev_instances, batch_sampler=batch_sampler_dev) #dev data\n",
        "\n",
        "\n",
        "#random sample and batch test data\n",
        "sampler_test = RandomSampler(data_source=test_instances) \n",
        "batch_sampler_test = BasicBatchSampler(sampler_test, batch_size=32, drop_last=False)\n",
        "test_data_loader = DataLoader(test_instances, batch_sampler= batch_sampler_test) \n",
        "\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx1bHpNjhFKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5e7208f-bc98-4673-bcdf-7b4977504e14"
      },
      "source": [
        "#lets look test data loader \n",
        "batch = next(iter(test_data_loader))\n",
        "batch\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first_sent': {'tokens': {'elmo_tokens': tensor([[[259,  85, 105,  ..., 261, 261, 261],\n",
              "            [259, 100,  98,  ..., 261, 261, 261],\n",
              "            [259,  99, 102,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  66, 260,  ..., 261, 261, 261],\n",
              "            [259, 110,  98,  ..., 261, 261, 261],\n",
              "            [259, 116, 106,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  73,  98,  ..., 261, 261, 261],\n",
              "            [259, 113,  98,  ..., 261, 261, 261],\n",
              "            [259, 104,  98,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           ...,\n",
              "   \n",
              "           [[259,  66, 106,  ..., 261, 261, 261],\n",
              "            [259, 116, 117,  ..., 261, 261, 261],\n",
              "            [259, 108, 106,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  66, 260,  ..., 261, 261, 261],\n",
              "            [259,  99, 109,  ..., 261, 261, 261],\n",
              "            [259,  98, 111,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  66, 260,  ..., 261, 261, 261],\n",
              "            [259, 113, 102,  ..., 261, 261, 261],\n",
              "            [259, 106, 116,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]]])}},\n",
              " 'score': tensor([[0.0000],\n",
              "         [0.0000],\n",
              "         [0.2000],\n",
              "         [0.4500],\n",
              "         [0.6000],\n",
              "         [0.0000],\n",
              "         [0.0800],\n",
              "         [1.0000],\n",
              "         [0.4000],\n",
              "         [0.2400],\n",
              "         [0.6400],\n",
              "         [0.4000],\n",
              "         [0.2000],\n",
              "         [0.7334],\n",
              "         [0.7600],\n",
              "         [0.5200],\n",
              "         [1.0000],\n",
              "         [0.4000],\n",
              "         [0.4800],\n",
              "         [0.4000],\n",
              "         [0.5000],\n",
              "         [0.4000],\n",
              "         [1.0000],\n",
              "         [0.1500],\n",
              "         [0.0000],\n",
              "         [0.2000],\n",
              "         [0.1200],\n",
              "         [0.1000],\n",
              "         [0.7600],\n",
              "         [0.5600],\n",
              "         [0.4000],\n",
              "         [0.7500]]),\n",
              " 'second_sent': {'tokens': {'elmo_tokens': tensor([[[259,  90, 102,  ..., 261, 261, 261],\n",
              "            [259,  45, 260,  ..., 261, 261, 261],\n",
              "            [259,  98, 109,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  80, 100,  ..., 261, 261, 261],\n",
              "            [259, 109, 106,  ..., 261, 261, 261],\n",
              "            [259, 100, 109,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  86,  84,  ..., 261, 261, 261],\n",
              "            [259,  84, 102,  ..., 261, 261, 261],\n",
              "            [259, 113,  98,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           ...,\n",
              "   \n",
              "           [[259,  86,  84,  ..., 261, 261, 261],\n",
              "            [259, 101, 115,  ..., 261, 261, 261],\n",
              "            [259, 116, 117,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  66, 260,  ..., 261, 261, 261],\n",
              "            [259, 120, 105,  ..., 261, 261, 261],\n",
              "            [259, 101, 112,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]],\n",
              "   \n",
              "           [[259,  66, 260,  ..., 261, 261, 261],\n",
              "            [259, 113, 102,  ..., 261, 261, 261],\n",
              "            [259, 106, 116,  ..., 261, 261, 261],\n",
              "            ...,\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0],\n",
              "            [  0,   0,   0,  ...,   0,   0,   0]]])}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiPtzrJ51inZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C6PRnkbg8xT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f9168262-60f7-49ce-8684-fb63f4ebd1b5"
      },
      "source": [
        "#a single batch contains collection of tokens and  single token is ocnverted into array of  character ids of 50 size\n",
        "print('Batch shape :', batch['first_sent']['tokens']['elmo_tokens'].shape) #(batch size, token length, char dim)\n",
        "print()\n",
        "print('tokens in a sequence: ')\n",
        "batch['first_sent']['tokens']['elmo_tokens'][0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch shape : torch.Size([32, 28, 50])\n",
            "\n",
            "tokens in a sequence: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[259,  85, 105,  ..., 261, 261, 261],\n",
              "        [259, 100,  98,  ..., 261, 261, 261],\n",
              "        [259,  99, 102,  ..., 261, 261, 261],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ...,   0,   0,   0],\n",
              "        [  0,   0,   0,  ...,   0,   0,   0],\n",
              "        [  0,   0,   0,  ...,   0,   0,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWN5GwZmgfQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWZBW3Zk_Wql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab5f71c4-0b96-4280-f88e-6bc274a1804e"
      },
      "source": [
        " \n",
        "# token indexer is responsible for mapping tokens to token id\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder\n",
        "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
        "\n",
        "weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
        "options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n",
        " \n",
        "#Two ways  ELMo representation can be computated\n",
        "# 1. ElmoTokenEmbeder \n",
        "elmo_embedding = ElmoTokenEmbedder(options_file = options_file, weight_file = weight_file, requires_grad = True)\n",
        "word_embedding = BasicTextFieldEmbedder({\"elmo_tokens\": elmo_embedding}) \n",
        "\n",
        "#2. Elmo module\n",
        "elmo_embedder = Elmo( options_file, \n",
        "                     weight_file,\n",
        "                     num_output_representations =1,\n",
        "                     requires_grad = True,\n",
        "                     ) \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 259776.25B/s]\n",
            "100%|██████████| 374434792/374434792 [00:25<00:00, 14868272.85B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q85YttKVT4qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
        "encoder: Seq2VecEncoder = PytorchSeq2VecWrapper(nn.LSTM(word_embedding.get_output_dim(), 64, bidirectional=True, batch_first=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUsK9K5Wuffd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1cf8d8a-22af-4e98-91b1-c2a10a8cc6ec"
      },
      "source": [
        "type(word_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "allennlp.modules.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4CeXw8uufKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHzmENF1Teiv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "b186ee06-8c99-4bb0-a186-990bebaba389"
      },
      "source": [
        "\n",
        "# use batch_to_ids to convert tokens into character ids and padding in a batch\n",
        "sentences = [['First', 'sentence', '.'], ['First', 'sentence', '.', 'hello']]\n",
        "character_ids = batch_to_ids(sentences) # (len(batch), max sentence length, max word length)\n",
        "print(character_ids)\n",
        "\n",
        "\n",
        "#tokens embeddig with mask\n",
        "embeddings_ = elmo_embedder(character_ids)\n",
        "#elmo_embedder = elmo_embedder.to(device)\n",
        "#embeddings_ = elmo_embedder(character_ids.to(device) )\n",
        "print('\\nEmbedded vectors with mask: \\n')\n",
        "embeddings_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[259,  71, 106, 115, 116, 117, 260, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261],\n",
            "         [259, 116, 102, 111, 117, 102, 111, 100, 102, 260, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261],\n",
            "         [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261],\n",
            "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "            0,   0,   0,   0,   0,   0,   0,   0]],\n",
            "\n",
            "        [[259,  71, 106, 115, 116, 117, 260, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261],\n",
            "         [259, 116, 102, 111, 117, 102, 111, 100, 102, 260, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261],\n",
            "         [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261],\n",
            "         [259, 105, 102, 109, 109, 112, 260, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
            "          261, 261, 261, 261, 261, 261, 261, 261]]])\n",
            "\n",
            "Embedded vectors with mask: \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'elmo_representations': [tensor([[[ 0.0000, -0.0000, -0.0000,  ...,  0.0343, -0.8414, -0.0000],\n",
              "           [ 0.0000,  0.0000,  0.6081,  ..., -0.0000, -0.0000,  0.0000],\n",
              "           [-0.0000, -0.0000, -1.4011,  ..., -0.1939,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "  \n",
              "          [[ 0.0000, -0.3342, -0.0000,  ..., -0.0000, -0.8025,  0.3477],\n",
              "           [ 0.0000,  0.0028,  0.0000,  ..., -0.5260, -0.3161,  0.7929],\n",
              "           [-1.5470, -1.0505, -1.3284,  ..., -0.3676,  0.5629,  0.7064],\n",
              "           [ 0.0000, -1.6153, -0.1053,  ..., -1.1243,  0.0000,  0.2757]]],\n",
              "         grad_fn=<MulBackward0>)],\n",
              " 'mask': tensor([[ True,  True,  True, False],\n",
              "         [ True,  True,  True,  True]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1GX5gKo2nv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSG9BXH7GH3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "c5b58bdf-ae8a-460a-9cbd-2524f9410d93"
      },
      "source": [
        "elmo_embedder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Elmo(\n",
              "  (_elmo_lstm): _ElmoBiLm(\n",
              "    (_token_embedder): _ElmoCharacterEncoder(\n",
              "      (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
              "      (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
              "      (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
              "      (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
              "      (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
              "      (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
              "      (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
              "      (_highways): Highway(\n",
              "        (_layers): ModuleList(\n",
              "          (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
              "          (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
              "    )\n",
              "    (_elmo_lstm): ElmoLstm(\n",
              "      (forward_layer_0): LstmCellWithProjection(\n",
              "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
              "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
              "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
              "      )\n",
              "      (backward_layer_0): LstmCellWithProjection(\n",
              "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
              "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
              "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
              "      )\n",
              "      (forward_layer_1): LstmCellWithProjection(\n",
              "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
              "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
              "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
              "      )\n",
              "      (backward_layer_1): LstmCellWithProjection(\n",
              "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
              "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
              "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (scalar_mix_0): ScalarMix(\n",
              "    (scalar_parameters): ParameterList(\n",
              "        (0): Parameter containing: [torch.FloatTensor of size 1]\n",
              "        (1): Parameter containing: [torch.FloatTensor of size 1]\n",
              "        (2): Parameter containing: [torch.FloatTensor of size 1]\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltScQLmfvcqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCOtEP4PvcXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNdpdSjE_ZF",
        "colab_type": "text"
      },
      "source": [
        "#Extract Sentence Level ELMo Embedding \n",
        "Calculate cosine similarity between a pair of sentence, and Pearson score and Spearman score between cosine similarity and gold score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OxW4fv433Ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6) # for cosine similarity\n",
        "score = defaultdict(list) #store score\n",
        "\n",
        "#iterate over dev and test data loader\n",
        "for name, each_data_loader in zip(['dev', 'test'], [dev_data_loader, test_data_loader]):\n",
        "   \n",
        "  pear_score_list = [] #pearson score\n",
        "  sperman_score_list = [] # sperman score \n",
        "\n",
        "  #iterate over each batchs\n",
        "  for batch in each_data_loader:\n",
        "    \n",
        "    #first sentence\n",
        "    elmo_embeding_1 = elmo_embedder(batch['first_sent']['tokens']['elmo_tokens']) #return token embeding and mask\n",
        "    token_level_embedding_1 = elmo_embeding_1['elmo_representations'][0] # token embedding\n",
        "    sent_level_embedding_1 = token_level_embedding_1.mean(dim=1) # tokens mean pooling\n",
        "    \n",
        "    #second sentence\n",
        "    elmo_embeding_2 = elmo_embedder(batch['second_sent']['tokens']['elmo_tokens'])\n",
        "    token_level_embedding_2 = elmo_embeding_2['elmo_representations'][0]\n",
        "    sent_level_embedding_2 = token_level_embedding_2.mean(dim=1)\n",
        "\n",
        "    #gold score\n",
        "    gold_score = batch['score']\n",
        "    gold_score = gold_score.flatten() # 1d\n",
        "    score[name+'_gold_score'].extend(gold_score) #extend to list\n",
        "    \n",
        "    #calculate cosine similarity between a pair of sentences\n",
        "    cosine_sim = cos(sent_level_embedding_1, sent_level_embedding_2)\n",
        "    cosine_sim = cosine_sim.detach() #numpy\n",
        "    score[name+'_cosine_score'].extend(cosine_sim) #extend to list\n",
        "    \n",
        "    #calcualte Pearson score\n",
        "    person_score, _ = pearsonr(gold_score, cosine_sim) \n",
        "    pear_score_list.append(person_score)\n",
        "  \n",
        "    #calcualte Spearman score\n",
        "    sperman_score, _ = spearmanr(gold_score, cosine_sim) \n",
        "    sperman_score_list.append(sperman_score)\n",
        "  \n",
        "  print('Score in ',name)\n",
        "  print('=='*5)\n",
        "  print('Pearson score: {}'.format(np.mean(pear_score_list)))\n",
        "  print('Spearman score: {}'.format(np.mean(sperman_score_list)))\n",
        "  print()\n",
        "  \n",
        "print('Score in whole data with out batching')\n",
        "print('dev_set score')\n",
        "print('=='*5)\n",
        "print('Pearson score: {}'.format(pearsonr(score['dev_gold_score'], score['dev_cosine_score'])[0]))\n",
        "print('Spearman score: {}'.format(spearmanr(score['dev_gold_score'], score['dev_cosine_score'])[0]))\n",
        "print()\n",
        "print('test_set score')\n",
        "print('=='*5)\n",
        "print('Pearson score: {}'.format(pearsonr(score['test_gold_score'], score['test_cosine_score'])[0]))\n",
        "print('Spearman score: {}'.format(spearmanr(score['test_gold_score'], score['test_cosine_score'])[0]))\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k8STBc9WiWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH5bAMazGRr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define function to extract elmo embedding\n",
        "def elmo_feature_embedding(data: [str], \n",
        "                           elmo_embedder: BasicTextFieldEmbedder,\n",
        "                           tokenizer: Tokenizer, \n",
        "                           token_indexer: ELMoTokenCharactersIndexer, \n",
        "                           vocab: Vocabulary\n",
        "                           ):\n",
        "  '''\n",
        "  Extract elmo embedding of given sequence.\n",
        "\n",
        "    Args:\n",
        "      data(list): list of strings/sequences\n",
        "      elmo_embedder: allennlp elmo TextField embedder object\n",
        "      tokenizer: allennlp SpacyTokenizer object\n",
        "      token_indexer: allennlp ELMoTokenCharactersIndexer object\n",
        "      vocab: allennlp vocabualry object\n",
        "\n",
        "    Returns:\n",
        "       Sentence embedding(2d) of 1024 dimensions\n",
        "  \n",
        "  '''\n",
        "\n",
        "  #empty array to store elmo embedding vectors\n",
        "  sent_embedded = np.empty((0, 1024)) # size(,1024) \n",
        "  \n",
        "  #iterate over sequence \n",
        "  for sequence in data:\n",
        "    tokens = tokenizer.tokenize(sequence) # tokenize sequence\n",
        "    text_field = TextField(tokens, {'elmo_tokens': token_indexer}) #tokens to ELMoTokenCharactersIndexer\n",
        "    text_field.index(vocab) #character_ids based on vocab\n",
        "    padding_lengths = text_field.get_padding_lengths() # get tokens length\n",
        "    tensor_dict = text_field.as_tensor(padding_lengths) #change to TextFieldTensors\n",
        "    batch_tensor_dict = text_field.batch_tensors([tensor_dict]) # batch_tensor\n",
        "    elmo_embedded = elmo_embedder(batch_tensor_dict)# get token embedding\n",
        "    mean_tokens_embed = elmo_embedded.mean(dim=1)# mean pooling over tokens\n",
        "    sent_embedded = np.append(sent_embedded, mean_tokens_embed.detach().numpy(), axis=0) # append vertically\n",
        "    \n",
        "  return sent_embedded\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTPJ5uLBxo-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FKPvRzXHWKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initiate necessary class object\n",
        "text_tokenizer = SpacyTokenizer() #tokenizer\n",
        "elmo_indexer = ELMoTokenCharactersIndexer() #character Indexer\n",
        "vocab = Vocabulary()# initiate vocab\n",
        "\n",
        "elmo_token_embedder = ElmoTokenEmbedder(options_file, weight_file) #elmo token embedder\n",
        "elmo_textfield_embedder = BasicTextFieldEmbedder({\"elmo_tokens\": elmo_token_embedder}) #textfield embedder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMiE3mGAyfwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ff3a3cdd-5c4b-433b-d81d-7696bfc2df67"
      },
      "source": [
        "%%time\n",
        "#extract embedding\n",
        "dev_first_sent_emlo_embeded = elmo_feature_embedding(df_dev.sentence_1.values.tolist(), elmo_textfield_embedder, text_tokenizer, elmo_indexer, vocab )\n",
        "dev_second_sent_emlo_embeded = elmo_feature_embedding(df_dev.sentence_2.values.tolist(), elmo_textfield_embedder, text_tokenizer, elmo_indexer, vocab )\n",
        "test_first_sent_emlo_embeded = elmo_feature_embedding(df_test.sentence_1.values.tolist(), elmo_textfield_embedder, text_tokenizer, elmo_indexer, vocab )\n",
        "test_second_sent_emlo_embeded = elmo_feature_embedding(df_test.sentence_2.values.tolist(), elmo_textfield_embedder, text_tokenizer, elmo_indexer, vocab )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 25min 9s, sys: 1.2 s, total: 25min 10s\n",
            "Wall time: 25min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm_83Yv6H-cC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b777528d-1233-4b9a-d10d-ffd7239acec1"
      },
      "source": [
        "#calcualte cosine similarity, pearson and spearman score\n",
        "elmo_pear_spear_score = defaultdict(list) #store scores\n",
        "dev_cos = calc_similarity_score(dev_first_sent_emlo_embeded, dev_second_sent_emlo_embeded).flatten()\n",
        "dev_pear, dev_spear = cal_pearson_spearman_score(dev_cos, df_dev.loc[:,'score(0-5)'].values )\n",
        "elmo_pear_spear_score['dev'].append(dev_pear)\n",
        "elmo_pear_spear_score['dev'].append(dev_pear)\n",
        "print('Dev data: Pearson score: {}, Spearman score: {}'.format(dev_pear,dev_spear))\n",
        "\n",
        "test_cos = calc_similarity_score(test_first_sent_emlo_embeded, test_second_sent_emlo_embeded).flatten()\n",
        "test_pear, test_spear = cal_pearson_spearman_score(test_cos, df_test.loc[:,'score(0-5)'].values )\n",
        "print('Test data: Pearson score: {}, Spearman score: {}'.format(test_pear,test_spear))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dev data: Pearson score: 0.6156837140767895, Spearman score: 0.6228056398813422\n",
            "Test data: Pearson score: 0.4802520423636257, Spearman score: 0.4572175016552449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3FRDsil3Ff7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjpKxtQqxD0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "547a6804-8f1c-4f18-d7ce-fb2da585a2c0"
      },
      "source": [
        "#show in panda dataframe\n",
        "pd.DataFrame.from_dict({'Pearson': [dev_pear, test_pear ] , 'Spearman': [ dev_spear,test_spear ]},\n",
        "                          orient='index',\n",
        "                          columns=['Dev_Data', 'Test_Data']\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dev_Data</th>\n",
              "      <th>Test_Data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Pearson</th>\n",
              "      <td>0.615684</td>\n",
              "      <td>0.480252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Spearman</th>\n",
              "      <td>0.622806</td>\n",
              "      <td>0.457218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Dev_Data  Test_Data\n",
              "Pearson   0.615684   0.480252\n",
              "Spearman  0.622806   0.457218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMVMz2NVOc8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxmTgrBAt2T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4tyyAHxt226",
        "colab_type": "text"
      },
      "source": [
        "#ELMO Fine-Tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02233QrFkE5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define custom model for elmo fine-tuning\n",
        "class ElmoSentenceSimilarityModel(Model):\n",
        "  \"\"\" \n",
        "  Extract sentence embedding by meaning pooling over tokens in a sentence.\n",
        "  And, Calculate cosine similarity between extracted embeding of a pair of sentences.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, elmo_embedding: BasicTextFieldEmbedder, vocab: Vocabulary):\n",
        "    super().__init__(vocab)\n",
        "    self.elmo_embedding = elmo_embedding.to(device)\n",
        "    self.pearson = PearsonCorrelation()\n",
        "    self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    self.loss = nn.MSELoss()\n",
        "    \n",
        "  #produce prediction, and computes the loss\n",
        "  def forward(self, first_sent: TextFieldTensors, second_sent: TextFieldTensors, score: torch.Tensor):\n",
        "    \n",
        "\n",
        "    #first sentence embedding\n",
        "    first_token_ids = first_sent['tokens']['elmo_tokens'].to(device) #move to gpu\n",
        "    first_embedding = self.elmo_embedding(first_token_ids) #first sentence\n",
        "    first_tokens_embed = first_embedding['elmo_representations'][0] #token embedding\n",
        "    first_sent_embed = first_tokens_embed.mean(dim=1) # sentence embedding --> mean pooling over tokens\n",
        "    \n",
        "    #second sentence embedding\n",
        "    second_token_ids = second_sent['tokens']['elmo_tokens'].to(device)\n",
        "    second_embedding = self.elmo_embedding(second_token_ids) #second sentence\n",
        "    second_tokens_embed = second_embedding['elmo_representations'][0] \n",
        "    second_sent_embed = second_tokens_embed.mean(dim=1)\n",
        "\n",
        "    #cosine similarity\n",
        "    cosine_sim = self.cos(first_sent_embed, second_sent_embed)\n",
        "    \n",
        "    #check if glod label is given\n",
        "    if score is not None:\n",
        "       gold_score = score.to(device).flatten() #gold score\n",
        "\n",
        "       #compute loss\n",
        "       loss = self.loss(cosine_sim, gold_score) \n",
        "\n",
        "       #compute metric (pearson correlation)\n",
        "       self.pearson(cosine_sim, gold_score) \n",
        "    \n",
        "    #output \n",
        "    output = {'loss': loss, 'cos_sim': cosine_sim.detach().cpu().numpy() }\n",
        "\n",
        "    return output\n",
        "  \n",
        "  #get metric score\n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        return {'pearson(r)': self.pearson.get_metric(reset)}\n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq-yNTFy_Oen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm9fQ19ITFMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch = next(iter(train_data_loader))\n",
        "batch = util.move_to_device(batch, 0)\n",
        "batch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNlHMHmwRTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0abb3ada-a28a-4441-a8bd-67021d7cb550"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "emodel(batch['first_sent'], \n",
        "       batch['second_sent'], \n",
        "       batch['score']\n",
        "       )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cos_sim': array([ 0.84641093,  0.22871672,  0.46762115,  0.9001056 ,  0.8584955 ,\n",
              "         0.4211528 ,  0.41627073,  0.88218176,  0.5953551 ,  0.86244404,\n",
              "         0.82402235,  0.89177275,  0.8333887 ,  0.4143513 ,  0.34576544,\n",
              "         0.70232004,  0.6211435 ,  0.19726954,  0.91380817,  0.6690772 ,\n",
              "         0.69988114,  0.9280833 ,  0.6804008 ,  0.8507508 ,  0.31050768,\n",
              "         0.78284997,  0.86650854,  0.76835245,  0.88610476,  0.4545058 ,\n",
              "         0.82804865, -0.03797304], dtype=float32),\n",
              " 'loss': tensor(0.0484, device='cuda:0', grad_fn=<MseLossBackward>)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwXj3mvDf_W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_embedder = Elmo( options_file, \n",
        "                     weight_file,\n",
        "                     num_output_representations =1,\n",
        "                     requires_grad = True,\n",
        "                     ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEYvA_epYcv0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b982daa3-9e5f-45f1-9c9b-ee632c3b3e74"
      },
      "source": [
        "\n",
        "#elmo_embedderr = elmo_embedder.to(device)\n",
        "emodel(batch['first_sent'],\n",
        "       batch['second_sent'],\n",
        "       batch['score']\n",
        "       )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cos_sim': array([0.5463659 , 0.62613887, 0.68439764, 0.5734607 , 0.5786696 ,\n",
              "        0.60487163, 0.7261458 , 0.55614525, 0.6030174 , 0.54711515,\n",
              "        0.6663646 , 0.60272706, 0.5505538 , 0.65301967, 0.59795034,\n",
              "        0.57376724, 0.6265677 , 0.5509161 , 0.74961257, 0.5938232 ,\n",
              "        0.6433973 , 0.47192073, 0.430308  , 0.53487307, 0.6100597 ,\n",
              "        0.62518144, 0.54516923, 0.59951353, 0.69369376, 0.5142402 ,\n",
              "        0.6328629 , 0.3902581 ], dtype=float32),\n",
              " 'loss': array(0.08893989, dtype=float32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVXdmVakiK4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW8Z9TNJf9OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzqawdtHwQ-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "51dbd4db-edad-42f2-e42c-8ff2577e4477"
      },
      "source": [
        "from allennlp.training.trainer import Trainer, GradientDescentTrainer\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "emodel = ElmoSentenceSimilarityModel(elmo_embedder, vocab)\n",
        "emodel = emodel.to(device)#gpu\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(emodel.parameters())\n",
        "serialization_dir = '/content/drive/My Drive/Google_Colab/Elmo'\n",
        "\n",
        "#training setup\n",
        "trainer = GradientDescentTrainer( \n",
        "    model=emodel,\n",
        "    serialization_dir=serialization_dir,\n",
        "    data_loader = train_data_loader,\n",
        "    validation_data_loader = train_data_loader,\n",
        "    num_epochs = 10,\n",
        "    optimizer = optimizer,\n",
        "    cuda_device = 0,\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-29c04d7dc6d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElmoSentenceSimilarityModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melmo_embedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0memodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ElmoSentenceSimilarityModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liKDzlD0U8n9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCSIaPkeU8Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import logging\n",
        "#logging.getLogger().setLevel(logging.CRITICAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iWzpzYbU7hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh9ZheDioxjD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d30ecd-2ffe-4168-b8ce-571016bbd593"
      },
      "source": [
        "print(\"Starting training\\n\")\n",
        "train = trainer.train()\n",
        "print(\"\\nFinished training\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "pearson(r): 0.5400, loss: 0.0793, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:01<03:43,  1.25s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.4350, loss: 0.0784, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:02<03:57,  1.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.4734, loss: 0.0695, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:03<03:30,  1.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5136, loss: 0.0710, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:05<04:06,  1.40s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5184, loss: 0.0639, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:07<04:11,  1.44s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5139, loss: 0.0665, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:09<04:53,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5484, loss: 0.0615, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:10<04:39,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5473, loss: 0.0618, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:12<04:31,  1.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5632, loss: 0.0614, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:13<04:17,  1.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5629, loss: 0.0600, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:14<04:08,  1.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5587, loss: 0.0594, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:16<03:58,  1.41s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5541, loss: 0.0607, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:17<04:10,  1.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5655, loss: 0.0604, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:20<05:13,  1.88s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5641, loss: 0.0592, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:22<05:23,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5836, loss: 0.0585, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:23<04:42,  1.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5859, loss: 0.0582, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:26<04:59,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5869, loss: 0.0580, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:27<04:46,  1.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6018, loss: 0.0565, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:29<04:39,  1.72s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5960, loss: 0.0568, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:30<04:27,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5928, loss: 0.0564, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:32<04:06,  1.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.5947, loss: 0.0554, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:33<03:59,  1.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6080, loss: 0.0547, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:34<03:36,  1.37s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6072, loss: 0.0543, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:35<03:30,  1.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6152, loss: 0.0540, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:37<03:31,  1.35s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6079, loss: 0.0543, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:38<03:48,  1.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6078, loss: 0.0545, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:40<03:47,  1.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6098, loss: 0.0546, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:41<03:39,  1.44s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6124, loss: 0.0543, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:43<03:50,  1.52s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6109, loss: 0.0541, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:44<03:45,  1.50s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6106, loss: 0.0546, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:47<04:53,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6108, loss: 0.0546, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:49<04:26,  1.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6179, loss: 0.0543, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:50<04:15,  1.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6181, loss: 0.0540, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:52<04:08,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6184, loss: 0.0541, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:54<04:08,  1.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6189, loss: 0.0537, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:55<04:00,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6126, loss: 0.0540, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:58<04:26,  1.85s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6171, loss: 0.0538, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:59<04:03,  1.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6154, loss: 0.0534, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:00<03:49,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6169, loss: 0.0533, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:02<03:44,  1.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6134, loss: 0.0532, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:04<04:14,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6169, loss: 0.0530, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:05<03:44,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6188, loss: 0.0529, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:07<03:45,  1.64s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6211, loss: 0.0528, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:08<03:31,  1.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6217, loss: 0.0527, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:10<03:24,  1.51s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6194, loss: 0.0531, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:12<03:53,  1.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6202, loss: 0.0530, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:14<03:47,  1.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6176, loss: 0.0527, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:15<03:34,  1.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6179, loss: 0.0527, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:17<03:33,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6185, loss: 0.0523, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:18<03:21,  1.53s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6184, loss: 0.0521, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:20<03:12,  1.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6178, loss: 0.0522, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:21<03:11,  1.48s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6182, loss: 0.0521, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:24<04:08,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6179, loss: 0.0525, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:27<04:48,  2.27s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6179, loss: 0.0524, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:29<04:49,  2.30s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6160, loss: 0.0527, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:31<04:16,  2.05s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6171, loss: 0.0526, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:32<03:56,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6183, loss: 0.0523, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:34<03:30,  1.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6144, loss: 0.0525, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:35<03:22,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6156, loss: 0.0523, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:37<03:15,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6158, loss: 0.0524, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:39<03:19,  1.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6184, loss: 0.0522, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:40<03:09,  1.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6198, loss: 0.0521, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:41<02:50,  1.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6162, loss: 0.0524, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:43<03:02,  1.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6173, loss: 0.0524, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:44<02:53,  1.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6133, loss: 0.0525, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:47<03:44,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6131, loss: 0.0525, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:49<03:25,  1.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6118, loss: 0.0527, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:50<03:08,  1.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6126, loss: 0.0526, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:52<02:59,  1.60s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6125, loss: 0.0528, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:53<02:41,  1.46s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6143, loss: 0.0528, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:55<02:55,  1.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6156, loss: 0.0530, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:56<02:53,  1.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6144, loss: 0.0531, reg_loss: 0.0000 ||:  40%|████      | 72/180 [01:59<03:37,  2.02s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6161, loss: 0.0530, reg_loss: 0.0000 ||:  41%|████      | 73/180 [02:00<03:00,  1.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6153, loss: 0.0530, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:02<02:52,  1.62s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6168, loss: 0.0527, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:03<02:45,  1.58s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6178, loss: 0.0526, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:05<03:04,  1.78s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6186, loss: 0.0527, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:07<02:49,  1.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "pearson(r): 0.6191, loss: 0.0525, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:08<02:55,  1.72s/it]\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "  0%|          | 0/180 [05:24<?, ?it/s]\n",
            "\n",
            "pearson(r): 0.6199, loss: 0.0525, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:10<02:47,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6187, loss: 0.0529, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:12<03:01,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.6172, loss: 0.0530, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:14<02:48,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6171, loss: 0.0529, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:15<02:39,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6167, loss: 0.0532, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:18<03:03,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.6140, loss: 0.0533, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:19<02:44,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6132, loss: 0.0533, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:20<02:34,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6141, loss: 0.0535, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:22<02:29,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6151, loss: 0.0535, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:23<02:29,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6135, loss: 0.0535, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:26<02:40,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6133, loss: 0.0535, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:27<02:29,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6134, loss: 0.0534, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:28<02:24,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6116, loss: 0.0535, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:30<02:21,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6116, loss: 0.0534, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:31<02:15,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6111, loss: 0.0532, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:33<02:08,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6124, loss: 0.0531, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:34<02:04,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.6096, loss: 0.0535, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:36<02:04,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6085, loss: 0.0536, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:37<02:00,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6087, loss: 0.0536, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:39<02:22,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6098, loss: 0.0535, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:41<02:21,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6079, loss: 0.0536, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:43<02:15,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.6075, loss: 0.0539, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:45<02:17,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6086, loss: 0.0537, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:46<02:00,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6077, loss: 0.0537, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:47<02:02,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6071, loss: 0.0537, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:49<02:01,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6066, loss: 0.0537, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:50<01:53,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.6069, loss: 0.0537, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:51<01:46,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.6052, loss: 0.0539, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:54<02:20,  1.89s/it]\u001b[A\n",
            "pearson(r): 0.6058, loss: 0.0539, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [02:56<02:10,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.6061, loss: 0.0538, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [02:57<01:58,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6059, loss: 0.0538, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [02:59<01:59,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6056, loss: 0.0538, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:01<01:54,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6066, loss: 0.0538, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:02<01:52,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6066, loss: 0.0539, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:04<01:49,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6069, loss: 0.0538, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:05<01:44,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6071, loss: 0.0538, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:07<01:38,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6060, loss: 0.0539, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:08<01:46,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6062, loss: 0.0539, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:10<01:39,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6042, loss: 0.0539, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:11<01:39,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6040, loss: 0.0538, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:14<01:47,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6048, loss: 0.0539, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:15<01:45,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6059, loss: 0.0537, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:17<01:38,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6067, loss: 0.0537, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:18<01:33,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6060, loss: 0.0537, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:20<01:32,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6061, loss: 0.0537, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:21<01:31,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6070, loss: 0.0536, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:23<01:30,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6055, loss: 0.0537, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:25<01:32,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6061, loss: 0.0537, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:26<01:27,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6065, loss: 0.0538, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:28<01:21,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6054, loss: 0.0538, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:30<01:25,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6042, loss: 0.0540, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:31<01:22,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6027, loss: 0.0543, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:33<01:19,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6028, loss: 0.0544, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:34<01:13,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6027, loss: 0.0544, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:36<01:13,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6033, loss: 0.0544, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:37<01:10,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.6031, loss: 0.0544, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:38<01:05,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.6030, loss: 0.0545, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:40<01:03,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6027, loss: 0.0546, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:41<01:02,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.6029, loss: 0.0546, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:43<01:01,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6014, loss: 0.0546, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:44<01:00,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0548, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:46<00:59,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6004, loss: 0.0548, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:47<00:57,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.5999, loss: 0.0549, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:48<00:54,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.6005, loss: 0.0551, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:50<01:02,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5988, loss: 0.0553, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [03:53<01:16,  2.06s/it]\u001b[A\n",
            "pearson(r): 0.5980, loss: 0.0553, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [03:55<01:05,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5973, loss: 0.0554, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [03:56<01:02,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.5960, loss: 0.0556, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [03:58<00:57,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.5948, loss: 0.0557, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:00<00:57,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5917, loss: 0.0559, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:03<01:08,  2.13s/it]\u001b[A\n",
            "pearson(r): 0.5903, loss: 0.0561, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:04<01:00,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.5884, loss: 0.0563, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:06<00:55,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.5866, loss: 0.0565, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:08<00:53,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.5843, loss: 0.0567, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:09<00:48,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.5823, loss: 0.0571, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:11<00:44,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5814, loss: 0.0571, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:13<00:45,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.5805, loss: 0.0572, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:14<00:39,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5788, loss: 0.0573, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:15<00:35,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5785, loss: 0.0576, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:17<00:34,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5782, loss: 0.0576, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:18<00:31,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.5782, loss: 0.0575, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:20<00:31,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5762, loss: 0.0578, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:23<00:38,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.5741, loss: 0.0580, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:24<00:34,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.5706, loss: 0.0583, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:26<00:31,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.5709, loss: 0.0584, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:27<00:28,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5714, loss: 0.0584, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:29<00:27,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.5704, loss: 0.0585, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:31<00:25,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5693, loss: 0.0586, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:32<00:23,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.5675, loss: 0.0588, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:35<00:27,  2.11s/it]\u001b[A\n",
            "pearson(r): 0.5681, loss: 0.0589, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:37<00:24,  2.07s/it]\u001b[A\n",
            "pearson(r): 0.5673, loss: 0.0589, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:39<00:20,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.5652, loss: 0.0592, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:40<00:17,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.5647, loss: 0.0592, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:41<00:13,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5626, loss: 0.0595, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:43<00:13,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5616, loss: 0.0596, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:46<00:13,  1.91s/it]\u001b[A\n",
            "pearson(r): 0.5597, loss: 0.0598, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:49<00:13,  2.23s/it]\u001b[A\n",
            "pearson(r): 0.5601, loss: 0.0598, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:50<00:09,  1.99s/it]\u001b[A\n",
            "pearson(r): 0.5600, loss: 0.0598, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:52<00:07,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.5597, loss: 0.0598, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:53<00:05,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.5575, loss: 0.0600, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:55<00:03,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5584, loss: 0.0599, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:56<00:01,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5578, loss: 0.0599, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [04:57<00:00,  1.65s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.5372, loss: 0.1189, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:44,  3.98it/s]\u001b[A\n",
            "pearson(r): 0.6194, loss: 0.1105, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:45,  3.95it/s]\u001b[A\n",
            "pearson(r): 0.5742, loss: 0.1015, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:00<00:47,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.5415, loss: 0.0956, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<00:46,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.5802, loss: 0.0899, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<00:46,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.5760, loss: 0.0948, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:01<00:46,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.5644, loss: 0.0993, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:01<00:46,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.5401, loss: 0.1035, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:49,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.5205, loss: 0.1074, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<01:01,  2.80it/s]\u001b[A\n",
            "pearson(r): 0.5372, loss: 0.1081, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:03<00:57,  2.93it/s]\u001b[A\n",
            "pearson(r): 0.5515, loss: 0.1054, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:03<00:54,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.5448, loss: 0.1062, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<00:55,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.5439, loss: 0.1050, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:03<00:51,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.5320, loss: 0.1059, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:04<00:55,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.5335, loss: 0.1072, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:55,  2.96it/s]\u001b[A\n",
            "pearson(r): 0.5358, loss: 0.1088, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:51,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.5351, loss: 0.1090, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:05<00:46,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.5358, loss: 0.1081, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:05<00:46,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.5242, loss: 0.1098, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:44,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.5255, loss: 0.1125, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:06<00:50,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.5247, loss: 0.1117, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:06<00:48,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.5159, loss: 0.1120, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:51,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.5140, loss: 0.1103, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:44,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.5160, loss: 0.1096, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:07<00:45,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.5128, loss: 0.1090, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:07<00:46,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.5053, loss: 0.1094, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:44,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.5084, loss: 0.1096, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:08<00:44,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.5070, loss: 0.1086, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:08<00:47,  3.22it/s]\u001b[A\n",
            "pearson(r): 0.5071, loss: 0.1080, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:51,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.5079, loss: 0.1085, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:09<00:47,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.5078, loss: 0.1083, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:09<00:45,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.5051, loss: 0.1087, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:09<00:44,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.5026, loss: 0.1077, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:10<00:46,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.5054, loss: 0.1067, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:10<00:43,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.5096, loss: 0.1065, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:10<00:42,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.4987, loss: 0.1070, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:10<00:46,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.4953, loss: 0.1080, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:11<00:55,  2.58it/s]\u001b[A\n",
            "pearson(r): 0.4981, loss: 0.1081, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:11<00:48,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.4951, loss: 0.1077, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:12<00:46,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.4888, loss: 0.1077, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:12<00:44,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.4927, loss: 0.1080, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:12<00:40,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.4936, loss: 0.1084, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:12<00:40,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.4884, loss: 0.1090, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:13<00:39,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.4909, loss: 0.1102, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:13<00:48,  2.78it/s]\u001b[A\n",
            "pearson(r): 0.4906, loss: 0.1101, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:13<00:45,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.4870, loss: 0.1104, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:14<00:46,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.4880, loss: 0.1101, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:14<00:41,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.4862, loss: 0.1100, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:14<00:39,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.4849, loss: 0.1106, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:15<00:41,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.4849, loss: 0.1112, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:15<00:37,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.4797, loss: 0.1113, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:15<00:37,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.4804, loss: 0.1118, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:15<00:35,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.4821, loss: 0.1112, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:16<00:34,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.4846, loss: 0.1115, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:16<00:35,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.4852, loss: 0.1113, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:16<00:35,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.4862, loss: 0.1115, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:17<00:34,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.4850, loss: 0.1116, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:17<00:34,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.4838, loss: 0.1110, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:17<00:37,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.4866, loss: 0.1112, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:17<00:35,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.4859, loss: 0.1112, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:18<00:34,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.4854, loss: 0.1112, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:18<00:32,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.4859, loss: 0.1110, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:18<00:30,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.4856, loss: 0.1107, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:19<00:30,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.4854, loss: 0.1113, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:19<00:35,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.4869, loss: 0.1110, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:19<00:35,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.4848, loss: 0.1111, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:20<00:34,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.4836, loss: 0.1116, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:20<00:35,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.4857, loss: 0.1109, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:20<00:34,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.4883, loss: 0.1103, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:20<00:31,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.4949, loss: 0.1104, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:21<00:32,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.4947, loss: 0.1103, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:21<00:33,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.4935, loss: 0.1106, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:21<00:36,  2.94it/s]\u001b[A\n",
            "pearson(r): 0.4951, loss: 0.1102, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:22<00:33,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.4927, loss: 0.1100, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:22<00:33,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.4945, loss: 0.1100, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:22<00:30,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.4924, loss: 0.1096, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:23<00:30,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.4914, loss: 0.1098, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:23<00:29,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.4914, loss: 0.1093, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:23<00:34,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.4879, loss: 0.1098, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:24<00:35,  2.85it/s]\u001b[A\n",
            "pearson(r): 0.4880, loss: 0.1100, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:24<00:30,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.4826, loss: 0.1108, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:24<00:30,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.4828, loss: 0.1107, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:24<00:29,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.4833, loss: 0.1108, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:25<00:27,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.4830, loss: 0.1108, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:25<00:27,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.4795, loss: 0.1109, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:26<00:34,  2.76it/s]\u001b[A\n",
            "pearson(r): 0.4790, loss: 0.1108, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:26<00:32,  2.93it/s]\u001b[A\n",
            "pearson(r): 0.4767, loss: 0.1112, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:26<00:36,  2.52it/s]\u001b[A\n",
            "pearson(r): 0.4793, loss: 0.1109, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:27<00:33,  2.74it/s]\u001b[A\n",
            "pearson(r): 0.4807, loss: 0.1107, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:27<00:31,  2.86it/s]\u001b[A\n",
            "pearson(r): 0.4808, loss: 0.1108, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:27<00:29,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.4825, loss: 0.1106, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:28<00:27,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.4821, loss: 0.1106, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:28<00:33,  2.62it/s]\u001b[A\n",
            "pearson(r): 0.4826, loss: 0.1113, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:28<00:30,  2.88it/s]\u001b[A\n",
            "pearson(r): 0.4832, loss: 0.1111, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:29<00:27,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.4818, loss: 0.1113, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:29<00:28,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.4807, loss: 0.1116, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:29<00:26,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.4799, loss: 0.1118, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:30<00:31,  2.64it/s]\u001b[A\n",
            "pearson(r): 0.4790, loss: 0.1115, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:30<00:28,  2.85it/s]\u001b[A\n",
            "pearson(r): 0.4797, loss: 0.1110, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:30<00:25,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.4819, loss: 0.1107, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:31<00:24,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.4816, loss: 0.1111, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:31<00:24,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.4805, loss: 0.1113, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:31<00:22,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.4812, loss: 0.1112, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:31<00:22,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.4808, loss: 0.1110, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:32<00:23,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.4807, loss: 0.1109, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:32<00:22,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.4815, loss: 0.1114, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:32<00:19,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.4817, loss: 0.1114, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:32<00:18,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.4819, loss: 0.1114, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:33<00:18,  4.00it/s]\u001b[A\n",
            "pearson(r): 0.4810, loss: 0.1115, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:33<00:17,  3.96it/s]\u001b[A\n",
            "pearson(r): 0.4815, loss: 0.1117, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:33<00:23,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.4819, loss: 0.1119, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:34<00:23,  2.96it/s]\u001b[A\n",
            "pearson(r): 0.4818, loss: 0.1115, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:34<00:26,  2.54it/s]\u001b[A\n",
            "pearson(r): 0.4835, loss: 0.1116, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:35<00:23,  2.81it/s]\u001b[A\n",
            "pearson(r): 0.4841, loss: 0.1113, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:35<00:22,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.4851, loss: 0.1116, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:35<00:20,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.4864, loss: 0.1114, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:35<00:19,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.4886, loss: 0.1114, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:36<00:18,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.4896, loss: 0.1114, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:36<00:16,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.4913, loss: 0.1111, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:36<00:14,  4.10it/s]\u001b[A\n",
            "pearson(r): 0.4904, loss: 0.1112, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:37<00:17,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.4889, loss: 0.1111, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:37<00:15,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.4882, loss: 0.1112, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:37<00:14,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.4885, loss: 0.1110, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:37<00:15,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.4882, loss: 0.1108, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:38<00:14,  3.83it/s]\u001b[A\n",
            "pearson(r): 0.4903, loss: 0.1109, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:38<00:14,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.4897, loss: 0.1113, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:38<00:13,  4.05it/s]\u001b[A\n",
            "pearson(r): 0.4908, loss: 0.1114, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:38<00:13,  4.06it/s]\u001b[A\n",
            "pearson(r): 0.4903, loss: 0.1115, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:38<00:12,  4.05it/s]\u001b[A\n",
            "pearson(r): 0.4911, loss: 0.1115, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:39<00:13,  3.92it/s]\u001b[A\n",
            "pearson(r): 0.4906, loss: 0.1116, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:39<00:12,  4.09it/s]\u001b[A\n",
            "pearson(r): 0.4933, loss: 0.1115, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:39<00:14,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.4932, loss: 0.1113, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:40<00:15,  3.02it/s]\u001b[A\n",
            "pearson(r): 0.4920, loss: 0.1115, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:40<00:14,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.4937, loss: 0.1118, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:41<00:17,  2.69it/s]\u001b[A\n",
            "pearson(r): 0.4929, loss: 0.1119, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:41<00:14,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.4923, loss: 0.1121, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:41<00:13,  3.22it/s]\u001b[A\n",
            "pearson(r): 0.4942, loss: 0.1120, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:41<00:12,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.4937, loss: 0.1119, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:42<00:12,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.4926, loss: 0.1118, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:42<00:11,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.4930, loss: 0.1119, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:42<00:11,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.4925, loss: 0.1118, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:42<00:10,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.4933, loss: 0.1114, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:43<00:10,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.4925, loss: 0.1117, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:43<00:10,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.4935, loss: 0.1116, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:43<00:09,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.4943, loss: 0.1113, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:43<00:09,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.4941, loss: 0.1112, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:44<00:08,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.4929, loss: 0.1114, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:44<00:08,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.4919, loss: 0.1116, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:44<00:08,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.4920, loss: 0.1115, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:45<00:08,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.4931, loss: 0.1112, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:45<00:07,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.4928, loss: 0.1114, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:45<00:07,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.4910, loss: 0.1116, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:45<00:07,  3.94it/s]\u001b[A\n",
            "pearson(r): 0.4903, loss: 0.1117, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:46<00:06,  3.98it/s]\u001b[A\n",
            "pearson(r): 0.4912, loss: 0.1120, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:46<00:06,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.4912, loss: 0.1121, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:46<00:06,  4.02it/s]\u001b[A\n",
            "pearson(r): 0.4915, loss: 0.1118, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:46<00:05,  4.11it/s]\u001b[A\n",
            "pearson(r): 0.4893, loss: 0.1119, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:47<00:06,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.4893, loss: 0.1119, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:47<00:06,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.4898, loss: 0.1117, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:47<00:05,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.4887, loss: 0.1115, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:48<00:07,  2.81it/s]\u001b[A\n",
            "pearson(r): 0.4888, loss: 0.1115, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:48<00:06,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.4900, loss: 0.1116, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:48<00:05,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.4900, loss: 0.1118, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:49<00:05,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.4906, loss: 0.1120, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:49<00:04,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.4903, loss: 0.1119, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:49<00:04,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.4897, loss: 0.1118, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:50<00:04,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.4889, loss: 0.1119, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:50<00:04,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.4894, loss: 0.1119, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:50<00:04,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.4904, loss: 0.1118, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:51<00:03,  2.95it/s]\u001b[A\n",
            "pearson(r): 0.4910, loss: 0.1118, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:51<00:03,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.4915, loss: 0.1119, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:51<00:02,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.4908, loss: 0.1122, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:51<00:02,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.4898, loss: 0.1123, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:52<00:02,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.4907, loss: 0.1123, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:52<00:01,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.4915, loss: 0.1122, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:52<00:01,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.4920, loss: 0.1120, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:53<00:01,  2.96it/s]\u001b[A\n",
            "pearson(r): 0.4921, loss: 0.1122, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:53<00:00,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.4919, loss: 0.1122, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.4923, loss: 0.1123, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:53<00:00,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.4926, loss: 0.1122, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:54<00:00,  3.32it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.2689, loss: 0.0616, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:01<04:03,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.3705, loss: 0.0787, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:02<04:07,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.4023, loss: 0.0748, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:04<04:30,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.4788, loss: 0.0728, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:06<04:39,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5225, loss: 0.0723, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:07<04:29,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5313, loss: 0.0670, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:09<04:28,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5384, loss: 0.0661, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:10<04:24,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5283, loss: 0.0660, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:12<04:37,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5295, loss: 0.0667, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:14<04:33,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5095, loss: 0.0671, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:16<05:11,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5034, loss: 0.0654, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:18<05:08,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5132, loss: 0.0646, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:19<04:52,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.5246, loss: 0.0628, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:21<04:34,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.5266, loss: 0.0634, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:22<04:29,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5310, loss: 0.0624, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:24<04:25,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5142, loss: 0.0645, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:27<05:33,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.5293, loss: 0.0641, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:29<05:11,  1.91s/it]\u001b[A\n",
            "pearson(r): 0.5332, loss: 0.0627, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:30<04:51,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.5546, loss: 0.0623, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:32<04:35,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.5455, loss: 0.0636, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:34<04:57,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.5496, loss: 0.0642, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:35<04:27,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.5534, loss: 0.0642, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:37<04:09,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5494, loss: 0.0638, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:39<04:53,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.5470, loss: 0.0637, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:41<05:04,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.5458, loss: 0.0637, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:43<04:47,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.5482, loss: 0.0634, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:44<04:26,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5432, loss: 0.0634, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:46<04:12,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5518, loss: 0.0627, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:47<03:46,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5511, loss: 0.0627, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:48<03:25,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.5490, loss: 0.0620, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:49<03:28,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.5557, loss: 0.0620, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:51<03:49,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5512, loss: 0.0623, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:53<04:14,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.5544, loss: 0.0624, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:55<04:19,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.5563, loss: 0.0627, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:57<04:06,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5506, loss: 0.0632, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:59<04:24,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5540, loss: 0.0632, reg_loss: 0.0000 ||:  20%|██        | 36/180 [01:00<04:02,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5515, loss: 0.0631, reg_loss: 0.0000 ||:  21%|██        | 37/180 [01:02<04:01,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5491, loss: 0.0630, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:03<03:38,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0635, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:05<03:37,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5499, loss: 0.0632, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:07<03:52,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5501, loss: 0.0630, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:08<03:49,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5529, loss: 0.0628, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:10<03:48,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5525, loss: 0.0631, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:11<03:37,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5521, loss: 0.0631, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:13<03:26,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5503, loss: 0.0631, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:14<03:26,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5515, loss: 0.0630, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:16<03:12,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.5508, loss: 0.0628, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:17<03:20,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5506, loss: 0.0625, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:19<03:19,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5493, loss: 0.0623, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:20<03:13,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5476, loss: 0.0626, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:21<03:06,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.5430, loss: 0.0626, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:23<03:06,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.5426, loss: 0.0626, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:24<03:04,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.5420, loss: 0.0626, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:27<03:39,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5446, loss: 0.0624, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:28<03:24,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5415, loss: 0.0625, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:30<03:18,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5452, loss: 0.0625, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:32<03:43,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.5438, loss: 0.0623, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:33<03:27,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5454, loss: 0.0623, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:35<03:17,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5495, loss: 0.0621, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:36<03:13,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5488, loss: 0.0621, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:38<02:55,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.5491, loss: 0.0620, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:39<02:54,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5483, loss: 0.0621, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:41<02:56,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5491, loss: 0.0617, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:42<02:56,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5474, loss: 0.0616, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:44<03:23,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.5490, loss: 0.0614, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:46<03:21,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0615, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:48<03:08,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5462, loss: 0.0615, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:50<03:20,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.5482, loss: 0.0614, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:51<03:13,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.5482, loss: 0.0613, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:53<03:10,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.5490, loss: 0.0613, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:55<03:19,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.5489, loss: 0.0612, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:56<02:58,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.5510, loss: 0.0608, reg_loss: 0.0000 ||:  40%|████      | 72/180 [01:58<03:15,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.5512, loss: 0.0608, reg_loss: 0.0000 ||:  41%|████      | 73/180 [02:00<03:05,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5504, loss: 0.0606, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:02<03:25,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.5515, loss: 0.0604, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:04<03:07,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.5516, loss: 0.0606, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:05<02:53,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5534, loss: 0.0605, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:07<02:42,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5556, loss: 0.0605, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:08<02:31,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5555, loss: 0.0605, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:09<02:29,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5553, loss: 0.0605, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:11<02:20,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.5547, loss: 0.0607, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:12<02:19,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.5525, loss: 0.0607, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:13<02:09,  1.33s/it]\u001b[A\n",
            "pearson(r): 0.5519, loss: 0.0605, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:15<02:14,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.5528, loss: 0.0605, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:16<02:13,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.5526, loss: 0.0607, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:19<02:57,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.5532, loss: 0.0609, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:21<02:47,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.5511, loss: 0.0609, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:23<02:53,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.5502, loss: 0.0608, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:25<02:54,  1.89s/it]\u001b[A\n",
            "pearson(r): 0.5515, loss: 0.0606, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:26<02:41,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.5514, loss: 0.0609, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:29<03:11,  2.13s/it]\u001b[A\n",
            "pearson(r): 0.5517, loss: 0.0608, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:32<03:21,  2.26s/it]\u001b[A\n",
            "pearson(r): 0.5490, loss: 0.0608, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:33<03:03,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.5484, loss: 0.0607, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:35<02:51,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.5478, loss: 0.0610, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:37<02:36,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5488, loss: 0.0611, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:38<02:27,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.5477, loss: 0.0612, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:40<02:31,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.5454, loss: 0.0615, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:42<02:23,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5456, loss: 0.0614, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:43<02:12,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0613, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:45<02:10,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5452, loss: 0.0614, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:46<02:05,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5445, loss: 0.0614, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:47<02:00,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5427, loss: 0.0615, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:49<01:59,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5417, loss: 0.0615, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:50<01:56,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5429, loss: 0.0615, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:52<01:53,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5426, loss: 0.0616, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:53<01:46,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.5431, loss: 0.0615, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:55<02:03,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5431, loss: 0.0614, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [02:57<01:56,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5415, loss: 0.0615, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [02:58<01:52,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5402, loss: 0.0614, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [03:00<01:59,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5399, loss: 0.0615, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:02<01:57,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.5400, loss: 0.0616, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:03<01:44,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5407, loss: 0.0616, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:05<01:40,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5405, loss: 0.0616, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:06<01:42,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5405, loss: 0.0615, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:07<01:37,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5403, loss: 0.0615, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:09<01:37,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5411, loss: 0.0615, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:11<01:37,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5406, loss: 0.0616, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:12<01:34,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5404, loss: 0.0616, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:13<01:30,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.5405, loss: 0.0615, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:15<01:26,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.5413, loss: 0.0615, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:18<01:56,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.5389, loss: 0.0618, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:21<02:13,  2.27s/it]\u001b[A\n",
            "pearson(r): 0.5393, loss: 0.0617, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:22<01:55,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.5393, loss: 0.0617, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:24<01:46,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.5393, loss: 0.0618, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:25<01:37,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.5366, loss: 0.0621, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:28<01:55,  2.11s/it]\u001b[A\n",
            "pearson(r): 0.5361, loss: 0.0622, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:31<02:08,  2.38s/it]\u001b[A\n",
            "pearson(r): 0.5350, loss: 0.0623, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:33<01:53,  2.15s/it]\u001b[A\n",
            "pearson(r): 0.5354, loss: 0.0622, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:35<01:48,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.5354, loss: 0.0624, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:36<01:38,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.5354, loss: 0.0626, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:38<01:26,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5363, loss: 0.0624, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:39<01:24,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5361, loss: 0.0625, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:42<01:41,  2.12s/it]\u001b[A\n",
            "pearson(r): 0.5367, loss: 0.0625, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:44<01:32,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.5358, loss: 0.0627, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:46<01:23,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5350, loss: 0.0626, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:47<01:20,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.5340, loss: 0.0627, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:49<01:12,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5337, loss: 0.0627, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:50<01:09,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5344, loss: 0.0626, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:51<01:03,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5323, loss: 0.0627, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:54<01:21,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.5323, loss: 0.0626, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:56<01:13,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5310, loss: 0.0627, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:57<01:06,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5313, loss: 0.0627, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:59<01:02,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5315, loss: 0.0627, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [04:00<01:00,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5310, loss: 0.0626, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [04:02<00:58,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5314, loss: 0.0626, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [04:04<00:56,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5297, loss: 0.0627, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [04:07<01:09,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.5293, loss: 0.0627, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:09<01:09,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.5290, loss: 0.0627, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:10<01:00,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.5289, loss: 0.0626, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:11<00:51,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5287, loss: 0.0627, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:13<00:49,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.5287, loss: 0.0626, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:14<00:46,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5293, loss: 0.0625, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:17<00:48,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5299, loss: 0.0625, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:18<00:44,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.5295, loss: 0.0626, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:19<00:41,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5308, loss: 0.0625, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:21<00:38,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5309, loss: 0.0625, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:22<00:35,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5295, loss: 0.0626, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:24<00:35,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5298, loss: 0.0625, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:26<00:35,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5288, loss: 0.0627, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:28<00:36,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.5286, loss: 0.0626, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:29<00:33,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5287, loss: 0.0627, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:31<00:30,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5288, loss: 0.0627, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:32<00:28,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5276, loss: 0.0628, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:34<00:26,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5276, loss: 0.0628, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:35<00:24,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5268, loss: 0.0629, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:37<00:24,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5267, loss: 0.0630, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:39<00:24,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5261, loss: 0.0630, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:40<00:20,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5257, loss: 0.0629, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:42<00:18,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5250, loss: 0.0629, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:43<00:16,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5246, loss: 0.0630, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:45<00:14,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5244, loss: 0.0630, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:46<00:13,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5251, loss: 0.0631, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:48<00:11,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5253, loss: 0.0630, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:49<00:10,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.5251, loss: 0.0630, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:51<00:09,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5245, loss: 0.0631, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:52<00:07,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5242, loss: 0.0632, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:53<00:05,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.5251, loss: 0.0631, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:55<00:04,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.5250, loss: 0.0632, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:58<00:03,  1.91s/it]\u001b[A\n",
            "pearson(r): 0.5251, loss: 0.0632, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [05:00<00:01,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.5259, loss: 0.0632, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [05:01<00:00,  1.68s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.7196, loss: 0.0992, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<01:36,  1.86it/s]\u001b[A\n",
            "pearson(r): 0.5856, loss: 0.0955, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<01:20,  2.21it/s]\u001b[A\n",
            "pearson(r): 0.5946, loss: 0.0911, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:01<01:10,  2.51it/s]\u001b[A\n",
            "pearson(r): 0.5837, loss: 0.0865, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<01:04,  2.74it/s]\u001b[A\n",
            "pearson(r): 0.5522, loss: 0.0877, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<01:02,  2.79it/s]\u001b[A\n",
            "pearson(r): 0.5484, loss: 0.0860, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:02<01:03,  2.75it/s]\u001b[A\n",
            "pearson(r): 0.5570, loss: 0.0833, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:02<01:01,  2.80it/s]\u001b[A\n",
            "pearson(r): 0.5841, loss: 0.0792, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:56,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.5983, loss: 0.0801, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<00:51,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.5980, loss: 0.0775, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:03<00:48,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6144, loss: 0.0755, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:03<00:48,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6129, loss: 0.0748, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<00:47,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.6186, loss: 0.0721, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:03<00:44,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6106, loss: 0.0732, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:04<00:45,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.6146, loss: 0.0752, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:44,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.6193, loss: 0.0755, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:40,  4.01it/s]\u001b[A\n",
            "pearson(r): 0.6169, loss: 0.0757, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:05<00:46,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6205, loss: 0.0759, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:05<00:45,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6176, loss: 0.0753, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:44,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.6262, loss: 0.0739, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:06<00:53,  2.96it/s]\u001b[A\n",
            "pearson(r): 0.6147, loss: 0.0753, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:06<00:48,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6155, loss: 0.0755, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:45,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.6113, loss: 0.0760, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:44,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6129, loss: 0.0753, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:07<00:46,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.6198, loss: 0.0742, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:07<00:42,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6142, loss: 0.0758, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:54,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.6126, loss: 0.0767, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:08<00:48,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.6124, loss: 0.0783, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:08<00:42,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.6119, loss: 0.0772, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:41,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6071, loss: 0.0787, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:08<00:38,  3.86it/s]\u001b[A\n",
            "pearson(r): 0.6060, loss: 0.0792, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:09<00:43,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.6050, loss: 0.0798, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:09<00:54,  2.74it/s]\u001b[A\n",
            "pearson(r): 0.6047, loss: 0.0807, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:10<00:49,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.6021, loss: 0.0809, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:10<00:43,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.6060, loss: 0.0804, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:10<00:42,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.6069, loss: 0.0803, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:10<00:40,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6074, loss: 0.0796, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:11<00:42,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.6017, loss: 0.0805, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:11<00:47,  3.00it/s]\u001b[A\n",
            "pearson(r): 0.6007, loss: 0.0809, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:11<00:45,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.6031, loss: 0.0812, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:12<00:42,  3.32it/s]\u001b[A\n",
            "pearson(r): 0.6018, loss: 0.0807, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:12<00:39,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.6005, loss: 0.0811, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:12<00:39,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0808, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:12<00:42,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.6048, loss: 0.0802, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:13<00:41,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.6012, loss: 0.0803, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:13<00:39,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.6017, loss: 0.0802, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:13<00:44,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.6017, loss: 0.0802, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:14<00:45,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.6034, loss: 0.0800, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:14<00:42,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.6037, loss: 0.0803, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:14<00:40,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.6046, loss: 0.0798, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:15<00:38,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.6068, loss: 0.0792, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:15<00:37,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.6029, loss: 0.0791, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:15<00:41,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0791, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:16<00:37,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0787, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:16<00:37,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.5990, loss: 0.0788, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:16<00:36,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.5995, loss: 0.0784, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:16<00:34,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.6015, loss: 0.0781, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:17<00:32,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0781, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:17<00:37,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.6005, loss: 0.0780, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:17<00:35,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.6011, loss: 0.0780, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:18<00:36,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.5974, loss: 0.0788, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:18<00:44,  2.70it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0787, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:18<00:40,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.6000, loss: 0.0788, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:19<00:36,  3.22it/s]\u001b[A\n",
            "pearson(r): 0.5979, loss: 0.0788, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:19<00:33,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.5919, loss: 0.0790, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:19<00:37,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.5941, loss: 0.0788, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:20<00:35,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.5952, loss: 0.0789, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:20<00:31,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0786, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:20<00:31,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.5970, loss: 0.0786, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:20<00:29,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.5978, loss: 0.0785, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:21<00:29,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.5967, loss: 0.0788, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:21<00:30,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.5962, loss: 0.0788, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:21<00:29,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.5998, loss: 0.0786, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:21<00:29,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6006, loss: 0.0786, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:22<00:27,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.6007, loss: 0.0785, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:22<00:25,  4.13it/s]\u001b[A\n",
            "pearson(r): 0.6015, loss: 0.0782, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:22<00:26,  3.89it/s]\u001b[A\n",
            "pearson(r): 0.6019, loss: 0.0784, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:22<00:24,  4.18it/s]\u001b[A\n",
            "pearson(r): 0.6021, loss: 0.0782, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:23<00:26,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.6010, loss: 0.0785, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:23<00:25,  3.89it/s]\u001b[A\n",
            "pearson(r): 0.6011, loss: 0.0784, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:23<00:24,  4.10it/s]\u001b[A\n",
            "pearson(r): 0.5997, loss: 0.0786, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:23<00:26,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.6012, loss: 0.0783, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:24<00:25,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.6030, loss: 0.0783, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:24<00:25,  3.85it/s]\u001b[A\n",
            "pearson(r): 0.6022, loss: 0.0783, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:24<00:32,  2.94it/s]\u001b[A\n",
            "pearson(r): 0.6031, loss: 0.0779, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:25<00:30,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.6036, loss: 0.0779, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:25<00:28,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.6017, loss: 0.0778, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:25<00:27,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.6018, loss: 0.0782, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:26<00:33,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.6011, loss: 0.0781, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:26<00:30,  2.96it/s]\u001b[A\n",
            "pearson(r): 0.6002, loss: 0.0780, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:26<00:29,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.6000, loss: 0.0778, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:27<00:27,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.5998, loss: 0.0780, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:27<00:28,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0777, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:27<00:26,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6001, loss: 0.0777, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:27<00:23,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.5979, loss: 0.0777, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:28<00:25,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0776, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:28<00:24,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.5961, loss: 0.0777, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:28<00:22,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.5956, loss: 0.0779, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:29<00:21,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.5947, loss: 0.0778, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:29<00:21,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.5952, loss: 0.0777, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:29<00:21,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.5936, loss: 0.0778, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:29<00:22,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.5947, loss: 0.0775, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:30<00:23,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.5949, loss: 0.0771, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:30<00:22,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.5937, loss: 0.0772, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:31<00:28,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.5923, loss: 0.0772, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:31<00:25,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.5944, loss: 0.0771, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:31<00:23,  3.12it/s]\u001b[A\n",
            "pearson(r): 0.5949, loss: 0.0771, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:31<00:21,  3.32it/s]\u001b[A\n",
            "pearson(r): 0.5955, loss: 0.0772, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:32<00:26,  2.69it/s]\u001b[A\n",
            "pearson(r): 0.5972, loss: 0.0771, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:32<00:23,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.5970, loss: 0.0771, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:32<00:20,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.5979, loss: 0.0771, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:33<00:22,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.5978, loss: 0.0772, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:33<00:21,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.5982, loss: 0.0770, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:33<00:19,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0770, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:34<00:18,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.5992, loss: 0.0769, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:34<00:19,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6007, loss: 0.0766, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:34<00:20,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0766, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:35<00:19,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.5998, loss: 0.0763, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:35<00:18,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0764, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:35<00:22,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0766, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:36<00:20,  2.90it/s]\u001b[A\n",
            "pearson(r): 0.6004, loss: 0.0766, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:36<00:19,  3.06it/s]\u001b[A\n",
            "pearson(r): 0.5998, loss: 0.0765, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:36<00:17,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0765, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:36<00:17,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.6011, loss: 0.0764, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:37<00:16,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.5998, loss: 0.0765, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:37<00:15,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6003, loss: 0.0768, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:37<00:13,  3.89it/s]\u001b[A\n",
            "pearson(r): 0.6000, loss: 0.0768, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:37<00:14,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.5986, loss: 0.0768, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:38<00:16,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.5997, loss: 0.0766, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:38<00:17,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.6001, loss: 0.0766, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:38<00:15,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.6001, loss: 0.0767, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:39<00:18,  2.66it/s]\u001b[A\n",
            "pearson(r): 0.6010, loss: 0.0768, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:39<00:16,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.6011, loss: 0.0768, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:40<00:14,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0767, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:40<00:13,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.6014, loss: 0.0768, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:40<00:12,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.6008, loss: 0.0769, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:40<00:12,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.5996, loss: 0.0771, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:41<00:12,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.5983, loss: 0.0771, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:41<00:12,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.5980, loss: 0.0770, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:41<00:11,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.5981, loss: 0.0770, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:41<00:11,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.5977, loss: 0.0772, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:42<00:12,  3.12it/s]\u001b[A\n",
            "pearson(r): 0.5972, loss: 0.0770, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:42<00:11,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.5964, loss: 0.0771, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:42<00:10,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.5954, loss: 0.0771, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:43<00:10,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.5942, loss: 0.0774, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:43<00:09,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.5955, loss: 0.0772, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:43<00:09,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.5958, loss: 0.0773, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:44<00:09,  3.32it/s]\u001b[A\n",
            "pearson(r): 0.5955, loss: 0.0772, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:44<00:09,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.5960, loss: 0.0771, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:44<00:08,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.5957, loss: 0.0771, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:44<00:09,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.5954, loss: 0.0771, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:45<00:08,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.5964, loss: 0.0771, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:45<00:08,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.5964, loss: 0.0772, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:45<00:07,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.5982, loss: 0.0771, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:46<00:07,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0770, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:46<00:06,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.5978, loss: 0.0771, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:46<00:06,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.5974, loss: 0.0771, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:46<00:06,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.5976, loss: 0.0771, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:47<00:06,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.5975, loss: 0.0771, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:47<00:06,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.5983, loss: 0.0772, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:47<00:05,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.5971, loss: 0.0772, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:48<00:05,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.5970, loss: 0.0772, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:48<00:05,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0771, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:48<00:05,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.5980, loss: 0.0772, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:49<00:04,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.5990, loss: 0.0771, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:49<00:04,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.5983, loss: 0.0769, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:49<00:04,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.5990, loss: 0.0768, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:49<00:03,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.5981, loss: 0.0768, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:50<00:03,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.5982, loss: 0.0769, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:50<00:03,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.5993, loss: 0.0768, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:50<00:02,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0770, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:50<00:02,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.5979, loss: 0.0771, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:51<00:02,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.5985, loss: 0.0770, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:51<00:02,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.5991, loss: 0.0771, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:51<00:01,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.5995, loss: 0.0769, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:52<00:01,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.5995, loss: 0.0769, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:52<00:01,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.5979, loss: 0.0770, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:52<00:00,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.5973, loss: 0.0773, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.5984, loss: 0.0771, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:53<00:00,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.5983, loss: 0.0771, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:53<00:00,  3.36it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.5906, loss: 0.0627, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:02<07:26,  2.49s/it]\u001b[A\n",
            "pearson(r): 0.6759, loss: 0.0550, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:03<06:28,  2.18s/it]\u001b[A\n",
            "pearson(r): 0.5900, loss: 0.0634, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:05<06:00,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.5910, loss: 0.0637, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:07<05:26,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.6212, loss: 0.0614, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:08<05:02,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6391, loss: 0.0583, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:09<04:43,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6397, loss: 0.0589, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:11<04:38,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6382, loss: 0.0587, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:14<05:26,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.6239, loss: 0.0597, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:15<05:22,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6143, loss: 0.0586, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:17<05:13,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.5961, loss: 0.0604, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:20<06:10,  2.19s/it]\u001b[A\n",
            "pearson(r): 0.6149, loss: 0.0592, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:22<05:46,  2.06s/it]\u001b[A\n",
            "pearson(r): 0.5973, loss: 0.0610, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:24<05:26,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.6083, loss: 0.0605, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:25<05:00,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.6079, loss: 0.0601, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:27<05:12,  1.89s/it]\u001b[A\n",
            "pearson(r): 0.6067, loss: 0.0601, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:29<04:45,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6065, loss: 0.0607, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:30<04:31,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5970, loss: 0.0615, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:32<04:25,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.5966, loss: 0.0601, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:33<04:21,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5957, loss: 0.0604, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:35<04:08,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5897, loss: 0.0610, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:36<04:02,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5703, loss: 0.0616, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:38<04:27,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5691, loss: 0.0612, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:40<04:19,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5627, loss: 0.0609, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:41<04:10,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5610, loss: 0.0606, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:42<03:53,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5596, loss: 0.0600, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:44<03:53,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5602, loss: 0.0604, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:45<03:49,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5624, loss: 0.0605, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:47<03:55,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5646, loss: 0.0608, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:49<04:27,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.5653, loss: 0.0607, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:51<04:33,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5698, loss: 0.0602, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:53<04:09,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5592, loss: 0.0609, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:54<03:55,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5562, loss: 0.0620, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:56<04:25,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.5584, loss: 0.0622, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:58<04:04,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5579, loss: 0.0625, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:59<03:39,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5573, loss: 0.0627, reg_loss: 0.0000 ||:  20%|██        | 36/180 [01:00<03:36,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5592, loss: 0.0623, reg_loss: 0.0000 ||:  21%|██        | 37/180 [01:03<04:13,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.5607, loss: 0.0613, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:04<03:59,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5581, loss: 0.0614, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:07<04:54,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.5602, loss: 0.0612, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:09<04:31,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.5657, loss: 0.0608, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:11<04:19,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.5664, loss: 0.0604, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:14<05:05,  2.22s/it]\u001b[A\n",
            "pearson(r): 0.5628, loss: 0.0606, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:15<04:29,  1.97s/it]\u001b[A\n",
            "pearson(r): 0.5627, loss: 0.0604, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:16<04:06,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.5615, loss: 0.0602, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:18<03:37,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5608, loss: 0.0602, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:19<03:31,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5588, loss: 0.0604, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:22<04:28,  2.02s/it]\u001b[A\n",
            "pearson(r): 0.5547, loss: 0.0609, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:24<04:01,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5584, loss: 0.0607, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:26<04:18,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.5588, loss: 0.0607, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:28<04:14,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.5590, loss: 0.0602, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:30<04:18,  2.00s/it]\u001b[A\n",
            "pearson(r): 0.5553, loss: 0.0610, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:31<03:52,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5546, loss: 0.0611, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:33<03:48,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.5556, loss: 0.0610, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:35<03:37,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.5523, loss: 0.0615, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:36<03:14,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5534, loss: 0.0615, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:37<03:11,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5537, loss: 0.0616, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:39<03:06,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5520, loss: 0.0621, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:41<03:24,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5506, loss: 0.0623, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:42<03:20,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5517, loss: 0.0624, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:44<03:11,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5523, loss: 0.0626, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:45<03:11,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5489, loss: 0.0629, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:47<03:05,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5467, loss: 0.0630, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:48<03:02,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5470, loss: 0.0632, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:50<02:59,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5504, loss: 0.0630, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:52<03:16,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.5495, loss: 0.0630, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:54<03:05,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5497, loss: 0.0629, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:55<03:08,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5500, loss: 0.0625, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:57<03:06,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5487, loss: 0.0628, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:58<02:56,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5488, loss: 0.0627, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [02:00<02:52,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5469, loss: 0.0632, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [02:01<02:48,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5477, loss: 0.0631, reg_loss: 0.0000 ||:  40%|████      | 72/180 [02:03<02:47,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5437, loss: 0.0638, reg_loss: 0.0000 ||:  41%|████      | 73/180 [02:06<03:32,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.5423, loss: 0.0644, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:08<03:35,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.5413, loss: 0.0646, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:09<03:12,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5433, loss: 0.0644, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:11<02:56,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5445, loss: 0.0642, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:12<02:43,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5457, loss: 0.0645, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:14<03:00,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.5455, loss: 0.0644, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:16<02:44,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5435, loss: 0.0647, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:17<02:27,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5455, loss: 0.0649, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:18<02:26,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5446, loss: 0.0650, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:20<02:40,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.5439, loss: 0.0649, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:22<02:38,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5433, loss: 0.0647, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:23<02:28,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5452, loss: 0.0645, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:25<02:28,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5450, loss: 0.0645, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:26<02:20,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5452, loss: 0.0645, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:29<02:44,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.5442, loss: 0.0644, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:30<02:39,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5444, loss: 0.0642, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:32<02:27,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5442, loss: 0.0643, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:33<02:24,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5422, loss: 0.0643, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:36<03:01,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.5430, loss: 0.0641, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:38<02:47,  1.91s/it]\u001b[A\n",
            "pearson(r): 0.5448, loss: 0.0639, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:40<02:42,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.5463, loss: 0.0638, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:41<02:28,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5471, loss: 0.0636, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:43<02:21,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5457, loss: 0.0638, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:44<02:11,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5449, loss: 0.0636, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:45<02:07,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0637, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:47<02:04,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5479, loss: 0.0636, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:48<02:01,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5505, loss: 0.0634, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:50<01:58,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5475, loss: 0.0636, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:53<02:34,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.5479, loss: 0.0635, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:54<02:20,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.5470, loss: 0.0633, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:56<02:11,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.5458, loss: 0.0635, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:58<02:18,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.5436, loss: 0.0634, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:59<02:06,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.5442, loss: 0.0634, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [03:01<01:59,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5437, loss: 0.0634, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [03:04<02:27,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.5453, loss: 0.0633, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [03:05<02:11,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5452, loss: 0.0635, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [03:06<02:02,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.5447, loss: 0.0636, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:08<01:51,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5430, loss: 0.0637, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:09<01:45,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5430, loss: 0.0637, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:11<01:44,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5424, loss: 0.0636, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:12<01:38,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5412, loss: 0.0635, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:14<01:39,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5411, loss: 0.0633, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:15<01:34,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.5419, loss: 0.0632, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:16<01:34,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5439, loss: 0.0630, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:18<01:36,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5444, loss: 0.0629, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:19<01:25,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.5443, loss: 0.0629, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:22<01:54,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.5442, loss: 0.0628, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:24<01:46,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.5436, loss: 0.0627, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:25<01:37,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.5426, loss: 0.0626, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:27<01:37,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.5416, loss: 0.0626, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:28<01:35,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.5429, loss: 0.0627, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:30<01:27,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5421, loss: 0.0628, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:32<01:34,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.5422, loss: 0.0628, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:33<01:27,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5400, loss: 0.0630, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:35<01:35,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.5403, loss: 0.0630, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:37<01:28,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.5418, loss: 0.0629, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:38<01:22,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.5424, loss: 0.0628, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:40<01:19,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5422, loss: 0.0626, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:41<01:16,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5442, loss: 0.0624, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:43<01:12,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5438, loss: 0.0624, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:45<01:17,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5441, loss: 0.0623, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:46<01:13,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.5446, loss: 0.0623, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:47<01:05,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.5453, loss: 0.0624, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:49<01:10,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.5461, loss: 0.0624, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:51<01:05,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.5448, loss: 0.0623, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:52<01:02,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5453, loss: 0.0622, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:54<01:01,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.5453, loss: 0.0621, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:55<01:00,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.5453, loss: 0.0620, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:57<01:01,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5466, loss: 0.0620, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:58<01:00,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5471, loss: 0.0619, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [04:00<00:55,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5455, loss: 0.0621, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [04:01<00:54,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5461, loss: 0.0621, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [04:03<00:58,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5470, loss: 0.0620, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [04:05<00:53,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5473, loss: 0.0619, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:08<01:06,  2.01s/it]\u001b[A\n",
            "pearson(r): 0.5488, loss: 0.0619, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:09<00:58,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.5487, loss: 0.0620, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:10<00:52,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.5488, loss: 0.0619, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:12<00:49,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.5479, loss: 0.0619, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:14<00:47,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5484, loss: 0.0619, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:15<00:44,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.5475, loss: 0.0619, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:18<00:53,  1.99s/it]\u001b[A\n",
            "pearson(r): 0.5466, loss: 0.0620, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:19<00:45,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.5471, loss: 0.0620, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:21<00:40,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.5467, loss: 0.0619, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:22<00:37,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5460, loss: 0.0619, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:24<00:39,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.5452, loss: 0.0619, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:25<00:33,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.5459, loss: 0.0618, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:27<00:32,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.5459, loss: 0.0618, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:28<00:30,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5466, loss: 0.0616, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:30<00:29,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.5476, loss: 0.0616, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:32<00:30,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.5462, loss: 0.0617, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:33<00:26,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.5468, loss: 0.0616, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:35<00:25,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0616, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:36<00:22,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.5463, loss: 0.0615, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:37<00:20,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0615, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:39<00:19,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5471, loss: 0.0613, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:40<00:17,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.5464, loss: 0.0613, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:42<00:15,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.5463, loss: 0.0612, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:43<00:14,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.5466, loss: 0.0612, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:45<00:13,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5461, loss: 0.0612, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:46<00:12,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5465, loss: 0.0611, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:48<00:10,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.5478, loss: 0.0610, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:49<00:08,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.5483, loss: 0.0610, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:50<00:07,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.5487, loss: 0.0609, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:52<00:05,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.5494, loss: 0.0609, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:53<00:04,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.5491, loss: 0.0609, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:55<00:02,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.5491, loss: 0.0609, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:56<00:01,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.5485, loss: 0.0608, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [04:57<00:00,  1.65s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.6489, loss: 0.0733, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:48,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.6259, loss: 0.0710, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:52,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.5409, loss: 0.0856, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:01<01:04,  2.74it/s]\u001b[A\n",
            "pearson(r): 0.5727, loss: 0.0774, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<00:58,  3.00it/s]\u001b[A\n",
            "pearson(r): 0.5983, loss: 0.0794, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<00:56,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.6087, loss: 0.0829, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:01<00:51,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.6205, loss: 0.0877, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:02<00:49,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.6297, loss: 0.0859, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:48,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6348, loss: 0.0830, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<00:47,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.6422, loss: 0.0812, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:03<00:46,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.6354, loss: 0.0828, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:03<00:52,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.6325, loss: 0.0823, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<01:01,  2.73it/s]\u001b[A\n",
            "pearson(r): 0.6399, loss: 0.0798, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:04<00:55,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.6481, loss: 0.0815, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:04<00:51,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.6524, loss: 0.0803, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:49,  3.32it/s]\u001b[A\n",
            "pearson(r): 0.6586, loss: 0.0797, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:47,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.6608, loss: 0.0793, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:05<00:46,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.6534, loss: 0.0797, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:05<00:46,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6464, loss: 0.0810, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:44,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.6445, loss: 0.0809, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:06<00:48,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6416, loss: 0.0795, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:06<00:43,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.6386, loss: 0.0791, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:47,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.6348, loss: 0.0801, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:46,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.6306, loss: 0.0808, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:07<00:44,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.6349, loss: 0.0802, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:07<00:43,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.6365, loss: 0.0801, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:43,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6394, loss: 0.0796, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:08<00:41,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.6453, loss: 0.0793, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:08<00:38,  3.97it/s]\u001b[A\n",
            "pearson(r): 0.6440, loss: 0.0810, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:38,  3.97it/s]\u001b[A\n",
            "pearson(r): 0.6473, loss: 0.0802, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:08<00:38,  3.87it/s]\u001b[A\n",
            "pearson(r): 0.6508, loss: 0.0800, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:09<00:44,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.6541, loss: 0.0796, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:09<00:43,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.6519, loss: 0.0796, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:09<00:41,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.6509, loss: 0.0801, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:10<00:41,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6525, loss: 0.0800, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:10<00:41,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6538, loss: 0.0805, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:10<00:40,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0801, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:10<00:40,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6584, loss: 0.0802, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:11<00:38,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.6601, loss: 0.0807, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:11<00:37,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.6588, loss: 0.0808, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:11<00:37,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6605, loss: 0.0812, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:11<00:37,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.6587, loss: 0.0804, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:12<00:40,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.6583, loss: 0.0808, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:12<00:45,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.6592, loss: 0.0805, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:12<00:43,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.6622, loss: 0.0803, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:13<00:39,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.6608, loss: 0.0806, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:13<00:46,  2.90it/s]\u001b[A\n",
            "pearson(r): 0.6589, loss: 0.0810, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:13<00:43,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.6609, loss: 0.0803, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:14<00:40,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.6591, loss: 0.0805, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:14<00:35,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6595, loss: 0.0803, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:14<00:33,  3.93it/s]\u001b[A\n",
            "pearson(r): 0.6603, loss: 0.0799, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:14<00:35,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.6591, loss: 0.0803, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:15<00:36,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6584, loss: 0.0806, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:15<00:40,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0809, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:15<00:37,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.6572, loss: 0.0806, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:16<00:36,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.6559, loss: 0.0805, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:16<00:35,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6540, loss: 0.0805, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:16<00:34,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.6536, loss: 0.0803, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:16<00:33,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.6530, loss: 0.0806, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:17<00:36,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6541, loss: 0.0805, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:17<00:34,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.6519, loss: 0.0812, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:18<00:42,  2.78it/s]\u001b[A\n",
            "pearson(r): 0.6522, loss: 0.0809, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:18<00:43,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.6525, loss: 0.0809, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:18<00:40,  2.90it/s]\u001b[A\n",
            "pearson(r): 0.6539, loss: 0.0807, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:19<00:37,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.6549, loss: 0.0807, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:19<00:34,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6545, loss: 0.0807, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:19<00:35,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.6540, loss: 0.0805, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:19<00:32,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6532, loss: 0.0810, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:20<00:30,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.6510, loss: 0.0811, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:20<00:38,  2.86it/s]\u001b[A\n",
            "pearson(r): 0.6518, loss: 0.0811, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:20<00:36,  3.06it/s]\u001b[A\n",
            "pearson(r): 0.6520, loss: 0.0806, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:21<00:37,  2.94it/s]\u001b[A\n",
            "pearson(r): 0.6540, loss: 0.0803, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:21<00:34,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.6520, loss: 0.0806, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:22<00:41,  2.60it/s]\u001b[A\n",
            "pearson(r): 0.6530, loss: 0.0806, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:22<00:36,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.6511, loss: 0.0806, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:22<00:36,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.6509, loss: 0.0806, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:23<00:34,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.6496, loss: 0.0807, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:23<00:32,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.6503, loss: 0.0807, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:23<00:38,  2.62it/s]\u001b[A\n",
            "pearson(r): 0.6510, loss: 0.0808, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:24<00:35,  2.85it/s]\u001b[A\n",
            "pearson(r): 0.6501, loss: 0.0809, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:24<00:36,  2.75it/s]\u001b[A\n",
            "pearson(r): 0.6527, loss: 0.0808, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:24<00:33,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.6537, loss: 0.0809, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:25<00:30,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.6542, loss: 0.0811, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:25<00:28,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.6544, loss: 0.0809, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:25<00:27,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6538, loss: 0.0809, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:25<00:25,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.6515, loss: 0.0811, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:26<00:27,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.6533, loss: 0.0809, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:26<00:25,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.6502, loss: 0.0815, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:26<00:28,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.6499, loss: 0.0814, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:27<00:26,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.6511, loss: 0.0810, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:27<00:25,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6516, loss: 0.0807, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:27<00:28,  3.12it/s]\u001b[A\n",
            "pearson(r): 0.6498, loss: 0.0807, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:27<00:27,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.6487, loss: 0.0804, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:28<00:26,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.6495, loss: 0.0804, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:28<00:24,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.6494, loss: 0.0802, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:28<00:26,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.6508, loss: 0.0799, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:29<00:24,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.6510, loss: 0.0800, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:29<00:22,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.6502, loss: 0.0799, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:29<00:22,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.6501, loss: 0.0801, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:29<00:24,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.6494, loss: 0.0799, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:30<00:24,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.6504, loss: 0.0797, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:30<00:22,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6498, loss: 0.0797, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:30<00:21,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.6485, loss: 0.0799, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:31<00:21,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.6489, loss: 0.0800, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:31<00:20,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.6489, loss: 0.0802, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:31<00:19,  3.80it/s]\u001b[A\n",
            "pearson(r): 0.6506, loss: 0.0801, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:31<00:20,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.6511, loss: 0.0799, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:32<00:19,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.6509, loss: 0.0799, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:32<00:19,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6499, loss: 0.0799, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:32<00:21,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.6499, loss: 0.0799, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:33<00:20,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.6501, loss: 0.0798, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:33<00:20,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.6509, loss: 0.0797, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:33<00:21,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.6512, loss: 0.0798, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:33<00:20,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.6517, loss: 0.0797, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:34<00:19,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6518, loss: 0.0795, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:34<00:19,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.6539, loss: 0.0794, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:34<00:18,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.6545, loss: 0.0794, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:35<00:17,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.6553, loss: 0.0793, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:35<00:18,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.6560, loss: 0.0794, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:35<00:17,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0792, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:35<00:15,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.6568, loss: 0.0792, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:36<00:15,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.6568, loss: 0.0792, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:36<00:15,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.6570, loss: 0.0790, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:36<00:15,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.6567, loss: 0.0791, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:36<00:15,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0789, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:37<00:14,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.6564, loss: 0.0790, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:37<00:18,  2.88it/s]\u001b[A\n",
            "pearson(r): 0.6573, loss: 0.0789, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:38<00:17,  3.00it/s]\u001b[A\n",
            "pearson(r): 0.6573, loss: 0.0789, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:38<00:17,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0789, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:38<00:15,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0788, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:38<00:15,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.6572, loss: 0.0786, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:39<00:15,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6578, loss: 0.0786, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:39<00:15,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.6572, loss: 0.0785, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:39<00:14,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.6577, loss: 0.0784, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:40<00:13,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.6594, loss: 0.0781, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:40<00:12,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.6592, loss: 0.0783, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:40<00:11,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6591, loss: 0.0782, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:40<00:11,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.6595, loss: 0.0784, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:41<00:11,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6597, loss: 0.0783, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:41<00:11,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.6595, loss: 0.0785, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:41<00:11,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.6594, loss: 0.0784, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:42<00:11,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.6589, loss: 0.0784, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:42<00:10,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.6584, loss: 0.0783, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:42<00:11,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6584, loss: 0.0782, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:43<00:11,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.6571, loss: 0.0786, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:43<00:13,  2.65it/s]\u001b[A\n",
            "pearson(r): 0.6575, loss: 0.0784, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:43<00:12,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.6573, loss: 0.0783, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:44<00:10,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0785, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:44<00:09,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.6568, loss: 0.0787, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:44<00:09,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.6565, loss: 0.0788, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:44<00:08,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.6566, loss: 0.0788, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:45<00:07,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.6572, loss: 0.0788, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:45<00:07,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.6579, loss: 0.0787, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:45<00:07,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.6583, loss: 0.0786, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:46<00:08,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.6593, loss: 0.0785, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:46<00:10,  2.50it/s]\u001b[A\n",
            "pearson(r): 0.6608, loss: 0.0782, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:47<00:09,  2.56it/s]\u001b[A\n",
            "pearson(r): 0.6603, loss: 0.0782, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:47<00:08,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.6601, loss: 0.0782, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:47<00:08,  2.47it/s]\u001b[A\n",
            "pearson(r): 0.6591, loss: 0.0783, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:48<00:07,  2.75it/s]\u001b[A\n",
            "pearson(r): 0.6590, loss: 0.0783, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:48<00:06,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.6592, loss: 0.0784, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:48<00:05,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.6581, loss: 0.0785, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:49<00:05,  3.06it/s]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0787, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:49<00:05,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.6567, loss: 0.0786, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:49<00:05,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.6566, loss: 0.0786, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:49<00:04,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.6566, loss: 0.0786, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:50<00:04,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6565, loss: 0.0786, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:50<00:04,  2.78it/s]\u001b[A\n",
            "pearson(r): 0.6571, loss: 0.0786, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:51<00:04,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.6576, loss: 0.0787, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:51<00:03,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.6586, loss: 0.0786, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:51<00:03,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.6577, loss: 0.0787, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:52<00:03,  2.95it/s]\u001b[A\n",
            "pearson(r): 0.6581, loss: 0.0786, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:52<00:02,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.6577, loss: 0.0785, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:52<00:02,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6572, loss: 0.0786, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:52<00:01,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.6564, loss: 0.0785, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:53<00:01,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6566, loss: 0.0785, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:53<00:01,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0785, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:53<00:00,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6571, loss: 0.0785, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.91it/s]\u001b[A\n",
            "pearson(r): 0.6570, loss: 0.0786, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:54<00:00,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.6565, loss: 0.0788, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.6080, loss: 0.0662, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:03<08:58,  3.01s/it]\u001b[A\n",
            "pearson(r): 0.6456, loss: 0.0623, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:04<07:16,  2.45s/it]\u001b[A\n",
            "pearson(r): 0.6901, loss: 0.0546, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:05<06:16,  2.13s/it]\u001b[A\n",
            "pearson(r): 0.6555, loss: 0.0550, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:06<05:36,  1.91s/it]\u001b[A\n",
            "pearson(r): 0.6684, loss: 0.0537, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:09<05:57,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.6410, loss: 0.0577, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:11<06:05,  2.10s/it]\u001b[A\n",
            "pearson(r): 0.6231, loss: 0.0577, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:13<05:31,  1.92s/it]\u001b[A\n",
            "pearson(r): 0.6368, loss: 0.0559, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:14<05:07,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.6312, loss: 0.0550, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:15<04:48,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6061, loss: 0.0567, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:18<05:05,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.5991, loss: 0.0567, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:19<04:49,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6182, loss: 0.0554, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:21<04:39,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6115, loss: 0.0553, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:22<04:31,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6229, loss: 0.0541, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:25<05:40,  2.05s/it]\u001b[A\n",
            "pearson(r): 0.6185, loss: 0.0542, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:27<05:36,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.6211, loss: 0.0534, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:29<05:14,  1.92s/it]\u001b[A\n",
            "pearson(r): 0.6205, loss: 0.0533, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:30<04:58,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.6154, loss: 0.0535, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:32<04:47,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6226, loss: 0.0528, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:34<04:29,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6276, loss: 0.0516, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:35<04:19,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6282, loss: 0.0514, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:36<04:05,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6307, loss: 0.0522, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:38<04:02,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6300, loss: 0.0519, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:39<03:55,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6394, loss: 0.0513, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:41<03:48,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6351, loss: 0.0520, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:43<04:15,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6326, loss: 0.0521, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:44<04:03,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6308, loss: 0.0527, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:45<03:40,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6301, loss: 0.0521, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:48<04:18,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.6274, loss: 0.0525, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:49<04:05,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6329, loss: 0.0517, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:51<04:14,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6300, loss: 0.0517, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:52<04:02,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6302, loss: 0.0515, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:54<03:55,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6255, loss: 0.0519, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:55<03:42,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6253, loss: 0.0515, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:57<03:31,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.6253, loss: 0.0513, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:58<03:32,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6280, loss: 0.0511, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:59<03:16,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.6277, loss: 0.0512, reg_loss: 0.0000 ||:  21%|██        | 37/180 [01:01<03:22,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6340, loss: 0.0506, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:02<03:28,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6327, loss: 0.0503, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:04<03:26,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6335, loss: 0.0503, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:06<03:38,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6340, loss: 0.0505, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:08<03:55,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6373, loss: 0.0503, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:09<03:30,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6365, loss: 0.0504, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:10<03:12,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6381, loss: 0.0504, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:11<03:18,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6375, loss: 0.0504, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:13<03:32,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6386, loss: 0.0506, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:14<03:19,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.6394, loss: 0.0502, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:16<03:19,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6389, loss: 0.0501, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:18<03:33,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6411, loss: 0.0500, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:19<03:13,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6431, loss: 0.0500, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:21<03:16,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6408, loss: 0.0501, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:23<03:41,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6407, loss: 0.0501, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:24<03:28,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6378, loss: 0.0504, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:26<03:15,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6400, loss: 0.0503, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:27<03:06,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6409, loss: 0.0506, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:28<02:52,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.6400, loss: 0.0503, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:29<02:49,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.6364, loss: 0.0502, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:31<02:47,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.6325, loss: 0.0507, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:34<03:47,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.6316, loss: 0.0508, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:35<03:30,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6328, loss: 0.0509, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:38<04:19,  2.16s/it]\u001b[A\n",
            "pearson(r): 0.6334, loss: 0.0508, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:40<03:56,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.6338, loss: 0.0506, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:41<03:35,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.6303, loss: 0.0508, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:44<04:15,  2.19s/it]\u001b[A\n",
            "pearson(r): 0.6314, loss: 0.0507, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:46<03:55,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.6314, loss: 0.0505, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:48<03:42,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.6336, loss: 0.0504, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:50<03:38,  1.92s/it]\u001b[A\n",
            "pearson(r): 0.6347, loss: 0.0503, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:51<03:17,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6371, loss: 0.0501, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:53<03:07,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6353, loss: 0.0500, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:54<02:58,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6370, loss: 0.0501, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:56<02:52,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6396, loss: 0.0499, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:57<02:45,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6416, loss: 0.0497, reg_loss: 0.0000 ||:  40%|████      | 72/180 [01:58<02:41,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.6415, loss: 0.0497, reg_loss: 0.0000 ||:  41%|████      | 73/180 [01:59<02:24,  1.35s/it]\u001b[A\n",
            "pearson(r): 0.6420, loss: 0.0496, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:01<02:25,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.6399, loss: 0.0497, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:02<02:28,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.6365, loss: 0.0501, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:04<02:31,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6364, loss: 0.0501, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:06<02:52,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6384, loss: 0.0499, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:07<02:37,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6379, loss: 0.0499, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:09<02:29,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6347, loss: 0.0504, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:12<03:14,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.6330, loss: 0.0504, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:13<03:05,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6341, loss: 0.0505, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:15<02:48,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6353, loss: 0.0502, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:16<02:38,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6348, loss: 0.0502, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:18<02:38,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6334, loss: 0.0503, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:20<02:55,  1.85s/it]\u001b[A\n",
            "pearson(r): 0.6321, loss: 0.0503, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:22<02:44,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6332, loss: 0.0502, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:23<02:41,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6318, loss: 0.0504, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:25<02:33,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.6317, loss: 0.0504, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:26<02:22,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6312, loss: 0.0505, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:27<02:12,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6308, loss: 0.0504, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:29<02:11,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6305, loss: 0.0505, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:30<02:06,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6313, loss: 0.0505, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:32<02:13,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6316, loss: 0.0505, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:34<02:10,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6321, loss: 0.0506, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:35<02:11,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6322, loss: 0.0506, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:37<02:12,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6310, loss: 0.0507, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:40<02:47,  2.02s/it]\u001b[A\n",
            "pearson(r): 0.6309, loss: 0.0509, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:43<03:10,  2.32s/it]\u001b[A\n",
            "pearson(r): 0.6306, loss: 0.0508, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:44<02:50,  2.10s/it]\u001b[A\n",
            "pearson(r): 0.6314, loss: 0.0508, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:47<03:09,  2.37s/it]\u001b[A\n",
            "pearson(r): 0.6327, loss: 0.0508, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:50<03:08,  2.39s/it]\u001b[A\n",
            "pearson(r): 0.6330, loss: 0.0507, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:52<02:48,  2.16s/it]\u001b[A\n",
            "pearson(r): 0.6342, loss: 0.0505, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:53<02:32,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.6342, loss: 0.0505, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:55<02:19,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.6337, loss: 0.0506, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:56<02:18,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.6335, loss: 0.0506, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:59<02:31,  2.05s/it]\u001b[A\n",
            "pearson(r): 0.6322, loss: 0.0507, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [03:01<02:26,  2.01s/it]\u001b[A\n",
            "pearson(r): 0.6299, loss: 0.0508, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [03:02<02:13,  1.85s/it]\u001b[A\n",
            "pearson(r): 0.6298, loss: 0.0509, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [03:04<02:01,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6283, loss: 0.0510, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:05<01:55,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6273, loss: 0.0513, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:07<01:58,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6236, loss: 0.0516, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:09<01:51,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6218, loss: 0.0517, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:10<01:46,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6194, loss: 0.0519, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:12<01:45,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6181, loss: 0.0522, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:13<01:39,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6172, loss: 0.0522, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:15<01:39,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6163, loss: 0.0523, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:17<01:49,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6158, loss: 0.0523, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:18<01:43,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.6160, loss: 0.0524, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:20<01:34,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6155, loss: 0.0524, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:21<01:27,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6139, loss: 0.0527, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:22<01:23,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6130, loss: 0.0528, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:23<01:17,  1.34s/it]\u001b[A\n",
            "pearson(r): 0.6121, loss: 0.0530, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:24<01:12,  1.27s/it]\u001b[A\n",
            "pearson(r): 0.6104, loss: 0.0532, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:26<01:15,  1.34s/it]\u001b[A\n",
            "pearson(r): 0.6117, loss: 0.0531, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:28<01:17,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6113, loss: 0.0531, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:30<01:27,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6096, loss: 0.0534, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:31<01:25,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6091, loss: 0.0535, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:33<01:21,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6091, loss: 0.0536, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:35<01:24,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6089, loss: 0.0537, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:36<01:17,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6087, loss: 0.0537, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:38<01:22,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6079, loss: 0.0537, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:39<01:18,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6075, loss: 0.0537, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:41<01:14,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6079, loss: 0.0537, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:42<01:13,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6072, loss: 0.0538, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:44<01:13,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6069, loss: 0.0538, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:46<01:10,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6074, loss: 0.0538, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:47<01:05,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6056, loss: 0.0540, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:50<01:23,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.6058, loss: 0.0540, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:52<01:25,  2.08s/it]\u001b[A\n",
            "pearson(r): 0.6032, loss: 0.0542, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:54<01:16,  1.92s/it]\u001b[A\n",
            "pearson(r): 0.6030, loss: 0.0543, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:55<01:09,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6025, loss: 0.0543, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:57<01:02,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6018, loss: 0.0543, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [03:58<00:57,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6024, loss: 0.0543, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [04:00<01:01,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6018, loss: 0.0543, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [04:01<00:54,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6020, loss: 0.0543, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [04:03<00:53,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6018, loss: 0.0543, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:05<00:56,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6019, loss: 0.0543, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:07<00:53,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6033, loss: 0.0544, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:08<00:51,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6015, loss: 0.0546, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:11<01:01,  2.07s/it]\u001b[A\n",
            "pearson(r): 0.6026, loss: 0.0546, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:12<00:52,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.6019, loss: 0.0547, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:14<00:47,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6018, loss: 0.0547, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:15<00:42,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6016, loss: 0.0549, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:17<00:39,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6019, loss: 0.0548, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:18<00:37,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.6012, loss: 0.0549, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:20<00:40,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.6017, loss: 0.0548, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:21<00:36,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6019, loss: 0.0547, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:23<00:34,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6028, loss: 0.0546, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:25<00:33,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6025, loss: 0.0548, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:26<00:31,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6027, loss: 0.0547, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:28<00:30,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6019, loss: 0.0548, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:29<00:29,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6014, loss: 0.0548, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:32<00:29,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.6020, loss: 0.0548, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:33<00:28,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0550, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:35<00:25,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0550, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:36<00:23,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6013, loss: 0.0550, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:38<00:21,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6025, loss: 0.0549, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:39<00:18,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6024, loss: 0.0549, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:41<00:16,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6020, loss: 0.0549, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:43<00:18,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.6020, loss: 0.0549, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:45<00:15,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6010, loss: 0.0550, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:47<00:13,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6002, loss: 0.0551, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:48<00:11,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6006, loss: 0.0550, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:50<00:09,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.6009, loss: 0.0551, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:51<00:08,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6007, loss: 0.0551, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:53<00:06,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6006, loss: 0.0552, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:54<00:04,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6006, loss: 0.0552, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:55<00:02,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.5996, loss: 0.0552, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:56<00:01,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.5995, loss: 0.0553, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [04:58<00:00,  1.66s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.7091, loss: 0.0773, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:54,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.6479, loss: 0.0902, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:50,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6893, loss: 0.0798, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:00<00:47,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.7220, loss: 0.0692, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<00:48,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7039, loss: 0.0644, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<00:50,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7220, loss: 0.0656, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:01<00:46,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7012, loss: 0.0731, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:01<00:45,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.6944, loss: 0.0749, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:45,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.6934, loss: 0.0744, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<00:45,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6847, loss: 0.0758, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:02<00:42,  4.04it/s]\u001b[A\n",
            "pearson(r): 0.6717, loss: 0.0785, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:02<00:42,  3.97it/s]\u001b[A\n",
            "pearson(r): 0.6740, loss: 0.0770, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<00:41,  4.02it/s]\u001b[A\n",
            "pearson(r): 0.6867, loss: 0.0760, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:03<00:55,  3.02it/s]\u001b[A\n",
            "pearson(r): 0.6759, loss: 0.0793, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:03<00:51,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6830, loss: 0.0776, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:45,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.6803, loss: 0.0787, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:44,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.6791, loss: 0.0798, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:04<00:43,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.6774, loss: 0.0796, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:04<00:45,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.6794, loss: 0.0789, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:44,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.6692, loss: 0.0799, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:05<00:46,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.6697, loss: 0.0793, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:05<00:45,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6696, loss: 0.0791, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:45,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6693, loss: 0.0788, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:44,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.6728, loss: 0.0784, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:06<00:47,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6746, loss: 0.0773, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:06<00:45,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.6723, loss: 0.0776, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:40,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.6775, loss: 0.0787, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:07<00:39,  3.87it/s]\u001b[A\n",
            "pearson(r): 0.6721, loss: 0.0789, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:07<00:43,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6730, loss: 0.0798, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:47,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.6702, loss: 0.0793, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:08<00:46,  3.22it/s]\u001b[A\n",
            "pearson(r): 0.6715, loss: 0.0789, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:08<00:44,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.6724, loss: 0.0787, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:08<00:39,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.6716, loss: 0.0783, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:09<00:36,  3.98it/s]\u001b[A\n",
            "pearson(r): 0.6717, loss: 0.0785, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:09<00:37,  3.89it/s]\u001b[A\n",
            "pearson(r): 0.6736, loss: 0.0781, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:09<00:40,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.6748, loss: 0.0782, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:09<00:40,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.6750, loss: 0.0781, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:10<00:42,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.6764, loss: 0.0776, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:10<00:46,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.6731, loss: 0.0781, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:11<00:51,  2.76it/s]\u001b[A\n",
            "pearson(r): 0.6737, loss: 0.0782, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:11<00:48,  2.88it/s]\u001b[A\n",
            "pearson(r): 0.6749, loss: 0.0782, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:11<00:46,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.6727, loss: 0.0784, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:12<00:55,  2.47it/s]\u001b[A\n",
            "pearson(r): 0.6720, loss: 0.0785, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:12<00:55,  2.49it/s]\u001b[A\n",
            "pearson(r): 0.6679, loss: 0.0794, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:13<00:49,  2.75it/s]\u001b[A\n",
            "pearson(r): 0.6654, loss: 0.0804, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:13<00:49,  2.70it/s]\u001b[A\n",
            "pearson(r): 0.6635, loss: 0.0807, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:13<00:47,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.6613, loss: 0.0807, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:14<00:47,  2.78it/s]\u001b[A\n",
            "pearson(r): 0.6621, loss: 0.0803, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:14<00:43,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.6625, loss: 0.0808, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:14<00:40,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.6624, loss: 0.0808, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:15<00:43,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.6657, loss: 0.0803, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:15<00:42,  3.00it/s]\u001b[A\n",
            "pearson(r): 0.6647, loss: 0.0805, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:15<00:42,  3.02it/s]\u001b[A\n",
            "pearson(r): 0.6649, loss: 0.0809, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:15<00:40,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.6671, loss: 0.0806, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:16<00:39,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.6692, loss: 0.0802, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:16<00:36,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.6687, loss: 0.0807, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:16<00:35,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6688, loss: 0.0806, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:17<00:34,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6697, loss: 0.0803, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:17<00:34,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6705, loss: 0.0796, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:17<00:32,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.6695, loss: 0.0796, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:18<00:42,  2.85it/s]\u001b[A\n",
            "pearson(r): 0.6702, loss: 0.0796, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:18<00:41,  2.88it/s]\u001b[A\n",
            "pearson(r): 0.6706, loss: 0.0795, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:18<00:38,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.6718, loss: 0.0794, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:18<00:35,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.6716, loss: 0.0793, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:19<00:36,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.6736, loss: 0.0795, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:19<00:35,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.6730, loss: 0.0795, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:19<00:32,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.6741, loss: 0.0795, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:20<00:32,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6741, loss: 0.0797, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:20<00:30,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.6728, loss: 0.0797, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:20<00:31,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6710, loss: 0.0801, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:20<00:31,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.6710, loss: 0.0798, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:21<00:31,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.6708, loss: 0.0801, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:21<00:30,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6703, loss: 0.0805, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:21<00:32,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.6699, loss: 0.0812, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:22<00:30,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.6705, loss: 0.0812, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:22<00:29,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.6703, loss: 0.0811, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:22<00:28,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.6723, loss: 0.0810, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:22<00:28,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6733, loss: 0.0809, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:23<00:26,  3.91it/s]\u001b[A\n",
            "pearson(r): 0.6747, loss: 0.0808, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:23<00:29,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.6745, loss: 0.0806, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:23<00:30,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.6747, loss: 0.0807, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:24<00:27,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.6752, loss: 0.0806, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:24<00:29,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.6758, loss: 0.0804, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:24<00:31,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.6755, loss: 0.0810, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:25<00:29,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.6759, loss: 0.0807, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:25<00:29,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.6733, loss: 0.0813, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:25<00:35,  2.67it/s]\u001b[A\n",
            "pearson(r): 0.6719, loss: 0.0813, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:26<00:33,  2.74it/s]\u001b[A\n",
            "pearson(r): 0.6713, loss: 0.0814, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:26<00:30,  3.02it/s]\u001b[A\n",
            "pearson(r): 0.6723, loss: 0.0817, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:26<00:28,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.6734, loss: 0.0814, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:27<00:26,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.6742, loss: 0.0812, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:27<00:25,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6766, loss: 0.0808, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:27<00:24,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.6774, loss: 0.0806, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:27<00:23,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.6758, loss: 0.0808, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:28<00:23,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.6769, loss: 0.0808, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:28<00:22,  3.80it/s]\u001b[A\n",
            "pearson(r): 0.6753, loss: 0.0806, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:28<00:24,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.6766, loss: 0.0804, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:29<00:24,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.6768, loss: 0.0800, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:29<00:23,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.6766, loss: 0.0799, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:29<00:24,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.6768, loss: 0.0801, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:29<00:23,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.6781, loss: 0.0798, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:30<00:20,  3.83it/s]\u001b[A\n",
            "pearson(r): 0.6787, loss: 0.0798, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:30<00:20,  3.87it/s]\u001b[A\n",
            "pearson(r): 0.6786, loss: 0.0798, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:30<00:18,  4.12it/s]\u001b[A\n",
            "pearson(r): 0.6784, loss: 0.0800, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:30<00:20,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.6781, loss: 0.0800, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:31<00:19,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6769, loss: 0.0798, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:31<00:21,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.6788, loss: 0.0795, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:31<00:20,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.6798, loss: 0.0793, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:32<00:19,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.6789, loss: 0.0794, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:32<00:25,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.6781, loss: 0.0797, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:32<00:23,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.6760, loss: 0.0797, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:33<00:26,  2.58it/s]\u001b[A\n",
            "pearson(r): 0.6763, loss: 0.0794, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:33<00:24,  2.82it/s]\u001b[A\n",
            "pearson(r): 0.6770, loss: 0.0793, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:33<00:21,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.6760, loss: 0.0793, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:34<00:21,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.6758, loss: 0.0790, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:34<00:19,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.6759, loss: 0.0790, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:34<00:19,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.6761, loss: 0.0790, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:35<00:18,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.6760, loss: 0.0792, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:35<00:17,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6745, loss: 0.0793, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:35<00:21,  2.79it/s]\u001b[A\n",
            "pearson(r): 0.6745, loss: 0.0794, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:36<00:19,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.6726, loss: 0.0798, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:36<00:22,  2.57it/s]\u001b[A\n",
            "pearson(r): 0.6718, loss: 0.0802, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:36<00:20,  2.81it/s]\u001b[A\n",
            "pearson(r): 0.6728, loss: 0.0801, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:37<00:19,  2.96it/s]\u001b[A\n",
            "pearson(r): 0.6730, loss: 0.0800, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:37<00:17,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.6735, loss: 0.0798, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:37<00:19,  2.77it/s]\u001b[A\n",
            "pearson(r): 0.6735, loss: 0.0797, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:38<00:17,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.6732, loss: 0.0797, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:38<00:15,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.6736, loss: 0.0796, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:38<00:13,  3.86it/s]\u001b[A\n",
            "pearson(r): 0.6738, loss: 0.0795, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:38<00:14,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.6738, loss: 0.0796, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:39<00:15,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.6750, loss: 0.0795, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:39<00:14,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.6749, loss: 0.0795, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:39<00:14,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.6745, loss: 0.0796, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:40<00:12,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.6747, loss: 0.0796, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:40<00:11,  3.86it/s]\u001b[A\n",
            "pearson(r): 0.6752, loss: 0.0795, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:40<00:11,  3.91it/s]\u001b[A\n",
            "pearson(r): 0.6744, loss: 0.0798, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:40<00:12,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.6740, loss: 0.0796, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:41<00:11,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6739, loss: 0.0797, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:41<00:11,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.6752, loss: 0.0795, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:41<00:11,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.6763, loss: 0.0794, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:42<00:13,  2.86it/s]\u001b[A\n",
            "pearson(r): 0.6769, loss: 0.0793, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:42<00:13,  2.80it/s]\u001b[A\n",
            "pearson(r): 0.6756, loss: 0.0798, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:43<00:15,  2.45it/s]\u001b[A\n",
            "pearson(r): 0.6753, loss: 0.0798, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:43<00:14,  2.57it/s]\u001b[A\n",
            "pearson(r): 0.6749, loss: 0.0798, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:43<00:13,  2.72it/s]\u001b[A\n",
            "pearson(r): 0.6753, loss: 0.0798, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:44<00:13,  2.63it/s]\u001b[A\n",
            "pearson(r): 0.6764, loss: 0.0795, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:44<00:12,  2.79it/s]\u001b[A\n",
            "pearson(r): 0.6763, loss: 0.0796, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:44<00:10,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.6767, loss: 0.0797, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:44<00:09,  3.32it/s]\u001b[A\n",
            "pearson(r): 0.6751, loss: 0.0800, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:45<00:08,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.6746, loss: 0.0802, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:45<00:10,  2.79it/s]\u001b[A\n",
            "pearson(r): 0.6745, loss: 0.0803, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:46<00:09,  3.00it/s]\u001b[A\n",
            "pearson(r): 0.6744, loss: 0.0802, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:46<00:08,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.6760, loss: 0.0801, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:46<00:08,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.6758, loss: 0.0802, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:47<00:09,  2.61it/s]\u001b[A\n",
            "pearson(r): 0.6769, loss: 0.0801, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:47<00:08,  2.87it/s]\u001b[A\n",
            "pearson(r): 0.6775, loss: 0.0802, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:47<00:07,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.6769, loss: 0.0803, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:47<00:06,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.6768, loss: 0.0803, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:48<00:05,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.6776, loss: 0.0801, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:48<00:05,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6774, loss: 0.0802, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:48<00:05,  3.94it/s]\u001b[A\n",
            "pearson(r): 0.6777, loss: 0.0803, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:48<00:04,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.6782, loss: 0.0801, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:49<00:04,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.6778, loss: 0.0800, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:49<00:04,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.6779, loss: 0.0800, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:49<00:04,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.6786, loss: 0.0801, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:50<00:04,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.6779, loss: 0.0801, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:50<00:04,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.6776, loss: 0.0801, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:50<00:04,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.6777, loss: 0.0799, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:50<00:03,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.6775, loss: 0.0799, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:51<00:03,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.6785, loss: 0.0798, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:51<00:02,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.6784, loss: 0.0797, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:51<00:02,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.6787, loss: 0.0796, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:52<00:02,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.6788, loss: 0.0797, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:52<00:02,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.6777, loss: 0.0798, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:52<00:01,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.6777, loss: 0.0799, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:53<00:01,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.6777, loss: 0.0799, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:53<00:01,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.6779, loss: 0.0798, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:53<00:00,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.6781, loss: 0.0797, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.6788, loss: 0.0796, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:54<00:00,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.6787, loss: 0.0795, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.7231, loss: 0.0442, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:01<04:43,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6749, loss: 0.0429, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:02<04:29,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6802, loss: 0.0396, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:04<04:21,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7149, loss: 0.0393, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:05<04:21,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7049, loss: 0.0391, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:08<05:41,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.7174, loss: 0.0383, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:10<05:07,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7121, loss: 0.0389, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:11<04:49,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.6906, loss: 0.0414, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:13<04:43,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6925, loss: 0.0426, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:14<04:30,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6943, loss: 0.0434, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:16<04:56,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6748, loss: 0.0447, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:17<04:25,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6712, loss: 0.0454, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:19<04:12,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6663, loss: 0.0468, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:20<04:00,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6756, loss: 0.0470, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:21<03:52,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.6724, loss: 0.0471, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:24<04:30,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6726, loss: 0.0468, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:26<05:12,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.6799, loss: 0.0470, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:27<04:43,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6762, loss: 0.0468, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:29<04:54,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.6797, loss: 0.0460, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:30<04:13,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6689, loss: 0.0480, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:32<04:15,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6699, loss: 0.0478, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:34<04:12,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6706, loss: 0.0479, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:35<04:07,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6710, loss: 0.0475, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:36<03:48,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6769, loss: 0.0466, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:38<03:45,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.6788, loss: 0.0462, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:40<04:13,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6808, loss: 0.0462, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:41<04:07,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6806, loss: 0.0462, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:43<04:03,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6787, loss: 0.0460, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:45<04:08,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6784, loss: 0.0457, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:46<04:06,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.6804, loss: 0.0454, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:48<04:16,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6771, loss: 0.0458, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:50<04:07,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6744, loss: 0.0457, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:51<03:58,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6744, loss: 0.0453, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:53<04:01,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6710, loss: 0.0453, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:54<03:51,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6715, loss: 0.0458, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:56<03:50,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6761, loss: 0.0454, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:57<03:37,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6728, loss: 0.0457, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:59<03:55,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6733, loss: 0.0456, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:01<03:46,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6771, loss: 0.0455, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:02<03:39,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6827, loss: 0.0454, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:04<03:30,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6871, loss: 0.0451, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:05<03:24,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.6838, loss: 0.0455, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:07<03:29,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6836, loss: 0.0455, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:08<03:25,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6816, loss: 0.0455, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:10<03:28,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6739, loss: 0.0464, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:13<04:27,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.6773, loss: 0.0462, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:15<04:40,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.6750, loss: 0.0463, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:17<04:37,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.6728, loss: 0.0465, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:19<04:33,  2.07s/it]\u001b[A\n",
            "pearson(r): 0.6699, loss: 0.0468, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:21<04:06,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6695, loss: 0.0468, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:22<03:59,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.6677, loss: 0.0472, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:24<03:42,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6654, loss: 0.0476, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:25<03:24,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6608, loss: 0.0481, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:27<03:15,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6592, loss: 0.0483, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:28<03:08,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6595, loss: 0.0483, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:30<03:27,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6605, loss: 0.0481, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:32<03:24,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6597, loss: 0.0483, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:33<03:17,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6585, loss: 0.0483, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:35<03:11,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6596, loss: 0.0483, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:36<03:01,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6591, loss: 0.0483, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:38<03:02,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6614, loss: 0.0482, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:41<03:55,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.6597, loss: 0.0485, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:42<03:41,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6610, loss: 0.0483, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:44<03:37,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.6621, loss: 0.0482, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:45<03:19,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6619, loss: 0.0483, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:48<03:35,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.6625, loss: 0.0483, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:49<03:12,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6615, loss: 0.0482, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:50<02:59,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6610, loss: 0.0483, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:52<02:59,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.6613, loss: 0.0485, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:54<02:58,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6635, loss: 0.0482, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:55<02:49,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6631, loss: 0.0483, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:58<03:37,  1.99s/it]\u001b[A\n",
            "pearson(r): 0.6616, loss: 0.0484, reg_loss: 0.0000 ||:  40%|████      | 72/180 [01:59<03:15,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.6609, loss: 0.0485, reg_loss: 0.0000 ||:  41%|████      | 73/180 [02:01<02:56,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6596, loss: 0.0487, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:03<03:05,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6598, loss: 0.0488, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:04<03:03,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6598, loss: 0.0487, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:06<02:51,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6624, loss: 0.0486, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:07<02:43,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6631, loss: 0.0484, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:09<03:00,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6644, loss: 0.0483, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:11<02:53,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6655, loss: 0.0482, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:13<02:58,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.6623, loss: 0.0485, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:15<03:06,  1.89s/it]\u001b[A\n",
            "pearson(r): 0.6609, loss: 0.0486, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:17<02:53,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6608, loss: 0.0485, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:18<02:44,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6593, loss: 0.0486, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:19<02:30,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6597, loss: 0.0486, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:22<02:46,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6588, loss: 0.0486, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:23<02:34,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6584, loss: 0.0486, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:24<02:26,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6587, loss: 0.0487, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:26<02:24,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.6565, loss: 0.0488, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:27<02:17,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6556, loss: 0.0490, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:29<02:16,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0491, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:31<02:29,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6555, loss: 0.0490, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:33<02:39,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.6554, loss: 0.0488, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:34<02:28,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6554, loss: 0.0487, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:36<02:23,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.6556, loss: 0.0488, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:37<02:11,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0489, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:39<02:05,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.6548, loss: 0.0488, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:40<01:57,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6548, loss: 0.0488, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:41<01:53,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.6565, loss: 0.0485, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:43<01:50,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0486, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:44<01:46,  1.34s/it]\u001b[A\n",
            "pearson(r): 0.6550, loss: 0.0487, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:45<01:46,  1.34s/it]\u001b[A\n",
            "pearson(r): 0.6550, loss: 0.0487, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:47<01:46,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.6542, loss: 0.0488, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:48<01:46,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.6561, loss: 0.0487, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:50<01:47,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.6563, loss: 0.0485, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:51<01:50,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6568, loss: 0.0485, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:53<01:51,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6573, loss: 0.0486, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [02:54<01:51,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6573, loss: 0.0487, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [02:56<01:44,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0488, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [02:59<02:16,  1.92s/it]\u001b[A\n",
            "pearson(r): 0.6565, loss: 0.0488, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:01<02:18,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.6564, loss: 0.0487, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:02<02:13,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.6560, loss: 0.0488, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:04<02:00,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6577, loss: 0.0486, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:06<01:58,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.6576, loss: 0.0487, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:08<02:02,  1.85s/it]\u001b[A\n",
            "pearson(r): 0.6567, loss: 0.0488, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:09<01:53,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.6554, loss: 0.0489, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:11<02:00,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6562, loss: 0.0488, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:13<01:54,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.6569, loss: 0.0487, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:16<02:06,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.6542, loss: 0.0490, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:17<01:52,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.6541, loss: 0.0491, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:18<01:42,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.6536, loss: 0.0492, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:21<02:03,  2.10s/it]\u001b[A\n",
            "pearson(r): 0.6533, loss: 0.0491, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:23<01:48,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6531, loss: 0.0491, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:24<01:41,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.6537, loss: 0.0490, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:26<01:29,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6533, loss: 0.0490, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:27<01:25,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6541, loss: 0.0489, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:28<01:20,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6540, loss: 0.0489, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:30<01:17,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6506, loss: 0.0493, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:33<01:41,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.6510, loss: 0.0493, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:34<01:28,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6516, loss: 0.0492, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:36<01:24,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.6519, loss: 0.0492, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:37<01:15,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.6528, loss: 0.0491, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:38<01:14,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6533, loss: 0.0491, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:40<01:09,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6529, loss: 0.0492, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:41<01:08,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6524, loss: 0.0492, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:43<01:12,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6525, loss: 0.0492, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:44<01:08,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6530, loss: 0.0493, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:46<01:05,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6520, loss: 0.0493, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:48<01:05,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.6523, loss: 0.0493, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:49<00:59,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6519, loss: 0.0494, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:50<00:56,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.6521, loss: 0.0494, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:52<00:56,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.6525, loss: 0.0494, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:53<00:56,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6505, loss: 0.0495, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [03:56<01:12,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.6507, loss: 0.0495, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [03:58<01:04,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.6501, loss: 0.0496, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [03:59<01:00,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6495, loss: 0.0496, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [04:00<00:53,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6480, loss: 0.0499, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:04<01:06,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.6481, loss: 0.0499, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:05<01:01,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.6482, loss: 0.0499, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:08<01:09,  2.26s/it]\u001b[A\n",
            "pearson(r): 0.6485, loss: 0.0498, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:10<00:59,  1.97s/it]\u001b[A\n",
            "pearson(r): 0.6491, loss: 0.0498, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:11<00:54,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.6501, loss: 0.0497, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:13<00:52,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.6497, loss: 0.0498, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:14<00:46,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6504, loss: 0.0497, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:16<00:44,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.6504, loss: 0.0497, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:18<00:41,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.6509, loss: 0.0497, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:20<00:43,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.6504, loss: 0.0497, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:21<00:39,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6511, loss: 0.0496, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:23<00:36,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6500, loss: 0.0496, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:24<00:33,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6495, loss: 0.0496, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:26<00:30,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6502, loss: 0.0496, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:27<00:25,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.6512, loss: 0.0496, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:29<00:28,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6515, loss: 0.0495, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:31<00:28,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6519, loss: 0.0495, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:32<00:25,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6514, loss: 0.0495, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:34<00:24,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6505, loss: 0.0496, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:35<00:23,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.6507, loss: 0.0496, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:37<00:22,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.6508, loss: 0.0496, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:39<00:20,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.6500, loss: 0.0497, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:40<00:16,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6497, loss: 0.0498, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:42<00:15,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.6497, loss: 0.0498, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:43<00:13,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6486, loss: 0.0498, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:45<00:12,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.6492, loss: 0.0497, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:46<00:10,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6489, loss: 0.0497, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:47<00:08,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.6490, loss: 0.0497, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:49<00:08,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.6484, loss: 0.0498, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:51<00:06,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6492, loss: 0.0497, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:52<00:04,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.6498, loss: 0.0496, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:53<00:02,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.6499, loss: 0.0496, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:55<00:01,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.6503, loss: 0.0496, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [04:56<00:00,  1.65s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.8025, loss: 0.0652, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:47,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.7811, loss: 0.0577, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:47,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7635, loss: 0.0625, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:00<00:46,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.7657, loss: 0.0629, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<00:44,  3.99it/s]\u001b[A\n",
            "pearson(r): 0.7506, loss: 0.0649, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<00:46,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.7603, loss: 0.0644, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:01<00:46,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.7555, loss: 0.0653, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:01<00:50,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7498, loss: 0.0648, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:51,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7421, loss: 0.0661, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<00:49,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.7484, loss: 0.0637, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:02<00:47,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7564, loss: 0.0634, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:03<00:45,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.7532, loss: 0.0646, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<00:45,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.7535, loss: 0.0644, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:03<00:45,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7585, loss: 0.0632, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:03<00:44,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.7648, loss: 0.0624, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:45,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.7625, loss: 0.0620, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:48,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.7527, loss: 0.0632, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:04<00:46,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7524, loss: 0.0639, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:04<00:44,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7501, loss: 0.0647, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:50,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.7516, loss: 0.0635, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:05<00:48,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.7406, loss: 0.0652, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:05<00:43,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.7398, loss: 0.0653, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:43,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7426, loss: 0.0647, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:44,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7438, loss: 0.0650, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:06<00:45,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.7456, loss: 0.0654, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:07<00:45,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.7482, loss: 0.0653, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:40,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.7507, loss: 0.0653, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:07<00:43,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7530, loss: 0.0654, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:07<00:44,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.7535, loss: 0.0653, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:49,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.7552, loss: 0.0651, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:08<00:52,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.7498, loss: 0.0659, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:08<00:46,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.7520, loss: 0.0655, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:09<00:40,  3.65it/s]\u001b[A\n",
            "pearson(r): 0.7540, loss: 0.0649, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:09<00:39,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.7560, loss: 0.0648, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:09<00:39,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.7569, loss: 0.0644, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:09<00:41,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7580, loss: 0.0639, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:10<00:42,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.7577, loss: 0.0638, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:10<00:41,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7591, loss: 0.0632, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:10<00:39,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7546, loss: 0.0643, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:11<00:36,  3.90it/s]\u001b[A\n",
            "pearson(r): 0.7549, loss: 0.0639, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:11<00:36,  3.85it/s]\u001b[A\n",
            "pearson(r): 0.7593, loss: 0.0633, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:11<00:35,  3.90it/s]\u001b[A\n",
            "pearson(r): 0.7589, loss: 0.0630, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:11<00:35,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.7561, loss: 0.0636, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:12<00:41,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7554, loss: 0.0636, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:12<00:40,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7558, loss: 0.0638, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:12<00:39,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7585, loss: 0.0636, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:13<00:38,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7566, loss: 0.0639, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:13<00:34,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.7573, loss: 0.0635, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:13<00:32,  4.02it/s]\u001b[A\n",
            "pearson(r): 0.7565, loss: 0.0643, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:13<00:43,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7560, loss: 0.0646, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:14<00:40,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.7558, loss: 0.0644, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:14<00:38,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.7553, loss: 0.0645, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:14<00:33,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.7568, loss: 0.0642, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:14<00:33,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.7571, loss: 0.0643, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:15<00:33,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.7578, loss: 0.0641, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:15<00:43,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.7584, loss: 0.0639, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:16<00:40,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7576, loss: 0.0638, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:16<00:36,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.7573, loss: 0.0638, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:16<00:33,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7564, loss: 0.0638, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:16<00:32,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.7553, loss: 0.0640, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:17<00:34,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.7557, loss: 0.0640, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:17<00:32,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.7564, loss: 0.0638, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:17<00:34,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7553, loss: 0.0639, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:17<00:32,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7558, loss: 0.0641, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:18<00:31,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7547, loss: 0.0642, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:18<00:31,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.7560, loss: 0.0639, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:18<00:28,  3.95it/s]\u001b[A\n",
            "pearson(r): 0.7559, loss: 0.0638, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:18<00:31,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.7558, loss: 0.0637, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:19<00:30,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.7549, loss: 0.0637, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:19<00:36,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7549, loss: 0.0636, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:19<00:33,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.7537, loss: 0.0641, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:20<00:41,  2.65it/s]\u001b[A\n",
            "pearson(r): 0.7536, loss: 0.0641, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:20<00:40,  2.65it/s]\u001b[A\n",
            "pearson(r): 0.7556, loss: 0.0639, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:21<00:37,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.7564, loss: 0.0637, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:21<00:35,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.7578, loss: 0.0635, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:21<00:33,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.7594, loss: 0.0633, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:22<00:31,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.7600, loss: 0.0633, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:22<00:30,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.7596, loss: 0.0634, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:22<00:30,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.7590, loss: 0.0638, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:22<00:28,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7611, loss: 0.0635, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:23<00:27,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7601, loss: 0.0635, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:23<00:31,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.7598, loss: 0.0635, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:23<00:32,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7585, loss: 0.0635, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:24<00:34,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.7580, loss: 0.0636, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:24<00:32,  2.95it/s]\u001b[A\n",
            "pearson(r): 0.7570, loss: 0.0641, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:24<00:31,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.7573, loss: 0.0643, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:25<00:28,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.7586, loss: 0.0641, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:25<00:27,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7601, loss: 0.0639, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:25<00:26,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7595, loss: 0.0641, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:26<00:29,  3.05it/s]\u001b[A\n",
            "pearson(r): 0.7597, loss: 0.0640, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:26<00:26,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7602, loss: 0.0640, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:26<00:25,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.7596, loss: 0.0643, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:27<00:31,  2.76it/s]\u001b[A\n",
            "pearson(r): 0.7593, loss: 0.0644, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:27<00:30,  2.86it/s]\u001b[A\n",
            "pearson(r): 0.7594, loss: 0.0644, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:27<00:28,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7592, loss: 0.0645, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:27<00:26,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.7593, loss: 0.0641, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:28<00:24,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7599, loss: 0.0641, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:28<00:24,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.7582, loss: 0.0643, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:29<00:28,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.7582, loss: 0.0643, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:29<00:24,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.7599, loss: 0.0642, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:29<00:22,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7594, loss: 0.0642, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:29<00:22,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7589, loss: 0.0645, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:30<00:24,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.7591, loss: 0.0645, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:30<00:28,  2.66it/s]\u001b[A\n",
            "pearson(r): 0.7588, loss: 0.0647, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:30<00:26,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.7577, loss: 0.0649, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:31<00:27,  2.75it/s]\u001b[A\n",
            "pearson(r): 0.7565, loss: 0.0653, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:31<00:30,  2.41it/s]\u001b[A\n",
            "pearson(r): 0.7552, loss: 0.0653, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:32<00:27,  2.63it/s]\u001b[A\n",
            "pearson(r): 0.7542, loss: 0.0655, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:32<00:26,  2.72it/s]\u001b[A\n",
            "pearson(r): 0.7537, loss: 0.0656, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:32<00:22,  3.12it/s]\u001b[A\n",
            "pearson(r): 0.7534, loss: 0.0656, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:32<00:21,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.7528, loss: 0.0659, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:33<00:20,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.7537, loss: 0.0657, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:33<00:22,  3.06it/s]\u001b[A\n",
            "pearson(r): 0.7531, loss: 0.0657, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:33<00:20,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.7543, loss: 0.0654, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:34<00:20,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.7542, loss: 0.0654, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:34<00:19,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7543, loss: 0.0653, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:34<00:18,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.7544, loss: 0.0652, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:35<00:18,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.7541, loss: 0.0651, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:35<00:17,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7537, loss: 0.0651, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:35<00:19,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.7545, loss: 0.0652, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:35<00:17,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7536, loss: 0.0653, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:36<00:17,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7542, loss: 0.0651, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:36<00:16,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7537, loss: 0.0652, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:36<00:15,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.7523, loss: 0.0657, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:37<00:19,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.7523, loss: 0.0657, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:37<00:19,  2.88it/s]\u001b[A\n",
            "pearson(r): 0.7535, loss: 0.0656, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:37<00:17,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.7528, loss: 0.0657, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:38<00:18,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.7527, loss: 0.0658, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:38<00:17,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7536, loss: 0.0658, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:38<00:15,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.7543, loss: 0.0657, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:39<00:13,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.7538, loss: 0.0658, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:39<00:13,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7536, loss: 0.0658, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:39<00:13,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7529, loss: 0.0658, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:39<00:13,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.7529, loss: 0.0658, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:40<00:16,  2.82it/s]\u001b[A\n",
            "pearson(r): 0.7525, loss: 0.0658, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:40<00:14,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.7526, loss: 0.0659, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:40<00:13,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.7518, loss: 0.0659, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:41<00:14,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.7524, loss: 0.0658, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:41<00:13,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.7521, loss: 0.0659, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:41<00:12,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.7512, loss: 0.0660, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:42<00:11,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.7514, loss: 0.0659, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:42<00:10,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7517, loss: 0.0659, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:42<00:10,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7518, loss: 0.0657, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:43<00:11,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.7514, loss: 0.0658, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:43<00:11,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.7518, loss: 0.0657, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:43<00:10,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.7516, loss: 0.0657, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:44<00:11,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7513, loss: 0.0658, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:44<00:10,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.7511, loss: 0.0658, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:44<00:08,  3.62it/s]\u001b[A\n",
            "pearson(r): 0.7511, loss: 0.0658, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:44<00:08,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7510, loss: 0.0659, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:45<00:08,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.7517, loss: 0.0658, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:45<00:07,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.7513, loss: 0.0658, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:45<00:07,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.7515, loss: 0.0658, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:45<00:07,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.7509, loss: 0.0657, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:46<00:09,  2.75it/s]\u001b[A\n",
            "pearson(r): 0.7500, loss: 0.0660, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:46<00:08,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7507, loss: 0.0659, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:46<00:07,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.7508, loss: 0.0659, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:47<00:06,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7508, loss: 0.0659, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:47<00:06,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7509, loss: 0.0660, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:47<00:05,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7517, loss: 0.0659, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:48<00:06,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.7510, loss: 0.0660, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:48<00:07,  2.61it/s]\u001b[A\n",
            "pearson(r): 0.7515, loss: 0.0660, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:48<00:06,  2.94it/s]\u001b[A\n",
            "pearson(r): 0.7508, loss: 0.0662, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:49<00:06,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.7506, loss: 0.0664, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:49<00:05,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.7505, loss: 0.0666, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:50<00:05,  2.63it/s]\u001b[A\n",
            "pearson(r): 0.7500, loss: 0.0666, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:50<00:04,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.7500, loss: 0.0666, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:50<00:04,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.7490, loss: 0.0667, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:50<00:03,  3.22it/s]\u001b[A\n",
            "pearson(r): 0.7490, loss: 0.0668, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:51<00:02,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7487, loss: 0.0668, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:51<00:02,  3.80it/s]\u001b[A\n",
            "pearson(r): 0.7490, loss: 0.0667, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:51<00:02,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.7493, loss: 0.0667, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:51<00:02,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.7492, loss: 0.0666, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:52<00:01,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.7488, loss: 0.0668, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:52<00:01,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7486, loss: 0.0668, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:52<00:01,  2.90it/s]\u001b[A\n",
            "pearson(r): 0.7479, loss: 0.0670, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:53<00:01,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.7478, loss: 0.0670, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:53<00:00,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.7482, loss: 0.0670, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.7478, loss: 0.0669, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:54<00:00,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7480, loss: 0.0669, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:54<00:00,  3.31it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.6221, loss: 0.0442, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:01<05:01,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.6404, loss: 0.0459, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:03<04:41,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6175, loss: 0.0449, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:04<04:21,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6147, loss: 0.0492, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:07<05:43,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.6606, loss: 0.0473, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:09<05:40,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.6770, loss: 0.0482, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:10<05:13,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.6853, loss: 0.0477, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:11<04:33,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6863, loss: 0.0481, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:13<04:19,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6915, loss: 0.0479, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:14<04:29,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.6981, loss: 0.0475, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:16<04:40,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.6952, loss: 0.0473, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:18<04:52,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.6917, loss: 0.0474, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:20<05:13,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.6958, loss: 0.0466, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:22<05:11,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.7111, loss: 0.0458, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:24<04:53,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7162, loss: 0.0449, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:25<04:39,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7196, loss: 0.0446, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:27<04:33,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7143, loss: 0.0450, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:30<05:37,  2.07s/it]\u001b[A\n",
            "pearson(r): 0.7204, loss: 0.0443, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:31<05:07,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.7237, loss: 0.0445, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:33<04:41,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.7236, loss: 0.0444, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:34<04:27,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7237, loss: 0.0436, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:36<04:31,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.7188, loss: 0.0438, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:37<04:15,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7181, loss: 0.0436, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:39<04:07,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7176, loss: 0.0435, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:40<04:05,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7158, loss: 0.0432, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:42<04:03,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7191, loss: 0.0425, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:43<03:56,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7218, loss: 0.0422, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:45<03:37,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7221, loss: 0.0419, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:46<03:47,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7244, loss: 0.0417, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:48<03:52,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7184, loss: 0.0427, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:49<03:46,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7148, loss: 0.0435, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:51<03:49,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7158, loss: 0.0438, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:53<03:47,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7187, loss: 0.0435, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:55<04:13,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7176, loss: 0.0431, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:56<04:05,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7162, loss: 0.0429, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:58<04:15,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7157, loss: 0.0433, reg_loss: 0.0000 ||:  20%|██        | 36/180 [01:01<05:08,  2.14s/it]\u001b[A\n",
            "pearson(r): 0.7155, loss: 0.0431, reg_loss: 0.0000 ||:  21%|██        | 37/180 [01:03<04:39,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.7120, loss: 0.0436, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:06<05:24,  2.28s/it]\u001b[A\n",
            "pearson(r): 0.7137, loss: 0.0435, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:07<04:46,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.7139, loss: 0.0437, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:09<04:25,  1.89s/it]\u001b[A\n",
            "pearson(r): 0.7095, loss: 0.0442, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:11<04:32,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.7095, loss: 0.0440, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:12<04:08,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.7098, loss: 0.0439, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:14<03:54,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.7069, loss: 0.0444, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:15<03:34,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7031, loss: 0.0450, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:18<04:32,  2.01s/it]\u001b[A\n",
            "pearson(r): 0.7001, loss: 0.0453, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:20<04:21,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.6971, loss: 0.0453, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:21<03:46,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.6963, loss: 0.0452, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:22<03:30,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.6960, loss: 0.0451, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:24<03:20,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.6972, loss: 0.0450, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:25<03:18,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.6980, loss: 0.0449, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:27<03:14,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.6982, loss: 0.0447, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:28<03:09,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.6988, loss: 0.0444, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:29<02:55,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.6983, loss: 0.0447, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:31<02:46,  1.32s/it]\u001b[A\n",
            "pearson(r): 0.6999, loss: 0.0445, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:32<02:50,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.7005, loss: 0.0447, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:33<02:51,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.7023, loss: 0.0446, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:35<02:51,  1.39s/it]\u001b[A\n",
            "pearson(r): 0.7002, loss: 0.0446, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:38<03:49,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7001, loss: 0.0449, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:39<03:30,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7020, loss: 0.0451, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:41<03:19,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7043, loss: 0.0446, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:42<03:16,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7054, loss: 0.0443, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:44<03:09,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7073, loss: 0.0441, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:46<03:12,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7091, loss: 0.0440, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:47<03:04,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7102, loss: 0.0439, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:49<02:57,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7123, loss: 0.0437, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:50<02:56,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7138, loss: 0.0436, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:51<02:49,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7109, loss: 0.0440, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:53<02:59,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7118, loss: 0.0439, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:55<02:55,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7110, loss: 0.0438, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:57<02:57,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7112, loss: 0.0437, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:58<02:59,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7124, loss: 0.0435, reg_loss: 0.0000 ||:  40%|████      | 72/180 [02:00<02:52,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7123, loss: 0.0434, reg_loss: 0.0000 ||:  41%|████      | 73/180 [02:01<02:46,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7132, loss: 0.0432, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:03<02:37,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7123, loss: 0.0434, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:04<02:31,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7122, loss: 0.0434, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:06<03:04,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7114, loss: 0.0433, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:08<02:53,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7091, loss: 0.0434, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:09<02:41,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7082, loss: 0.0436, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:11<02:42,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7078, loss: 0.0435, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:12<02:31,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7093, loss: 0.0433, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:14<02:51,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7108, loss: 0.0431, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:16<02:37,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7090, loss: 0.0432, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:17<02:27,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7084, loss: 0.0433, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:19<02:30,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7086, loss: 0.0433, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:21<02:35,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7093, loss: 0.0433, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:22<02:28,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7109, loss: 0.0431, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:24<02:37,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7114, loss: 0.0430, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:25<02:28,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7127, loss: 0.0427, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:27<02:24,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7129, loss: 0.0425, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:28<02:22,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7141, loss: 0.0425, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:30<02:16,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7156, loss: 0.0424, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:31<02:12,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7151, loss: 0.0423, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:33<02:08,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7141, loss: 0.0423, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:34<02:04,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7146, loss: 0.0423, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:36<02:08,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7140, loss: 0.0424, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:38<02:24,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7123, loss: 0.0427, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:40<02:18,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7125, loss: 0.0427, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:41<02:14,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7115, loss: 0.0428, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:43<02:14,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7111, loss: 0.0429, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:46<02:42,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.7111, loss: 0.0429, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:47<02:19,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7114, loss: 0.0428, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:48<02:10,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7114, loss: 0.0428, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:50<02:19,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.7110, loss: 0.0428, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:52<02:10,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7104, loss: 0.0428, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:55<02:37,  2.10s/it]\u001b[A\n",
            "pearson(r): 0.7090, loss: 0.0430, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:56<02:14,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.7094, loss: 0.0431, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [02:58<02:17,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7089, loss: 0.0431, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [03:00<02:08,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.7080, loss: 0.0431, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [03:02<02:11,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.7087, loss: 0.0431, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:03<02:04,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.7089, loss: 0.0432, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:05<01:54,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7092, loss: 0.0432, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:06<01:47,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7091, loss: 0.0432, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:08<01:44,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7085, loss: 0.0433, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:11<02:11,  1.99s/it]\u001b[A\n",
            "pearson(r): 0.7084, loss: 0.0433, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:12<02:02,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7077, loss: 0.0433, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:14<01:51,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7063, loss: 0.0435, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:15<01:40,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7064, loss: 0.0435, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:17<01:42,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7078, loss: 0.0434, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:18<01:37,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7066, loss: 0.0435, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:19<01:29,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7065, loss: 0.0434, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:22<01:55,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.7052, loss: 0.0434, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:24<01:46,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.7059, loss: 0.0434, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:26<01:41,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7069, loss: 0.0432, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:27<01:33,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7077, loss: 0.0431, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:28<01:25,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7077, loss: 0.0430, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:30<01:18,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7074, loss: 0.0430, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:31<01:16,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7071, loss: 0.0431, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:32<01:15,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7068, loss: 0.0430, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:34<01:15,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7056, loss: 0.0432, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:35<01:11,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.7067, loss: 0.0430, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:37<01:07,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.7076, loss: 0.0428, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:39<01:17,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7079, loss: 0.0428, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:40<01:11,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7089, loss: 0.0427, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:42<01:11,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7089, loss: 0.0427, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:43<01:12,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7088, loss: 0.0426, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:45<01:12,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7085, loss: 0.0426, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:47<01:12,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7090, loss: 0.0425, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:49<01:11,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7096, loss: 0.0424, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:52<01:26,  2.11s/it]\u001b[A\n",
            "pearson(r): 0.7098, loss: 0.0425, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:53<01:16,  1.91s/it]\u001b[A\n",
            "pearson(r): 0.7100, loss: 0.0424, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:55<01:10,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.7103, loss: 0.0425, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:56<01:05,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.7109, loss: 0.0424, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [03:57<00:54,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7104, loss: 0.0424, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [03:59<00:53,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7104, loss: 0.0424, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [04:00<00:49,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7089, loss: 0.0425, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [04:02<00:55,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.7075, loss: 0.0426, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:03<00:51,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7069, loss: 0.0427, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:05<00:53,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7077, loss: 0.0427, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:06<00:46,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7069, loss: 0.0426, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:08<00:46,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7067, loss: 0.0427, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:10<00:43,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7062, loss: 0.0426, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:11<00:41,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7049, loss: 0.0429, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:14<00:52,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.7042, loss: 0.0429, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:16<00:47,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.7037, loss: 0.0431, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:17<00:43,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.7036, loss: 0.0431, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:19<00:40,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7030, loss: 0.0433, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:20<00:35,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7031, loss: 0.0432, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:21<00:33,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7033, loss: 0.0432, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:24<00:36,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7035, loss: 0.0432, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:26<00:37,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7036, loss: 0.0433, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:28<00:38,  2.00s/it]\u001b[A\n",
            "pearson(r): 0.7041, loss: 0.0433, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:30<00:37,  2.08s/it]\u001b[A\n",
            "pearson(r): 0.7044, loss: 0.0433, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:32<00:32,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.7054, loss: 0.0432, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:33<00:28,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7056, loss: 0.0431, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:35<00:26,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7064, loss: 0.0431, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:36<00:22,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7062, loss: 0.0432, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:38<00:20,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7068, loss: 0.0431, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:39<00:19,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7072, loss: 0.0430, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:41<00:16,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7077, loss: 0.0429, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:42<00:14,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7083, loss: 0.0429, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:44<00:12,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7076, loss: 0.0429, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:45<00:11,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7081, loss: 0.0429, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:46<00:09,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.7071, loss: 0.0430, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:48<00:08,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.7068, loss: 0.0430, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:49<00:06,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.7065, loss: 0.0430, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:51<00:05,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7064, loss: 0.0430, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:54<00:05,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.7069, loss: 0.0429, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:55<00:03,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.7074, loss: 0.0429, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:57<00:01,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.7075, loss: 0.0429, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [04:59<00:00,  1.66s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.5298, loss: 0.0402, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:54,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.8414, loss: 0.0423, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:52,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.8090, loss: 0.0455, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:01<00:59,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7921, loss: 0.0502, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<00:55,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.8009, loss: 0.0488, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<00:52,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7959, loss: 0.0491, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:01<00:49,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7994, loss: 0.0487, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:02<00:47,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7911, loss: 0.0503, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:46,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.7960, loss: 0.0505, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<00:51,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7973, loss: 0.0520, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:03<00:54,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7891, loss: 0.0534, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:03<00:50,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7913, loss: 0.0535, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<00:48,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7966, loss: 0.0539, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:03<00:49,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.7954, loss: 0.0536, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:04<00:48,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7893, loss: 0.0534, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:47,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.7940, loss: 0.0533, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:59,  2.77it/s]\u001b[A\n",
            "pearson(r): 0.7913, loss: 0.0530, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:05<00:54,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7829, loss: 0.0537, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:05<00:51,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7806, loss: 0.0537, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:51,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7821, loss: 0.0535, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:06<00:48,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7863, loss: 0.0529, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:06<00:47,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7836, loss: 0.0527, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:46,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.7856, loss: 0.0520, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:45,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7844, loss: 0.0517, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:07<00:41,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.7826, loss: 0.0522, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:07<00:46,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0521, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:44,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.7830, loss: 0.0519, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:08<00:42,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.7802, loss: 0.0529, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:08<00:44,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7794, loss: 0.0534, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:44,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.7842, loss: 0.0527, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:08<00:44,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.7831, loss: 0.0534, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:09<00:43,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7839, loss: 0.0534, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:09<00:44,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0532, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:09<00:43,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.7801, loss: 0.0536, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:10<00:39,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7802, loss: 0.0536, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:10<00:50,  2.90it/s]\u001b[A\n",
            "pearson(r): 0.7792, loss: 0.0535, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:10<00:52,  2.76it/s]\u001b[A\n",
            "pearson(r): 0.7781, loss: 0.0542, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:11<00:44,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.7758, loss: 0.0541, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:11<00:48,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.7741, loss: 0.0547, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:11<00:43,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.7747, loss: 0.0544, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:12<00:41,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7756, loss: 0.0541, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:12<00:36,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.7754, loss: 0.0538, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:12<00:36,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.7767, loss: 0.0537, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:12<00:35,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.7779, loss: 0.0531, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:13<00:36,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.7767, loss: 0.0535, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:13<00:38,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7754, loss: 0.0540, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:13<00:40,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7757, loss: 0.0541, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:13<00:36,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.7763, loss: 0.0540, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:14<00:38,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.7770, loss: 0.0542, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:14<00:36,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7758, loss: 0.0543, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:14<00:38,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.7757, loss: 0.0540, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:15<00:36,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7763, loss: 0.0537, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:15<00:35,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7761, loss: 0.0535, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:15<00:35,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7765, loss: 0.0537, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:15<00:36,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.7767, loss: 0.0534, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:16<00:39,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.7761, loss: 0.0536, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:16<00:36,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7736, loss: 0.0537, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:17<00:40,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7727, loss: 0.0537, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:17<00:47,  2.57it/s]\u001b[A\n",
            "pearson(r): 0.7739, loss: 0.0535, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:17<00:41,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.7732, loss: 0.0536, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:18<00:38,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7724, loss: 0.0535, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:18<00:36,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.7741, loss: 0.0533, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:18<00:32,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.7753, loss: 0.0530, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:18<00:32,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.7762, loss: 0.0528, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:19<00:34,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.7775, loss: 0.0527, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:19<00:33,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.7774, loss: 0.0530, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:19<00:32,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7761, loss: 0.0533, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:19<00:32,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7762, loss: 0.0535, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:20<00:31,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7760, loss: 0.0536, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:20<00:30,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.7741, loss: 0.0540, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:20<00:29,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7738, loss: 0.0540, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:21<00:30,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7745, loss: 0.0540, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:21<00:29,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.7751, loss: 0.0539, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:21<00:28,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.7763, loss: 0.0539, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:21<00:27,  3.85it/s]\u001b[A\n",
            "pearson(r): 0.7759, loss: 0.0537, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:22<00:28,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.7759, loss: 0.0537, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:22<00:27,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.7763, loss: 0.0537, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:22<00:29,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.7768, loss: 0.0537, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:22<00:28,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7754, loss: 0.0542, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:23<00:26,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.7761, loss: 0.0540, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:23<00:26,  3.83it/s]\u001b[A\n",
            "pearson(r): 0.7738, loss: 0.0545, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:23<00:33,  2.93it/s]\u001b[A\n",
            "pearson(r): 0.7737, loss: 0.0544, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:24<00:31,  3.16it/s]\u001b[A\n",
            "pearson(r): 0.7721, loss: 0.0546, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:24<00:32,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.7731, loss: 0.0544, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:24<00:31,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7733, loss: 0.0544, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:25<00:30,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.7730, loss: 0.0545, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:25<00:28,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7734, loss: 0.0543, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:25<00:29,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.7718, loss: 0.0545, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:26<00:28,  3.20it/s]\u001b[A\n",
            "pearson(r): 0.7711, loss: 0.0546, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:26<00:28,  3.21it/s]\u001b[A\n",
            "pearson(r): 0.7702, loss: 0.0549, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:26<00:24,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7696, loss: 0.0551, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:26<00:23,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.7701, loss: 0.0549, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:27<00:23,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7704, loss: 0.0549, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:27<00:23,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.7697, loss: 0.0549, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:27<00:24,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7705, loss: 0.0548, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:28<00:23,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.7709, loss: 0.0547, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:28<00:21,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.7697, loss: 0.0549, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:28<00:21,  3.83it/s]\u001b[A\n",
            "pearson(r): 0.7686, loss: 0.0549, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:28<00:25,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.7690, loss: 0.0550, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:29<00:24,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7692, loss: 0.0550, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:29<00:22,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.7700, loss: 0.0549, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:29<00:25,  3.11it/s]\u001b[A\n",
            "pearson(r): 0.7693, loss: 0.0550, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:30<00:27,  2.81it/s]\u001b[A\n",
            "pearson(r): 0.7689, loss: 0.0550, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:30<00:26,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.7673, loss: 0.0552, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:31<00:30,  2.50it/s]\u001b[A\n",
            "pearson(r): 0.7670, loss: 0.0553, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:31<00:27,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.7669, loss: 0.0553, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:31<00:24,  2.97it/s]\u001b[A\n",
            "pearson(r): 0.7674, loss: 0.0553, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:31<00:23,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.7670, loss: 0.0553, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:32<00:25,  2.86it/s]\u001b[A\n",
            "pearson(r): 0.7666, loss: 0.0552, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:32<00:23,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7671, loss: 0.0552, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:32<00:21,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.7678, loss: 0.0552, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:33<00:19,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7676, loss: 0.0553, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:33<00:19,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.7678, loss: 0.0553, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:33<00:18,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7668, loss: 0.0554, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:34<00:20,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.7671, loss: 0.0553, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:34<00:19,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.7676, loss: 0.0552, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:34<00:18,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7678, loss: 0.0552, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:34<00:17,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.7681, loss: 0.0552, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:35<00:19,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.7686, loss: 0.0551, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:35<00:21,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.7679, loss: 0.0553, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:36<00:21,  2.73it/s]\u001b[A\n",
            "pearson(r): 0.7690, loss: 0.0552, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:36<00:20,  2.93it/s]\u001b[A\n",
            "pearson(r): 0.7701, loss: 0.0551, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:36<00:18,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7706, loss: 0.0552, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:36<00:17,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7703, loss: 0.0551, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:37<00:16,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7704, loss: 0.0552, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:37<00:15,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.7696, loss: 0.0554, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:37<00:14,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.7682, loss: 0.0556, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:38<00:18,  2.90it/s]\u001b[A\n",
            "pearson(r): 0.7684, loss: 0.0556, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:38<00:18,  2.88it/s]\u001b[A\n",
            "pearson(r): 0.7689, loss: 0.0555, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:38<00:16,  3.13it/s]\u001b[A\n",
            "pearson(r): 0.7683, loss: 0.0556, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:39<00:14,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.7677, loss: 0.0557, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:39<00:18,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.7681, loss: 0.0557, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:40<00:20,  2.37it/s]\u001b[A\n",
            "pearson(r): 0.7682, loss: 0.0557, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:40<00:17,  2.66it/s]\u001b[A\n",
            "pearson(r): 0.7689, loss: 0.0557, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:40<00:15,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.7686, loss: 0.0556, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:41<00:15,  2.81it/s]\u001b[A\n",
            "pearson(r): 0.7689, loss: 0.0555, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:41<00:15,  2.91it/s]\u001b[A\n",
            "pearson(r): 0.7685, loss: 0.0555, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:41<00:14,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.7681, loss: 0.0557, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:42<00:16,  2.56it/s]\u001b[A\n",
            "pearson(r): 0.7684, loss: 0.0556, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:42<00:14,  2.87it/s]\u001b[A\n",
            "pearson(r): 0.7681, loss: 0.0556, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:42<00:12,  3.12it/s]\u001b[A\n",
            "pearson(r): 0.7671, loss: 0.0558, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:42<00:11,  3.29it/s]\u001b[A\n",
            "pearson(r): 0.7679, loss: 0.0556, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:43<00:10,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.7670, loss: 0.0558, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:43<00:11,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.7670, loss: 0.0558, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:43<00:10,  3.42it/s]\u001b[A\n",
            "pearson(r): 0.7680, loss: 0.0558, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:44<00:09,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.7687, loss: 0.0558, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:44<00:09,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7693, loss: 0.0557, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:44<00:08,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7692, loss: 0.0557, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:44<00:09,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7692, loss: 0.0556, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:45<00:09,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7689, loss: 0.0558, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:45<00:08,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7687, loss: 0.0557, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:45<00:07,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7683, loss: 0.0557, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:46<00:07,  3.69it/s]\u001b[A\n",
            "pearson(r): 0.7689, loss: 0.0556, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:46<00:07,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.7680, loss: 0.0558, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:46<00:09,  2.89it/s]\u001b[A\n",
            "pearson(r): 0.7683, loss: 0.0558, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:47<00:10,  2.49it/s]\u001b[A\n",
            "pearson(r): 0.7687, loss: 0.0557, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:47<00:08,  2.72it/s]\u001b[A\n",
            "pearson(r): 0.7691, loss: 0.0556, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:48<00:08,  2.70it/s]\u001b[A\n",
            "pearson(r): 0.7693, loss: 0.0556, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:48<00:07,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7699, loss: 0.0556, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:48<00:06,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.7696, loss: 0.0557, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:48<00:05,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7700, loss: 0.0556, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:49<00:05,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7702, loss: 0.0557, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:49<00:04,  3.78it/s]\u001b[A\n",
            "pearson(r): 0.7703, loss: 0.0558, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:49<00:04,  3.80it/s]\u001b[A\n",
            "pearson(r): 0.7706, loss: 0.0558, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:49<00:04,  3.87it/s]\u001b[A\n",
            "pearson(r): 0.7704, loss: 0.0557, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:49<00:03,  4.15it/s]\u001b[A\n",
            "pearson(r): 0.7704, loss: 0.0558, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:50<00:03,  4.22it/s]\u001b[A\n",
            "pearson(r): 0.7702, loss: 0.0557, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:50<00:03,  4.18it/s]\u001b[A\n",
            "pearson(r): 0.7705, loss: 0.0557, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:50<00:03,  3.97it/s]\u001b[A\n",
            "pearson(r): 0.7708, loss: 0.0557, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:51<00:03,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7712, loss: 0.0557, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:51<00:02,  3.85it/s]\u001b[A\n",
            "pearson(r): 0.7712, loss: 0.0558, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:51<00:02,  3.66it/s]\u001b[A\n",
            "pearson(r): 0.7715, loss: 0.0558, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:51<00:02,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.7715, loss: 0.0558, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:52<00:01,  3.80it/s]\u001b[A\n",
            "pearson(r): 0.7711, loss: 0.0558, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:52<00:01,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.7716, loss: 0.0558, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:52<00:01,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.7713, loss: 0.0558, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:53<00:01,  3.49it/s]\u001b[A\n",
            "pearson(r): 0.7714, loss: 0.0557, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:53<00:00,  3.97it/s]\u001b[A\n",
            "pearson(r): 0.7711, loss: 0.0557, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.90it/s]\u001b[A\n",
            "pearson(r): 0.7710, loss: 0.0557, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:53<00:00,  3.89it/s]\u001b[A\n",
            "pearson(r): 0.7707, loss: 0.0558, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:53<00:00,  3.34it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.7855, loss: 0.0368, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<02:47,  1.07it/s]\u001b[A\n",
            "pearson(r): 0.6871, loss: 0.0463, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:02<03:12,  1.08s/it]\u001b[A\n",
            "pearson(r): 0.7262, loss: 0.0411, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:04<03:55,  1.33s/it]\u001b[A\n",
            "pearson(r): 0.7340, loss: 0.0389, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:05<04:10,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7784, loss: 0.0360, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:07<04:01,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.7694, loss: 0.0358, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:08<04:00,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.7659, loss: 0.0380, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:09<03:52,  1.34s/it]\u001b[A\n",
            "pearson(r): 0.7621, loss: 0.0391, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:11<03:43,  1.30s/it]\u001b[A\n",
            "pearson(r): 0.7730, loss: 0.0390, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:12<03:52,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.7712, loss: 0.0384, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:13<03:45,  1.33s/it]\u001b[A\n",
            "pearson(r): 0.7765, loss: 0.0371, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:15<04:02,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7822, loss: 0.0369, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:16<03:47,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.7831, loss: 0.0369, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:17<03:33,  1.28s/it]\u001b[A\n",
            "pearson(r): 0.7703, loss: 0.0373, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:19<03:46,  1.37s/it]\u001b[A\n",
            "pearson(r): 0.7713, loss: 0.0367, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:20<03:51,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.7726, loss: 0.0357, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:22<03:55,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.7723, loss: 0.0352, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:23<03:57,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7754, loss: 0.0358, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:25<04:00,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7746, loss: 0.0356, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:27<04:10,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7773, loss: 0.0353, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:28<04:07,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7775, loss: 0.0351, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:30<04:03,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7750, loss: 0.0356, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:31<03:48,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7730, loss: 0.0361, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:33<04:03,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7727, loss: 0.0357, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:34<04:04,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7691, loss: 0.0361, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:36<04:34,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7680, loss: 0.0361, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:39<04:45,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.7650, loss: 0.0365, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:42<05:37,  2.21s/it]\u001b[A\n",
            "pearson(r): 0.7637, loss: 0.0363, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:43<05:04,  2.00s/it]\u001b[A\n",
            "pearson(r): 0.7641, loss: 0.0364, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:45<04:44,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7670, loss: 0.0362, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:46<04:14,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7662, loss: 0.0365, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:47<03:55,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7651, loss: 0.0367, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:49<03:42,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7671, loss: 0.0367, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:50<03:26,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.7682, loss: 0.0362, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:51<03:25,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.7670, loss: 0.0363, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:53<03:35,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7675, loss: 0.0361, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:55<03:41,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7645, loss: 0.0364, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:56<03:36,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7654, loss: 0.0365, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:57<03:35,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7655, loss: 0.0362, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:59<03:34,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7599, loss: 0.0367, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:01<03:36,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7580, loss: 0.0369, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:02<03:34,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7591, loss: 0.0369, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:03<03:23,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7572, loss: 0.0369, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:05<03:12,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.7583, loss: 0.0368, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:07<03:37,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7536, loss: 0.0374, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:09<03:58,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7533, loss: 0.0377, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:10<03:47,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7496, loss: 0.0378, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:12<03:48,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7483, loss: 0.0379, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:14<03:30,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7477, loss: 0.0381, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:15<03:19,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7468, loss: 0.0383, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:16<03:11,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7454, loss: 0.0385, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:18<03:06,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7433, loss: 0.0389, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:19<03:04,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7424, loss: 0.0389, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:21<03:04,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7413, loss: 0.0390, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:22<03:03,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7402, loss: 0.0393, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:23<02:52,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.7398, loss: 0.0393, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:24<02:29,  1.21s/it]\u001b[A\n",
            "pearson(r): 0.7379, loss: 0.0396, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:25<02:35,  1.26s/it]\u001b[A\n",
            "pearson(r): 0.7347, loss: 0.0399, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:26<02:27,  1.21s/it]\u001b[A\n",
            "pearson(r): 0.7344, loss: 0.0400, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:28<02:30,  1.25s/it]\u001b[A\n",
            "pearson(r): 0.7355, loss: 0.0400, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:29<02:32,  1.27s/it]\u001b[A\n",
            "pearson(r): 0.7363, loss: 0.0399, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:31<02:34,  1.30s/it]\u001b[A\n",
            "pearson(r): 0.7373, loss: 0.0398, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:32<02:44,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.7375, loss: 0.0400, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:33<02:36,  1.34s/it]\u001b[A\n",
            "pearson(r): 0.7353, loss: 0.0404, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:36<03:11,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7361, loss: 0.0405, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:37<03:01,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7358, loss: 0.0405, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:39<03:00,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7337, loss: 0.0407, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:40<03:03,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7343, loss: 0.0405, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:42<02:57,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7337, loss: 0.0404, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:43<02:51,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7326, loss: 0.0404, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:46<03:38,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.7329, loss: 0.0404, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:48<03:23,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.7319, loss: 0.0405, reg_loss: 0.0000 ||:  40%|████      | 72/180 [01:50<03:17,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.7319, loss: 0.0405, reg_loss: 0.0000 ||:  41%|████      | 73/180 [01:52<03:26,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.7329, loss: 0.0404, reg_loss: 0.0000 ||:  41%|████      | 74/180 [01:53<03:07,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7339, loss: 0.0403, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [01:55<02:54,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7316, loss: 0.0405, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [01:56<02:44,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7313, loss: 0.0406, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [01:58<02:55,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7307, loss: 0.0407, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [01:59<02:44,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7302, loss: 0.0408, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:02<03:01,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.7295, loss: 0.0410, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:03<02:52,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7308, loss: 0.0409, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:05<02:55,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7312, loss: 0.0408, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:06<02:34,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7310, loss: 0.0409, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:08<02:25,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7324, loss: 0.0407, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:09<02:28,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7319, loss: 0.0409, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:11<02:32,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7327, loss: 0.0407, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:13<02:30,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7315, loss: 0.0408, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:14<02:30,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7307, loss: 0.0408, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:16<02:19,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7296, loss: 0.0407, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:17<02:17,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7287, loss: 0.0408, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:18<02:10,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7276, loss: 0.0409, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:20<02:20,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7279, loss: 0.0409, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:22<02:15,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7265, loss: 0.0410, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:24<02:27,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7266, loss: 0.0409, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:25<02:24,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7264, loss: 0.0410, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:27<02:14,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7271, loss: 0.0409, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:29<02:23,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.7247, loss: 0.0412, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:32<02:55,  2.11s/it]\u001b[A\n",
            "pearson(r): 0.7255, loss: 0.0412, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:33<02:40,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.7251, loss: 0.0412, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:35<02:29,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.7263, loss: 0.0412, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:36<02:19,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7276, loss: 0.0411, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:38<02:08,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.7277, loss: 0.0411, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:39<01:58,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7274, loss: 0.0411, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:41<01:55,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7242, loss: 0.0414, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:44<02:28,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.7237, loss: 0.0414, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:45<02:15,  1.80s/it]\u001b[A\n",
            "pearson(r): 0.7242, loss: 0.0414, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:47<02:19,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7248, loss: 0.0413, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [02:48<02:05,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7256, loss: 0.0412, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [02:50<02:04,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7253, loss: 0.0412, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [02:52<01:57,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7246, loss: 0.0412, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [02:53<01:44,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7245, loss: 0.0413, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [02:54<01:40,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.7257, loss: 0.0412, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [02:56<01:53,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7255, loss: 0.0412, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [02:58<01:45,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7258, loss: 0.0411, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [02:59<01:40,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7232, loss: 0.0413, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:02<02:07,  1.97s/it]\u001b[A\n",
            "pearson(r): 0.7234, loss: 0.0412, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:03<01:56,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.7230, loss: 0.0413, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:06<02:16,  2.17s/it]\u001b[A\n",
            "pearson(r): 0.7226, loss: 0.0413, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:08<01:54,  1.85s/it]\u001b[A\n",
            "pearson(r): 0.7230, loss: 0.0412, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:09<01:44,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7234, loss: 0.0411, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:11<01:41,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7234, loss: 0.0411, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:12<01:37,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7232, loss: 0.0411, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:14<01:36,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7246, loss: 0.0409, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:15<01:32,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7243, loss: 0.0409, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:17<01:32,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7230, loss: 0.0410, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:20<01:53,  2.06s/it]\u001b[A\n",
            "pearson(r): 0.7230, loss: 0.0411, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:21<01:38,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.7233, loss: 0.0411, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:23<01:31,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7232, loss: 0.0411, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:25<01:37,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.7232, loss: 0.0410, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:27<01:28,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7225, loss: 0.0412, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:30<01:46,  2.13s/it]\u001b[A\n",
            "pearson(r): 0.7222, loss: 0.0413, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:31<01:34,  1.93s/it]\u001b[A\n",
            "pearson(r): 0.7216, loss: 0.0413, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:32<01:24,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7214, loss: 0.0413, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:34<01:15,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7229, loss: 0.0412, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:35<01:11,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7221, loss: 0.0413, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:37<01:09,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7219, loss: 0.0413, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:38<01:01,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.7214, loss: 0.0413, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:40<01:05,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7214, loss: 0.0413, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:43<01:23,  1.98s/it]\u001b[A\n",
            "pearson(r): 0.7218, loss: 0.0414, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:45<01:22,  2.00s/it]\u001b[A\n",
            "pearson(r): 0.7217, loss: 0.0412, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:46<01:13,  1.84s/it]\u001b[A\n",
            "pearson(r): 0.7215, loss: 0.0413, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:48<01:15,  1.94s/it]\u001b[A\n",
            "pearson(r): 0.7224, loss: 0.0411, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:50<01:08,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.7217, loss: 0.0412, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [03:51<01:02,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7195, loss: 0.0414, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [03:54<01:10,  1.97s/it]\u001b[A\n",
            "pearson(r): 0.7199, loss: 0.0413, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [03:55<01:04,  1.85s/it]\u001b[A\n",
            "pearson(r): 0.7195, loss: 0.0413, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [03:57<01:00,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7185, loss: 0.0413, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [03:58<00:54,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7182, loss: 0.0413, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:00<00:49,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7186, loss: 0.0413, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:01<00:47,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7188, loss: 0.0412, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:03<00:46,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7186, loss: 0.0413, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:04<00:44,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7190, loss: 0.0412, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:06<00:47,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7186, loss: 0.0412, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:08<00:46,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7181, loss: 0.0413, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:10<00:47,  1.83s/it]\u001b[A\n",
            "pearson(r): 0.7185, loss: 0.0412, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:12<00:44,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7186, loss: 0.0412, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:13<00:41,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.7196, loss: 0.0411, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:15<00:36,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7201, loss: 0.0411, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:16<00:33,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7208, loss: 0.0411, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:18<00:31,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7209, loss: 0.0411, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:19<00:29,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7207, loss: 0.0411, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:22<00:35,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.7200, loss: 0.0412, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:23<00:31,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7203, loss: 0.0411, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:25<00:30,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.7205, loss: 0.0411, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:26<00:26,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7209, loss: 0.0411, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:28<00:23,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7221, loss: 0.0410, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:29<00:21,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7218, loss: 0.0410, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:31<00:22,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7220, loss: 0.0410, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:33<00:19,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7216, loss: 0.0410, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:34<00:17,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7220, loss: 0.0410, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:36<00:15,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7221, loss: 0.0409, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:39<00:18,  2.00s/it]\u001b[A\n",
            "pearson(r): 0.7222, loss: 0.0410, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:41<00:16,  2.06s/it]\u001b[A\n",
            "pearson(r): 0.7218, loss: 0.0411, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:43<00:13,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.7219, loss: 0.0411, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:44<00:10,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7218, loss: 0.0411, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:45<00:08,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7210, loss: 0.0412, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:48<00:08,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.7198, loss: 0.0414, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:51<00:06,  2.33s/it]\u001b[A\n",
            "pearson(r): 0.7201, loss: 0.0414, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:53<00:04,  2.05s/it]\u001b[A\n",
            "pearson(r): 0.7198, loss: 0.0414, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:54<00:01,  1.96s/it]\u001b[A\n",
            "pearson(r): 0.7207, loss: 0.0413, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [04:56<00:00,  1.65s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.7552, loss: 0.0379, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:56,  3.19it/s]\u001b[A\n",
            "pearson(r): 0.8338, loss: 0.0378, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:55,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.7968, loss: 0.0446, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:00<00:50,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.8126, loss: 0.0441, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<00:52,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7929, loss: 0.0495, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<00:50,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7913, loss: 0.0511, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:01<00:50,  3.48it/s]\u001b[A\n",
            "pearson(r): 0.7881, loss: 0.0507, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:02<00:54,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.7977, loss: 0.0509, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:02<00:51,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.7954, loss: 0.0526, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:02<00:48,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.8108, loss: 0.0499, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:02<00:47,  3.61it/s]\u001b[A\n",
            "pearson(r): 0.8087, loss: 0.0498, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:03<00:46,  3.64it/s]\u001b[A\n",
            "pearson(r): 0.8146, loss: 0.0495, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:03<00:45,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.8212, loss: 0.0487, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:03<00:44,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.8193, loss: 0.0483, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:03<00:45,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.8143, loss: 0.0494, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:04<00:43,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.8107, loss: 0.0501, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:04<00:48,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.8090, loss: 0.0498, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:04<00:45,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.8087, loss: 0.0501, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:05<00:41,  3.88it/s]\u001b[A\n",
            "pearson(r): 0.8093, loss: 0.0501, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:05<00:41,  3.92it/s]\u001b[A\n",
            "pearson(r): 0.8100, loss: 0.0498, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:05<00:41,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.8047, loss: 0.0511, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:05<00:41,  3.86it/s]\u001b[A\n",
            "pearson(r): 0.7989, loss: 0.0519, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:06<00:42,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.7992, loss: 0.0518, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:06<00:42,  3.71it/s]\u001b[A\n",
            "pearson(r): 0.8002, loss: 0.0513, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:06<00:41,  3.72it/s]\u001b[A\n",
            "pearson(r): 0.8028, loss: 0.0510, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:07<00:48,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.8013, loss: 0.0509, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:07<00:46,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.8028, loss: 0.0504, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:07<00:42,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.7998, loss: 0.0509, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:07<00:43,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7981, loss: 0.0508, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:08<00:43,  3.51it/s]\u001b[A\n",
            "pearson(r): 0.7975, loss: 0.0507, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:08<00:41,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7993, loss: 0.0502, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:08<00:39,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7953, loss: 0.0504, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:09<00:52,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.7965, loss: 0.0502, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:09<00:47,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7967, loss: 0.0499, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:09<00:43,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7916, loss: 0.0509, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:09<00:42,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7958, loss: 0.0503, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:10<00:40,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7921, loss: 0.0513, reg_loss: 0.0000 ||:  21%|██        | 37/180 [00:10<00:38,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.7919, loss: 0.0517, reg_loss: 0.0000 ||:  21%|██        | 38/180 [00:10<00:43,  3.28it/s]\u001b[A\n",
            "pearson(r): 0.7950, loss: 0.0512, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [00:11<00:40,  3.50it/s]\u001b[A\n",
            "pearson(r): 0.7908, loss: 0.0520, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [00:11<00:50,  2.78it/s]\u001b[A\n",
            "pearson(r): 0.7902, loss: 0.0520, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [00:11<00:46,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7934, loss: 0.0516, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [00:12<00:38,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7941, loss: 0.0513, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [00:12<00:36,  3.80it/s]\u001b[A\n",
            "pearson(r): 0.7961, loss: 0.0507, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [00:12<00:34,  3.91it/s]\u001b[A\n",
            "pearson(r): 0.7959, loss: 0.0507, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [00:12<00:33,  3.99it/s]\u001b[A\n",
            "pearson(r): 0.7960, loss: 0.0506, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [00:13<00:36,  3.70it/s]\u001b[A\n",
            "pearson(r): 0.7946, loss: 0.0508, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [00:13<00:36,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7940, loss: 0.0509, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [00:13<00:35,  3.68it/s]\u001b[A\n",
            "pearson(r): 0.7925, loss: 0.0513, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [00:13<00:36,  3.57it/s]\u001b[A\n",
            "pearson(r): 0.7928, loss: 0.0511, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [00:14<00:34,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7923, loss: 0.0509, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [00:14<00:34,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7931, loss: 0.0507, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [00:14<00:33,  3.81it/s]\u001b[A\n",
            "pearson(r): 0.7909, loss: 0.0511, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [00:14<00:33,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.7925, loss: 0.0509, reg_loss: 0.0000 ||:  30%|███       | 54/180 [00:15<00:37,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.7922, loss: 0.0509, reg_loss: 0.0000 ||:  31%|███       | 55/180 [00:15<00:36,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7915, loss: 0.0510, reg_loss: 0.0000 ||:  31%|███       | 56/180 [00:15<00:36,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.7929, loss: 0.0510, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [00:16<00:36,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7940, loss: 0.0507, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [00:16<00:36,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.7946, loss: 0.0507, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [00:16<00:35,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.7952, loss: 0.0505, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [00:17<00:34,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7938, loss: 0.0507, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [00:17<00:33,  3.54it/s]\u001b[A\n",
            "pearson(r): 0.7936, loss: 0.0509, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [00:17<00:35,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7941, loss: 0.0507, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [00:17<00:32,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.7954, loss: 0.0503, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [00:18<00:32,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7946, loss: 0.0505, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [00:18<00:33,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7954, loss: 0.0503, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [00:18<00:30,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7956, loss: 0.0503, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [00:19<00:33,  3.36it/s]\u001b[A\n",
            "pearson(r): 0.7957, loss: 0.0506, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [00:19<00:41,  2.71it/s]\u001b[A\n",
            "pearson(r): 0.7964, loss: 0.0507, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [00:19<00:36,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7966, loss: 0.0506, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [00:20<00:35,  3.08it/s]\u001b[A\n",
            "pearson(r): 0.7971, loss: 0.0505, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [00:20<00:33,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.7967, loss: 0.0504, reg_loss: 0.0000 ||:  40%|████      | 72/180 [00:20<00:36,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7954, loss: 0.0505, reg_loss: 0.0000 ||:  41%|████      | 73/180 [00:21<00:32,  3.31it/s]\u001b[A\n",
            "pearson(r): 0.7951, loss: 0.0505, reg_loss: 0.0000 ||:  41%|████      | 74/180 [00:21<00:31,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7932, loss: 0.0507, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [00:21<00:38,  2.72it/s]\u001b[A\n",
            "pearson(r): 0.7937, loss: 0.0506, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [00:22<00:38,  2.70it/s]\u001b[A\n",
            "pearson(r): 0.7949, loss: 0.0504, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [00:22<00:36,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.7930, loss: 0.0507, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [00:22<00:34,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7935, loss: 0.0506, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [00:23<00:30,  3.35it/s]\u001b[A\n",
            "pearson(r): 0.7922, loss: 0.0511, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [00:23<00:28,  3.46it/s]\u001b[A\n",
            "pearson(r): 0.7904, loss: 0.0513, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [00:23<00:26,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.7908, loss: 0.0509, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [00:23<00:25,  3.86it/s]\u001b[A\n",
            "pearson(r): 0.7910, loss: 0.0508, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [00:24<00:32,  2.94it/s]\u001b[A\n",
            "pearson(r): 0.7914, loss: 0.0508, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [00:24<00:30,  3.10it/s]\u001b[A\n",
            "pearson(r): 0.7923, loss: 0.0507, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [00:24<00:31,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7932, loss: 0.0506, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [00:25<00:34,  2.69it/s]\u001b[A\n",
            "pearson(r): 0.7938, loss: 0.0505, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [00:25<00:31,  2.93it/s]\u001b[A\n",
            "pearson(r): 0.7951, loss: 0.0504, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [00:25<00:29,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7961, loss: 0.0502, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [00:26<00:26,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.7963, loss: 0.0502, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [00:26<00:26,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.7965, loss: 0.0502, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [00:26<00:26,  3.39it/s]\u001b[A\n",
            "pearson(r): 0.7966, loss: 0.0502, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [00:26<00:23,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.7970, loss: 0.0500, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [00:27<00:22,  3.91it/s]\u001b[A\n",
            "pearson(r): 0.7972, loss: 0.0499, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [00:27<00:22,  3.89it/s]\u001b[A\n",
            "pearson(r): 0.7982, loss: 0.0497, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [00:27<00:22,  3.84it/s]\u001b[A\n",
            "pearson(r): 0.7969, loss: 0.0496, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [00:28<00:25,  3.23it/s]\u001b[A\n",
            "pearson(r): 0.7969, loss: 0.0496, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [00:28<00:24,  3.34it/s]\u001b[A\n",
            "pearson(r): 0.7962, loss: 0.0498, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [00:28<00:27,  3.00it/s]\u001b[A\n",
            "pearson(r): 0.7981, loss: 0.0497, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [00:29<00:23,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7976, loss: 0.0498, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [00:29<00:23,  3.37it/s]\u001b[A\n",
            "pearson(r): 0.7969, loss: 0.0498, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [00:29<00:23,  3.38it/s]\u001b[A\n",
            "pearson(r): 0.7979, loss: 0.0496, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [00:29<00:22,  3.47it/s]\u001b[A\n",
            "pearson(r): 0.7980, loss: 0.0496, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [00:30<00:21,  3.58it/s]\u001b[A\n",
            "pearson(r): 0.7984, loss: 0.0495, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [00:30<00:19,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.7974, loss: 0.0496, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [00:30<00:19,  3.90it/s]\u001b[A\n",
            "pearson(r): 0.7969, loss: 0.0497, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [00:30<00:19,  3.75it/s]\u001b[A\n",
            "pearson(r): 0.7984, loss: 0.0495, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [00:31<00:21,  3.41it/s]\u001b[A\n",
            "pearson(r): 0.7982, loss: 0.0496, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [00:31<00:20,  3.56it/s]\u001b[A\n",
            "pearson(r): 0.7986, loss: 0.0495, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [00:31<00:17,  3.98it/s]\u001b[A\n",
            "pearson(r): 0.7980, loss: 0.0495, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [00:31<00:18,  3.86it/s]\u001b[A\n",
            "pearson(r): 0.7976, loss: 0.0495, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [00:32<00:23,  2.92it/s]\u001b[A\n",
            "pearson(r): 0.7978, loss: 0.0494, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [00:32<00:22,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.7984, loss: 0.0496, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [00:33<00:20,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.7983, loss: 0.0495, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [00:33<00:19,  3.44it/s]\u001b[A\n",
            "pearson(r): 0.7983, loss: 0.0496, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [00:33<00:18,  3.53it/s]\u001b[A\n",
            "pearson(r): 0.7985, loss: 0.0495, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [00:34<00:20,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7983, loss: 0.0495, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [00:34<00:19,  3.24it/s]\u001b[A\n",
            "pearson(r): 0.7987, loss: 0.0494, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [00:34<00:20,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.7987, loss: 0.0493, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [00:34<00:20,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.7987, loss: 0.0494, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [00:35<00:19,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.7989, loss: 0.0494, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [00:35<00:17,  3.32it/s]\u001b[A\n",
            "pearson(r): 0.7992, loss: 0.0495, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [00:35<00:16,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.7994, loss: 0.0495, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [00:36<00:15,  3.63it/s]\u001b[A\n",
            "pearson(r): 0.7987, loss: 0.0497, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [00:36<00:14,  3.82it/s]\u001b[A\n",
            "pearson(r): 0.7995, loss: 0.0496, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [00:36<00:12,  4.31it/s]\u001b[A\n",
            "pearson(r): 0.7995, loss: 0.0497, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [00:36<00:13,  4.10it/s]\u001b[A\n",
            "pearson(r): 0.7988, loss: 0.0499, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [00:36<00:13,  3.99it/s]\u001b[A\n",
            "pearson(r): 0.7990, loss: 0.0498, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [00:37<00:13,  3.94it/s]\u001b[A\n",
            "pearson(r): 0.7984, loss: 0.0498, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [00:37<00:17,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7982, loss: 0.0499, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [00:38<00:16,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7983, loss: 0.0500, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [00:38<00:17,  2.82it/s]\u001b[A\n",
            "pearson(r): 0.7985, loss: 0.0499, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [00:38<00:15,  3.04it/s]\u001b[A\n",
            "pearson(r): 0.7985, loss: 0.0498, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [00:39<00:14,  3.15it/s]\u001b[A\n",
            "pearson(r): 0.7992, loss: 0.0498, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [00:39<00:14,  3.07it/s]\u001b[A\n",
            "pearson(r): 0.7989, loss: 0.0497, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [00:39<00:17,  2.56it/s]\u001b[A\n",
            "pearson(r): 0.7990, loss: 0.0497, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [00:40<00:15,  2.82it/s]\u001b[A\n",
            "pearson(r): 0.7989, loss: 0.0496, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [00:40<00:14,  3.03it/s]\u001b[A\n",
            "pearson(r): 0.7995, loss: 0.0494, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [00:40<00:13,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.8000, loss: 0.0494, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [00:41<00:12,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.8004, loss: 0.0492, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [00:41<00:11,  3.45it/s]\u001b[A\n",
            "pearson(r): 0.8010, loss: 0.0491, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [00:41<00:10,  3.59it/s]\u001b[A\n",
            "pearson(r): 0.8012, loss: 0.0491, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [00:41<00:11,  3.27it/s]\u001b[A\n",
            "pearson(r): 0.8013, loss: 0.0490, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [00:42<00:10,  3.67it/s]\u001b[A\n",
            "pearson(r): 0.8014, loss: 0.0490, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [00:42<00:09,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.8013, loss: 0.0489, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [00:42<00:09,  3.77it/s]\u001b[A\n",
            "pearson(r): 0.8018, loss: 0.0488, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [00:42<00:09,  3.74it/s]\u001b[A\n",
            "pearson(r): 0.8019, loss: 0.0488, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [00:43<00:08,  3.73it/s]\u001b[A\n",
            "pearson(r): 0.8021, loss: 0.0487, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [00:43<00:08,  3.79it/s]\u001b[A\n",
            "pearson(r): 0.8020, loss: 0.0487, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [00:43<00:09,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.8022, loss: 0.0487, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [00:44<00:07,  3.76it/s]\u001b[A\n",
            "pearson(r): 0.8022, loss: 0.0487, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [00:44<00:07,  3.85it/s]\u001b[A\n",
            "pearson(r): 0.8024, loss: 0.0487, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [00:44<00:06,  4.04it/s]\u001b[A\n",
            "pearson(r): 0.8022, loss: 0.0487, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [00:45<00:08,  3.01it/s]\u001b[A\n",
            "pearson(r): 0.8018, loss: 0.0488, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [00:45<00:07,  3.26it/s]\u001b[A\n",
            "pearson(r): 0.8013, loss: 0.0489, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [00:45<00:07,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.8013, loss: 0.0489, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [00:46<00:08,  2.78it/s]\u001b[A\n",
            "pearson(r): 0.8012, loss: 0.0489, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [00:46<00:07,  2.94it/s]\u001b[A\n",
            "pearson(r): 0.8009, loss: 0.0490, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [00:46<00:06,  3.18it/s]\u001b[A\n",
            "pearson(r): 0.8007, loss: 0.0490, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [00:47<00:08,  2.61it/s]\u001b[A\n",
            "pearson(r): 0.8000, loss: 0.0492, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [00:47<00:07,  2.83it/s]\u001b[A\n",
            "pearson(r): 0.7996, loss: 0.0492, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [00:47<00:06,  3.09it/s]\u001b[A\n",
            "pearson(r): 0.7992, loss: 0.0493, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [00:48<00:06,  2.72it/s]\u001b[A\n",
            "pearson(r): 0.7989, loss: 0.0492, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [00:48<00:07,  2.41it/s]\u001b[A\n",
            "pearson(r): 0.7989, loss: 0.0492, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [00:48<00:06,  2.60it/s]\u001b[A\n",
            "pearson(r): 0.7986, loss: 0.0493, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [00:49<00:05,  2.70it/s]\u001b[A\n",
            "pearson(r): 0.7980, loss: 0.0493, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [00:49<00:04,  2.84it/s]\u001b[A\n",
            "pearson(r): 0.7976, loss: 0.0494, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [00:49<00:04,  2.99it/s]\u001b[A\n",
            "pearson(r): 0.7977, loss: 0.0494, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [00:50<00:03,  3.14it/s]\u001b[A\n",
            "pearson(r): 0.7970, loss: 0.0496, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [00:50<00:03,  2.98it/s]\u001b[A\n",
            "pearson(r): 0.7972, loss: 0.0496, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [00:50<00:03,  3.06it/s]\u001b[A\n",
            "pearson(r): 0.7974, loss: 0.0495, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [00:51<00:02,  3.17it/s]\u001b[A\n",
            "pearson(r): 0.7977, loss: 0.0495, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [00:51<00:02,  3.25it/s]\u001b[A\n",
            "pearson(r): 0.7977, loss: 0.0495, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [00:51<00:02,  3.33it/s]\u001b[A\n",
            "pearson(r): 0.7984, loss: 0.0495, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [00:52<00:01,  3.30it/s]\u001b[A\n",
            "pearson(r): 0.7986, loss: 0.0495, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [00:52<00:01,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7990, loss: 0.0495, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [00:52<00:01,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7991, loss: 0.0494, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [00:52<00:00,  3.52it/s]\u001b[A\n",
            "pearson(r): 0.7992, loss: 0.0494, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [00:53<00:00,  3.55it/s]\u001b[A\n",
            "pearson(r): 0.7994, loss: 0.0493, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [00:53<00:00,  3.60it/s]\u001b[A\n",
            "pearson(r): 0.7991, loss: 0.0494, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [00:53<00:00,  3.35it/s]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.7910, loss: 0.0364, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:01<04:35,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7885, loss: 0.0336, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:03<04:47,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7886, loss: 0.0359, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:04<04:41,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7937, loss: 0.0349, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:06<04:19,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7974, loss: 0.0343, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:08<04:51,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.8035, loss: 0.0339, reg_loss: 0.0000 ||:   3%|▎         | 6/180 [00:09<04:41,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.8047, loss: 0.0328, reg_loss: 0.0000 ||:   4%|▍         | 7/180 [00:11<04:30,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.8083, loss: 0.0312, reg_loss: 0.0000 ||:   4%|▍         | 8/180 [00:13<04:57,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.8099, loss: 0.0306, reg_loss: 0.0000 ||:   5%|▌         | 9/180 [00:14<04:55,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.8132, loss: 0.0300, reg_loss: 0.0000 ||:   6%|▌         | 10/180 [00:16<04:46,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.8190, loss: 0.0300, reg_loss: 0.0000 ||:   6%|▌         | 11/180 [00:18<04:34,  1.63s/it]\u001b[A\n",
            "pearson(r): 0.8129, loss: 0.0313, reg_loss: 0.0000 ||:   7%|▋         | 12/180 [00:20<05:39,  2.02s/it]\u001b[A\n",
            "pearson(r): 0.8172, loss: 0.0308, reg_loss: 0.0000 ||:   7%|▋         | 13/180 [00:22<05:03,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.8142, loss: 0.0312, reg_loss: 0.0000 ||:   8%|▊         | 14/180 [00:23<04:40,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.8156, loss: 0.0306, reg_loss: 0.0000 ||:   8%|▊         | 15/180 [00:24<04:15,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.8083, loss: 0.0312, reg_loss: 0.0000 ||:   9%|▉         | 16/180 [00:27<05:27,  2.00s/it]\u001b[A\n",
            "pearson(r): 0.8070, loss: 0.0308, reg_loss: 0.0000 ||:   9%|▉         | 17/180 [00:29<05:02,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.8095, loss: 0.0302, reg_loss: 0.0000 ||:  10%|█         | 18/180 [00:31<04:53,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.8065, loss: 0.0312, reg_loss: 0.0000 ||:  11%|█         | 19/180 [00:32<04:42,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.8068, loss: 0.0311, reg_loss: 0.0000 ||:  11%|█         | 20/180 [00:34<04:31,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.8059, loss: 0.0310, reg_loss: 0.0000 ||:  12%|█▏        | 21/180 [00:36<04:59,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.8052, loss: 0.0314, reg_loss: 0.0000 ||:  12%|█▏        | 22/180 [00:38<04:36,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.8014, loss: 0.0315, reg_loss: 0.0000 ||:  13%|█▎        | 23/180 [00:39<04:24,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.8015, loss: 0.0314, reg_loss: 0.0000 ||:  13%|█▎        | 24/180 [00:41<04:06,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7965, loss: 0.0320, reg_loss: 0.0000 ||:  14%|█▍        | 25/180 [00:42<03:44,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7996, loss: 0.0316, reg_loss: 0.0000 ||:  14%|█▍        | 26/180 [00:43<03:35,  1.40s/it]\u001b[A\n",
            "pearson(r): 0.8003, loss: 0.0314, reg_loss: 0.0000 ||:  15%|█▌        | 27/180 [00:45<03:54,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.8017, loss: 0.0314, reg_loss: 0.0000 ||:  16%|█▌        | 28/180 [00:46<03:29,  1.38s/it]\u001b[A\n",
            "pearson(r): 0.8017, loss: 0.0314, reg_loss: 0.0000 ||:  16%|█▌        | 29/180 [00:47<03:32,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.7939, loss: 0.0321, reg_loss: 0.0000 ||:  17%|█▋        | 30/180 [00:49<03:44,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7906, loss: 0.0321, reg_loss: 0.0000 ||:  17%|█▋        | 31/180 [00:50<03:37,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.7869, loss: 0.0322, reg_loss: 0.0000 ||:  18%|█▊        | 32/180 [00:53<04:44,  1.92s/it]\u001b[A\n",
            "pearson(r): 0.7867, loss: 0.0323, reg_loss: 0.0000 ||:  18%|█▊        | 33/180 [00:55<04:23,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.7871, loss: 0.0322, reg_loss: 0.0000 ||:  19%|█▉        | 34/180 [00:56<04:01,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7862, loss: 0.0324, reg_loss: 0.0000 ||:  19%|█▉        | 35/180 [00:58<03:52,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7850, loss: 0.0324, reg_loss: 0.0000 ||:  20%|██        | 36/180 [00:59<04:00,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7826, loss: 0.0326, reg_loss: 0.0000 ||:  21%|██        | 37/180 [01:01<03:49,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7837, loss: 0.0325, reg_loss: 0.0000 ||:  21%|██        | 38/180 [01:02<03:30,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7825, loss: 0.0324, reg_loss: 0.0000 ||:  22%|██▏       | 39/180 [01:04<03:26,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0326, reg_loss: 0.0000 ||:  22%|██▏       | 40/180 [01:06<04:04,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7831, loss: 0.0327, reg_loss: 0.0000 ||:  23%|██▎       | 41/180 [01:08<04:01,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7837, loss: 0.0325, reg_loss: 0.0000 ||:  23%|██▎       | 42/180 [01:09<03:51,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7803, loss: 0.0330, reg_loss: 0.0000 ||:  24%|██▍       | 43/180 [01:12<04:48,  2.11s/it]\u001b[A\n",
            "pearson(r): 0.7754, loss: 0.0335, reg_loss: 0.0000 ||:  24%|██▍       | 44/180 [01:14<04:17,  1.89s/it]\u001b[A\n",
            "pearson(r): 0.7774, loss: 0.0334, reg_loss: 0.0000 ||:  25%|██▌       | 45/180 [01:15<03:57,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7789, loss: 0.0335, reg_loss: 0.0000 ||:  26%|██▌       | 46/180 [01:17<03:51,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7776, loss: 0.0335, reg_loss: 0.0000 ||:  26%|██▌       | 47/180 [01:18<03:34,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7774, loss: 0.0335, reg_loss: 0.0000 ||:  27%|██▋       | 48/180 [01:20<03:25,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7753, loss: 0.0336, reg_loss: 0.0000 ||:  27%|██▋       | 49/180 [01:23<04:18,  1.97s/it]\u001b[A\n",
            "pearson(r): 0.7745, loss: 0.0338, reg_loss: 0.0000 ||:  28%|██▊       | 50/180 [01:24<04:03,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.7777, loss: 0.0336, reg_loss: 0.0000 ||:  28%|██▊       | 51/180 [01:26<03:41,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7782, loss: 0.0334, reg_loss: 0.0000 ||:  29%|██▉       | 52/180 [01:28<03:52,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.7807, loss: 0.0332, reg_loss: 0.0000 ||:  29%|██▉       | 53/180 [01:29<03:43,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7798, loss: 0.0333, reg_loss: 0.0000 ||:  30%|███       | 54/180 [01:31<03:29,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7786, loss: 0.0334, reg_loss: 0.0000 ||:  31%|███       | 55/180 [01:32<03:24,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7784, loss: 0.0333, reg_loss: 0.0000 ||:  31%|███       | 56/180 [01:34<03:17,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7794, loss: 0.0331, reg_loss: 0.0000 ||:  32%|███▏      | 57/180 [01:35<03:15,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7802, loss: 0.0331, reg_loss: 0.0000 ||:  32%|███▏      | 58/180 [01:37<03:08,  1.54s/it]\u001b[A\n",
            "pearson(r): 0.7808, loss: 0.0332, reg_loss: 0.0000 ||:  33%|███▎      | 59/180 [01:38<03:03,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7828, loss: 0.0329, reg_loss: 0.0000 ||:  33%|███▎      | 60/180 [01:40<03:08,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7832, loss: 0.0329, reg_loss: 0.0000 ||:  34%|███▍      | 61/180 [01:42<03:08,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7829, loss: 0.0328, reg_loss: 0.0000 ||:  34%|███▍      | 62/180 [01:43<02:54,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0328, reg_loss: 0.0000 ||:  35%|███▌      | 63/180 [01:44<02:54,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7843, loss: 0.0328, reg_loss: 0.0000 ||:  36%|███▌      | 64/180 [01:46<02:53,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7828, loss: 0.0329, reg_loss: 0.0000 ||:  36%|███▌      | 65/180 [01:48<03:12,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7828, loss: 0.0329, reg_loss: 0.0000 ||:  37%|███▋      | 66/180 [01:49<03:00,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7823, loss: 0.0329, reg_loss: 0.0000 ||:  37%|███▋      | 67/180 [01:51<02:52,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7814, loss: 0.0329, reg_loss: 0.0000 ||:  38%|███▊      | 68/180 [01:52<02:49,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7806, loss: 0.0329, reg_loss: 0.0000 ||:  38%|███▊      | 69/180 [01:54<02:49,  1.53s/it]\u001b[A\n",
            "pearson(r): 0.7801, loss: 0.0329, reg_loss: 0.0000 ||:  39%|███▉      | 70/180 [01:55<02:53,  1.58s/it]\u001b[A\n",
            "pearson(r): 0.7811, loss: 0.0330, reg_loss: 0.0000 ||:  39%|███▉      | 71/180 [01:57<03:05,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7811, loss: 0.0329, reg_loss: 0.0000 ||:  40%|████      | 72/180 [01:59<02:50,  1.57s/it]\u001b[A\n",
            "pearson(r): 0.7805, loss: 0.0330, reg_loss: 0.0000 ||:  41%|████      | 73/180 [02:00<02:34,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7811, loss: 0.0329, reg_loss: 0.0000 ||:  41%|████      | 74/180 [02:01<02:30,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7828, loss: 0.0327, reg_loss: 0.0000 ||:  42%|████▏     | 75/180 [02:03<02:36,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0328, reg_loss: 0.0000 ||:  42%|████▏     | 76/180 [02:04<02:20,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.7813, loss: 0.0329, reg_loss: 0.0000 ||:  43%|████▎     | 77/180 [02:05<02:19,  1.36s/it]\u001b[A\n",
            "pearson(r): 0.7815, loss: 0.0329, reg_loss: 0.0000 ||:  43%|████▎     | 78/180 [02:07<02:25,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7807, loss: 0.0331, reg_loss: 0.0000 ||:  44%|████▍     | 79/180 [02:10<03:12,  1.90s/it]\u001b[A\n",
            "pearson(r): 0.7809, loss: 0.0330, reg_loss: 0.0000 ||:  44%|████▍     | 80/180 [02:11<03:02,  1.82s/it]\u001b[A\n",
            "pearson(r): 0.7818, loss: 0.0330, reg_loss: 0.0000 ||:  45%|████▌     | 81/180 [02:13<02:47,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.7813, loss: 0.0331, reg_loss: 0.0000 ||:  46%|████▌     | 82/180 [02:16<03:18,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.7823, loss: 0.0330, reg_loss: 0.0000 ||:  46%|████▌     | 83/180 [02:17<02:51,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7823, loss: 0.0330, reg_loss: 0.0000 ||:  47%|████▋     | 84/180 [02:18<02:43,  1.71s/it]\u001b[A\n",
            "pearson(r): 0.7822, loss: 0.0331, reg_loss: 0.0000 ||:  47%|████▋     | 85/180 [02:20<02:51,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.7819, loss: 0.0331, reg_loss: 0.0000 ||:  48%|████▊     | 86/180 [02:22<02:38,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.7826, loss: 0.0329, reg_loss: 0.0000 ||:  48%|████▊     | 87/180 [02:24<02:44,  1.77s/it]\u001b[A\n",
            "pearson(r): 0.7831, loss: 0.0328, reg_loss: 0.0000 ||:  49%|████▉     | 88/180 [02:25<02:35,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.7837, loss: 0.0327, reg_loss: 0.0000 ||:  49%|████▉     | 89/180 [02:28<02:48,  1.86s/it]\u001b[A\n",
            "pearson(r): 0.7835, loss: 0.0326, reg_loss: 0.0000 ||:  50%|█████     | 90/180 [02:29<02:36,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0327, reg_loss: 0.0000 ||:  51%|█████     | 91/180 [02:31<02:33,  1.72s/it]\u001b[A\n",
            "pearson(r): 0.7825, loss: 0.0326, reg_loss: 0.0000 ||:  51%|█████     | 92/180 [02:32<02:24,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7830, loss: 0.0328, reg_loss: 0.0000 ||:  52%|█████▏    | 93/180 [02:34<02:18,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7839, loss: 0.0328, reg_loss: 0.0000 ||:  52%|█████▏    | 94/180 [02:35<02:18,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7839, loss: 0.0328, reg_loss: 0.0000 ||:  53%|█████▎    | 95/180 [02:37<02:16,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7840, loss: 0.0327, reg_loss: 0.0000 ||:  53%|█████▎    | 96/180 [02:38<02:15,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7853, loss: 0.0326, reg_loss: 0.0000 ||:  54%|█████▍    | 97/180 [02:40<02:09,  1.56s/it]\u001b[A\n",
            "pearson(r): 0.7852, loss: 0.0325, reg_loss: 0.0000 ||:  54%|█████▍    | 98/180 [02:41<01:57,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.7851, loss: 0.0325, reg_loss: 0.0000 ||:  55%|█████▌    | 99/180 [02:43<01:58,  1.46s/it]\u001b[A\n",
            "pearson(r): 0.7855, loss: 0.0324, reg_loss: 0.0000 ||:  56%|█████▌    | 100/180 [02:44<01:55,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7854, loss: 0.0324, reg_loss: 0.0000 ||:  56%|█████▌    | 101/180 [02:45<01:53,  1.44s/it]\u001b[A\n",
            "pearson(r): 0.7851, loss: 0.0325, reg_loss: 0.0000 ||:  57%|█████▋    | 102/180 [02:47<01:50,  1.42s/it]\u001b[A\n",
            "pearson(r): 0.7855, loss: 0.0324, reg_loss: 0.0000 ||:  57%|█████▋    | 103/180 [02:48<01:53,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7847, loss: 0.0325, reg_loss: 0.0000 ||:  58%|█████▊    | 104/180 [02:50<01:57,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7847, loss: 0.0325, reg_loss: 0.0000 ||:  58%|█████▊    | 105/180 [02:52<01:53,  1.51s/it]\u001b[A\n",
            "pearson(r): 0.7835, loss: 0.0327, reg_loss: 0.0000 ||:  59%|█████▉    | 106/180 [02:54<02:04,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7832, loss: 0.0327, reg_loss: 0.0000 ||:  59%|█████▉    | 107/180 [02:56<02:16,  1.87s/it]\u001b[A\n",
            "pearson(r): 0.7829, loss: 0.0327, reg_loss: 0.0000 ||:  60%|██████    | 108/180 [02:57<02:05,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7831, loss: 0.0327, reg_loss: 0.0000 ||:  61%|██████    | 109/180 [02:59<01:56,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0328, reg_loss: 0.0000 ||:  61%|██████    | 110/180 [03:00<01:48,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7822, loss: 0.0329, reg_loss: 0.0000 ||:  62%|██████▏   | 111/180 [03:02<01:44,  1.52s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0329, reg_loss: 0.0000 ||:  62%|██████▏   | 112/180 [03:04<01:52,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7830, loss: 0.0329, reg_loss: 0.0000 ||:  63%|██████▎   | 113/180 [03:05<01:46,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7816, loss: 0.0331, reg_loss: 0.0000 ||:  63%|██████▎   | 114/180 [03:08<02:14,  2.04s/it]\u001b[A\n",
            "pearson(r): 0.7816, loss: 0.0331, reg_loss: 0.0000 ||:  64%|██████▍   | 115/180 [03:10<02:02,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7823, loss: 0.0330, reg_loss: 0.0000 ||:  64%|██████▍   | 116/180 [03:11<01:51,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7829, loss: 0.0329, reg_loss: 0.0000 ||:  65%|██████▌   | 117/180 [03:12<01:41,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7829, loss: 0.0330, reg_loss: 0.0000 ||:  66%|██████▌   | 118/180 [03:14<01:38,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7810, loss: 0.0332, reg_loss: 0.0000 ||:  66%|██████▌   | 119/180 [03:17<02:03,  2.02s/it]\u001b[A\n",
            "pearson(r): 0.7813, loss: 0.0332, reg_loss: 0.0000 ||:  67%|██████▋   | 120/180 [03:18<01:45,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.7819, loss: 0.0331, reg_loss: 0.0000 ||:  67%|██████▋   | 121/180 [03:19<01:36,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7823, loss: 0.0331, reg_loss: 0.0000 ||:  68%|██████▊   | 122/180 [03:21<01:38,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7820, loss: 0.0331, reg_loss: 0.0000 ||:  68%|██████▊   | 123/180 [03:23<01:30,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7824, loss: 0.0330, reg_loss: 0.0000 ||:  69%|██████▉   | 124/180 [03:24<01:34,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7819, loss: 0.0331, reg_loss: 0.0000 ||:  69%|██████▉   | 125/180 [03:26<01:28,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7816, loss: 0.0331, reg_loss: 0.0000 ||:  70%|███████   | 126/180 [03:27<01:26,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7810, loss: 0.0333, reg_loss: 0.0000 ||:  71%|███████   | 127/180 [03:30<01:47,  2.02s/it]\u001b[A\n",
            "pearson(r): 0.7808, loss: 0.0333, reg_loss: 0.0000 ||:  71%|███████   | 128/180 [03:33<02:00,  2.32s/it]\u001b[A\n",
            "pearson(r): 0.7810, loss: 0.0333, reg_loss: 0.0000 ||:  72%|███████▏  | 129/180 [03:35<01:46,  2.09s/it]\u001b[A\n",
            "pearson(r): 0.7813, loss: 0.0333, reg_loss: 0.0000 ||:  72%|███████▏  | 130/180 [03:37<01:44,  2.08s/it]\u001b[A\n",
            "pearson(r): 0.7819, loss: 0.0333, reg_loss: 0.0000 ||:  73%|███████▎  | 131/180 [03:39<01:35,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.7820, loss: 0.0333, reg_loss: 0.0000 ||:  73%|███████▎  | 132/180 [03:40<01:25,  1.78s/it]\u001b[A\n",
            "pearson(r): 0.7826, loss: 0.0333, reg_loss: 0.0000 ||:  74%|███████▍  | 133/180 [03:42<01:21,  1.73s/it]\u001b[A\n",
            "pearson(r): 0.7830, loss: 0.0332, reg_loss: 0.0000 ||:  74%|███████▍  | 134/180 [03:43<01:16,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0332, reg_loss: 0.0000 ||:  75%|███████▌  | 135/180 [03:45<01:12,  1.62s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0332, reg_loss: 0.0000 ||:  76%|███████▌  | 136/180 [03:48<01:26,  1.97s/it]\u001b[A\n",
            "pearson(r): 0.7824, loss: 0.0332, reg_loss: 0.0000 ||:  76%|███████▌  | 137/180 [03:50<01:28,  2.07s/it]\u001b[A\n",
            "pearson(r): 0.7820, loss: 0.0333, reg_loss: 0.0000 ||:  77%|███████▋  | 138/180 [03:51<01:18,  1.88s/it]\u001b[A\n",
            "pearson(r): 0.7827, loss: 0.0331, reg_loss: 0.0000 ||:  77%|███████▋  | 139/180 [03:53<01:09,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.7823, loss: 0.0331, reg_loss: 0.0000 ||:  78%|███████▊  | 140/180 [03:54<01:05,  1.65s/it]\u001b[A\n",
            "pearson(r): 0.7808, loss: 0.0333, reg_loss: 0.0000 ||:  78%|███████▊  | 141/180 [03:55<00:58,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7804, loss: 0.0334, reg_loss: 0.0000 ||:  79%|███████▉  | 142/180 [03:57<00:57,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7800, loss: 0.0335, reg_loss: 0.0000 ||:  79%|███████▉  | 143/180 [03:58<00:52,  1.41s/it]\u001b[A\n",
            "pearson(r): 0.7789, loss: 0.0336, reg_loss: 0.0000 ||:  80%|████████  | 144/180 [03:59<00:51,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.7795, loss: 0.0335, reg_loss: 0.0000 ||:  81%|████████  | 145/180 [04:01<00:50,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.7790, loss: 0.0336, reg_loss: 0.0000 ||:  81%|████████  | 146/180 [04:02<00:50,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7791, loss: 0.0336, reg_loss: 0.0000 ||:  82%|████████▏ | 147/180 [04:04<00:48,  1.47s/it]\u001b[A\n",
            "pearson(r): 0.7773, loss: 0.0339, reg_loss: 0.0000 ||:  82%|████████▏ | 148/180 [04:05<00:48,  1.50s/it]\u001b[A\n",
            "pearson(r): 0.7769, loss: 0.0339, reg_loss: 0.0000 ||:  83%|████████▎ | 149/180 [04:07<00:45,  1.45s/it]\u001b[A\n",
            "pearson(r): 0.7765, loss: 0.0340, reg_loss: 0.0000 ||:  83%|████████▎ | 150/180 [04:08<00:42,  1.43s/it]\u001b[A\n",
            "pearson(r): 0.7765, loss: 0.0340, reg_loss: 0.0000 ||:  84%|████████▍ | 151/180 [04:10<00:49,  1.69s/it]\u001b[A\n",
            "pearson(r): 0.7760, loss: 0.0341, reg_loss: 0.0000 ||:  84%|████████▍ | 152/180 [04:12<00:44,  1.59s/it]\u001b[A\n",
            "pearson(r): 0.7755, loss: 0.0341, reg_loss: 0.0000 ||:  85%|████████▌ | 153/180 [04:14<00:48,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.7753, loss: 0.0341, reg_loss: 0.0000 ||:  86%|████████▌ | 154/180 [04:16<00:45,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7742, loss: 0.0342, reg_loss: 0.0000 ||:  86%|████████▌ | 155/180 [04:19<00:53,  2.15s/it]\u001b[A\n",
            "pearson(r): 0.7745, loss: 0.0342, reg_loss: 0.0000 ||:  87%|████████▋ | 156/180 [04:20<00:46,  1.95s/it]\u001b[A\n",
            "pearson(r): 0.7744, loss: 0.0342, reg_loss: 0.0000 ||:  87%|████████▋ | 157/180 [04:22<00:41,  1.79s/it]\u001b[A\n",
            "pearson(r): 0.7734, loss: 0.0343, reg_loss: 0.0000 ||:  88%|████████▊ | 158/180 [04:23<00:38,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7729, loss: 0.0344, reg_loss: 0.0000 ||:  88%|████████▊ | 159/180 [04:25<00:35,  1.68s/it]\u001b[A\n",
            "pearson(r): 0.7733, loss: 0.0344, reg_loss: 0.0000 ||:  89%|████████▉ | 160/180 [04:26<00:32,  1.64s/it]\u001b[A\n",
            "pearson(r): 0.7733, loss: 0.0344, reg_loss: 0.0000 ||:  89%|████████▉ | 161/180 [04:29<00:34,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.7733, loss: 0.0344, reg_loss: 0.0000 ||:  90%|█████████ | 162/180 [04:30<00:31,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.7725, loss: 0.0346, reg_loss: 0.0000 ||:  91%|█████████ | 163/180 [04:32<00:27,  1.61s/it]\u001b[A\n",
            "pearson(r): 0.7728, loss: 0.0345, reg_loss: 0.0000 ||:  91%|█████████ | 164/180 [04:35<00:32,  2.03s/it]\u001b[A\n",
            "pearson(r): 0.7732, loss: 0.0346, reg_loss: 0.0000 ||:  92%|█████████▏| 165/180 [04:36<00:27,  1.81s/it]\u001b[A\n",
            "pearson(r): 0.7729, loss: 0.0347, reg_loss: 0.0000 ||:  92%|█████████▏| 166/180 [04:37<00:24,  1.75s/it]\u001b[A\n",
            "pearson(r): 0.7729, loss: 0.0347, reg_loss: 0.0000 ||:  93%|█████████▎| 167/180 [04:39<00:22,  1.70s/it]\u001b[A\n",
            "pearson(r): 0.7723, loss: 0.0348, reg_loss: 0.0000 ||:  93%|█████████▎| 168/180 [04:40<00:19,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7718, loss: 0.0349, reg_loss: 0.0000 ||:  94%|█████████▍| 169/180 [04:42<00:16,  1.48s/it]\u001b[A\n",
            "pearson(r): 0.7724, loss: 0.0348, reg_loss: 0.0000 ||:  94%|█████████▍| 170/180 [04:43<00:15,  1.60s/it]\u001b[A\n",
            "pearson(r): 0.7728, loss: 0.0348, reg_loss: 0.0000 ||:  95%|█████████▌| 171/180 [04:46<00:15,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7729, loss: 0.0348, reg_loss: 0.0000 ||:  96%|█████████▌| 172/180 [04:48<00:14,  1.85s/it]\u001b[A\n",
            "pearson(r): 0.7725, loss: 0.0349, reg_loss: 0.0000 ||:  96%|█████████▌| 173/180 [04:49<00:11,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7725, loss: 0.0349, reg_loss: 0.0000 ||:  97%|█████████▋| 174/180 [04:51<00:10,  1.76s/it]\u001b[A\n",
            "pearson(r): 0.7721, loss: 0.0348, reg_loss: 0.0000 ||:  97%|█████████▋| 175/180 [04:52<00:08,  1.66s/it]\u001b[A\n",
            "pearson(r): 0.7724, loss: 0.0348, reg_loss: 0.0000 ||:  98%|█████████▊| 176/180 [04:54<00:06,  1.67s/it]\u001b[A\n",
            "pearson(r): 0.7726, loss: 0.0348, reg_loss: 0.0000 ||:  98%|█████████▊| 177/180 [04:55<00:04,  1.55s/it]\u001b[A\n",
            "pearson(r): 0.7723, loss: 0.0347, reg_loss: 0.0000 ||:  99%|█████████▉| 178/180 [04:57<00:02,  1.49s/it]\u001b[A\n",
            "pearson(r): 0.7721, loss: 0.0347, reg_loss: 0.0000 ||:  99%|█████████▉| 179/180 [04:59<00:01,  1.74s/it]\u001b[A\n",
            "pearson(r): 0.7719, loss: 0.0348, reg_loss: 0.0000 ||: 100%|██████████| 180/180 [05:00<00:00,  1.67s/it]\n",
            "\n",
            "  0%|          | 0/180 [00:00<?, ?it/s]\u001b[A\n",
            "pearson(r): 0.9029, loss: 0.0329, reg_loss: 0.0000 ||:   1%|          | 1/180 [00:00<00:45,  3.96it/s]\u001b[A\n",
            "pearson(r): 0.8898, loss: 0.0377, reg_loss: 0.0000 ||:   1%|          | 2/180 [00:00<00:52,  3.40it/s]\u001b[A\n",
            "pearson(r): 0.8743, loss: 0.0425, reg_loss: 0.0000 ||:   2%|▏         | 3/180 [00:00<00:51,  3.43it/s]\u001b[A\n",
            "pearson(r): 0.8609, loss: 0.0428, reg_loss: 0.0000 ||:   2%|▏         | 4/180 [00:01<01:04,  2.72it/s]\u001b[A\n",
            "pearson(r): 0.8570, loss: 0.0416, reg_loss: 0.0000 ||:   3%|▎         | 5/180 [00:01<01:04,  2.72it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFka63P0EAWV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "dc43aefe-5aa5-484a-b8ce-2b68aad4fad7"
      },
      "source": [
        "train\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 4,\n",
              " 'best_validation_loss': 0.04422299886743228,\n",
              " 'best_validation_pearson': 0.8080389196795864,\n",
              " 'best_validation_reg_loss': 0.0,\n",
              " 'epoch': 4,\n",
              " 'peak_gpu_0_memory_MB': 7599,\n",
              " 'peak_worker_0_memory_MB': 5581.7,\n",
              " 'training_duration': '0:29:50.721154',\n",
              " 'training_epochs': 4,\n",
              " 'training_gpu_0_memory_MB': 7599,\n",
              " 'training_loss': 0.0415545004626943,\n",
              " 'training_pearson': 0.7186851943178267,\n",
              " 'training_reg_loss': 0.0,\n",
              " 'training_start_epoch': 0,\n",
              " 'training_worker_0_memory_MB': 5581.7,\n",
              " 'validation_loss': 0.04422299886743228,\n",
              " 'validation_pearson': 0.8080389196795864,\n",
              " 'validation_reg_loss': 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd29w0FX64-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqrnoH3165ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlyW6BPt5jAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b974f67c-4951-43c5-c50e-f2cdefd1a78c"
      },
      "source": [
        "#evaluate on test data\n",
        "test_evaluate = evaluate(emodel, test_data_loader)\n",
        "test_evaluate\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pearson: 0.61, loss: 0.08 ||: : 44it [00:11,  3.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 0.0828661983832717, 'pearson': 0.6100706243663699}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgxsIjLP5juA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.predictors.sentence_tagger import SentenceTaggerPredictor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByaA1h0zzmud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ismpol_m693G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model\n",
        "# Here's how to save the model.\n",
        "with open(serialization_dir + \"/model.th\", 'wb') as f: #location\n",
        "    torch.save(model.state_dict(), f)\n",
        "\n",
        "#save  vocab\n",
        "vocab.save_to_files(serialization_dir + \"/vocabulary\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQATb3hT76Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reload the model.\n",
        "vocab2 = Vocabulary.from_files(serialization_dir +\"/vocabulary\")\n",
        "model2 = LstmTagger(word_embeddings, lstm, vocab2)\n",
        "with open(\"/tmp/model.th\", 'rb') as f:\n",
        "    model2.load_state_dict(torch.load(f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf-lzdex76EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYG6Bblq-n3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9a32a4d1-cfad-4a05-9bdc-99c77c07ff47"
      },
      "source": [
        "for x in train_instances:\n",
        "  print(x)\n",
        "  break"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance with fields:\n",
            " \t first_sent: TextField of length 6 with text: \n",
            " \t\t[A, plane, is, taking, off, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
            " \t second_sent: TextField of length 7 with text: \n",
            " \t\t[An, air, plane, is, taking, off, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'ELMoTokenCharactersIndexer'} \n",
            " \t score: ArrayField with shape: (1,) and dtype: <class 'numpy.float32'>. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpGEgGqP-nmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Jk2LSO-nTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UwXzNJ7rVYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fcb02049-44f9-415b-ed7f-8333749eab72"
      },
      "source": [
        "for x,y in emodel.named_parameters():\n",
        "  if y.requires_grad:\n",
        "    print(x,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo_embedding.scalar_mix_0.gamma Parameter containing:\n",
            "tensor([1.], requires_grad=True)\n",
            "elmo_embedding.scalar_mix_0.scalar_parameters.0 Parameter containing:\n",
            "tensor([0.], requires_grad=True)\n",
            "elmo_embedding.scalar_mix_0.scalar_parameters.1 Parameter containing:\n",
            "tensor([0.], requires_grad=True)\n",
            "elmo_embedding.scalar_mix_0.scalar_parameters.2 Parameter containing:\n",
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkjmw3WCDFke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbrNE-MeDFL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kve2YrG46U4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SemanticTextDataReader(DatasetReader):\n",
        "  def __init__(self, \n",
        "               lazy: bool = False, \n",
        "               tokenizer = None, \n",
        "               token_indexers: Dict[str, TokenIndexer] = None, \n",
        "               max_tokens: int = None\n",
        "               ):\n",
        "    super().__init__(lazy)\n",
        "    self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
        "    self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
        "    self.max_tokens = max_tokens\n",
        "    \n",
        "\n",
        "  #convert given text into instance\n",
        "  @overrides\n",
        "  def text_to_instance(self, sent_1: str, sent_2: str, gold_score: float = None) -> Instance:\n",
        "    \n",
        "    #tokenize text\n",
        "    token_1 = self.tokenizer.tokenize(sent_1) \n",
        "    token_2 = self.tokenizer.tokenize(sent_2)\n",
        "    \n",
        "    #tokens\n",
        "    if self.max_tokens:\n",
        "            token_1 = token_1[:self.max_tokens]\n",
        "            token_2 = token_2[:self.max_tokens]\n",
        "    \n",
        "    #Textfield\n",
        "    text_field_1 = TextField(token_1, self.token_indexers) \n",
        "    text_field_2 = TextField(token_2, self.token_indexers)\n",
        "\n",
        "    #fields contain 'Textfield' and 'LabelField'\n",
        "    fields = {'first_sent': text_field_1, 'second_sent': text_field_2 } \n",
        "    \n",
        "    #check score/label is given or not\n",
        "    if gold_score is not None:\n",
        "      fields['score'] = ArrayField(np.array([gold_score])) # labelfield\n",
        "   \n",
        "    return Instance(fields) #instance with inputs and outputs fields \n",
        "\n",
        "    \n",
        "  #Read dataset and convert them to Iterable Instance\n",
        "  @overrides\n",
        "  def _read(self, file_path: str) -> Iterable[Instance]:\n",
        "    \n",
        "    #read data with pandas\n",
        "    df = pd.read_csv(file_path, \n",
        "              delimiter=',' , \n",
        "              header= None,\n",
        "              names= col_names\n",
        "              )\n",
        "    \n",
        "    #scale down gold-score(0-5) to (0-1) since cosine-simialrity score(0-1)\n",
        "    df['scaled_score(0-1)'] = MinMaxScaler(feature_range=(0,1)).fit_transform(df[['score(0-5)']])\n",
        "    print('Data reading started .....')\n",
        "    \n",
        "    \n",
        "    #iterate over rows in df\n",
        "    for index, row in df.iterrows():\n",
        "      sent_1 = row['sentence_1']#first sent\n",
        "      sent_2 = row['sentence_2']#second sent\n",
        "      gold_score = row['scaled_score(0-1)']#score\n",
        "      yield self.text_to_instance(sent_1, sent_2, gold_score) #iterable\n",
        "\n",
        "    print('...... Completed.')\n",
        "    print()\n",
        "  \n",
        "    "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jQCJebO_Pez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQJpr7HyFqXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.data import Vocabulary\n",
        "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmbnAdmq_QDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "21caf0d4-a3f6-4d8d-8173-1e13a68dbba1"
      },
      "source": [
        "#Read dataset\n",
        "file_path = \"/content/drive/My Drive/Google_Colab/stsbenchmark_dataset/\" #file path\n",
        "dataset_types = [ \"sts-train.csv\", \"sts-dev.csv\", \"sts-test.csv\",] # 3 datasets\n",
        "col_names = [\"genre\", \"file\", \"years\", \"_\", \"score(0-5)\", \"sentence_1\", \"sentence_2\"] #columns names\n",
        "\n",
        "\n",
        "text_tokenizer = SpacyTokenizer() #tokenizer\n",
        "dataset_reader = SemanticTextDataReader(tokenizer= text_tokenizer, token_indexers= {'tokens': SingleIdTokenIndexer()},  max_tokens=70) #initiate dataset reader\n",
        "train_instances_glove, dev_instances_glove, test_instances_glove = (dataset_reader.read(os.path.join(file_path, file_name)) for file_name in dataset_types)\n"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "456it [00:00, 2122.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data reading started .....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5749it [00:02, 2051.65it/s]\n",
            "234it [00:00, 2334.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...... Completed.\n",
            "\n",
            "Data reading started .....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1500it [00:00, 1695.53it/s]\n",
            "240it [00:00, 2395.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...... Completed.\n",
            "\n",
            "Data reading started .....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1379it [00:01, 1061.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...... Completed.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpqWeyE_YVFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c2bc79c6-a59a-4f70-e979-2ab8c8d31105"
      },
      "source": [
        "for x in train_instances_glove:\n",
        "  print(x)\n",
        "  break"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance with fields:\n",
            " \t first_sent: TextField of length 6 with text: \n",
            " \t\t[A, plane, is, taking, off, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
            " \t second_sent: TextField of length 7 with text: \n",
            " \t\t[An, air, plane, is, taking, off, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
            " \t score: ArrayField with shape: (1,) and dtype: <class 'numpy.float32'>. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbkF1pKTPr7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#counts number of time words/tokens appear in corpus\n",
        "count_tokens = defaultdict(Counter)\n",
        "for x in train_instances_glove+dev_instances_glove +test_instances_glove:\n",
        "  x.count_vocab_items(count_tokens) #instance counter\n",
        "  \n",
        " "
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzKqq157Znqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtNydgC0_PMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ebad7361-0a9d-4a23-b78b-4a6b7fefc84f"
      },
      "source": [
        "#set glove vocab\n",
        "vocab_glove = Vocabulary(counter=count_tokens)\n",
        "vocab_glove.from_instances(train_instances_glove + dev_instances_glove + test_instances_glove )\n",
        "    "
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8628/8628 [00:00<00:00, 41381.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Vocabulary with namespaces:  tokens, Size: 17739 || Non Padded Namespaces: {'*tags', '*labels'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMRyH0RoDreJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8950910b-23ca-4cd4-98da-4f37be7b5dec"
      },
      "source": [
        "#check index to token\n",
        "vocab_glove.get_token_from_index(2)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UuzkdKcfTWs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eac983ac-2b87-4d41-9f70-910f1657dcc5"
      },
      "source": [
        "#check vocab size\n",
        "vocab_glove.get_vocab_size()"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17739"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-QchPnoSKS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#index tokens into ids\n",
        "train_instances_glove.index_with(vocab_glove)\n",
        "dev_instances_glove.index_with(vocab_glove)\n",
        "test_instances_glove.index_with(vocab_glove)"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-rv-NhSSQO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85e67a22-34fa-436f-85b6-f735a48cf8a3"
      },
      "source": [
        ""
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PMEddjzRDHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#random sample and batch train data\n",
        "sampler_tr = RandomSampler(data_source=train_instances) \n",
        "batch_sampler_tr = BasicBatchSampler(sampler_tr, batch_size=32, drop_last=False) \n",
        "train_data_loader = DataLoader( train_instances_glove, batch_sampler= batch_sampler_tr)\n",
        "\n",
        "#random sample and batch dev data\n",
        "sampler_dev = RandomSampler(data_source=dev_instances) \n",
        "batch_sampler_dev = BasicBatchSampler(sampler_dev, batch_size=32, drop_last=False)\n",
        "dev_data_loader = DataLoader(dev_instances_glove, batch_sampler=batch_sampler_dev) #dev data\n",
        "\n",
        "\n",
        "#random sample and batch test data\n",
        "sampler_test = RandomSampler(data_source=test_instances) \n",
        "batch_sampler_test = BasicBatchSampler(sampler_test, batch_size=32, drop_last=False)\n",
        "test_data_loader = DataLoader(test_instances_glove, batch_sampler= batch_sampler_test) "
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srk9uLR3_PIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "b503e4bc-8745-4f8a-8156-472150ca25e9"
      },
      "source": [
        "for x in train_data_loader:\n",
        "  print(x)\n",
        "  break"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'first_sent': {'tokens': {'tokens': tensor([[  15, 4820,   11,  ...,    0,    0,    0],\n",
            "        [ 267, 1564,    5,  ...,    0,    0,    0],\n",
            "        [   7,   64,    8,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   7,   42,  108,  ...,    0,    0,    0],\n",
            "        [2471, 7414, 1795,  ...,    0,    0,    0],\n",
            "        [   7,   14,  326,  ...,    0,    0,    0]])}}, 'second_sent': {'tokens': {'tokens': tensor([[  15, 4820,   11,  ...,    0,    0,    0],\n",
            "        [ 267, 1564,    5,  ...,    0,    0,    0],\n",
            "        [   7,   14,    8,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   7,  108, 1898,  ...,    0,    0,    0],\n",
            "        [ 445, 1129,   37,  ...,    0,    0,    0],\n",
            "        [   7,   14,    8,  ...,    0,    0,    0]])}}, 'score': tensor([[0.8000],\n",
            "        [0.7200],\n",
            "        [0.5600],\n",
            "        [0.7500],\n",
            "        [0.2000],\n",
            "        [0.4500],\n",
            "        [0.1600],\n",
            "        [0.4400],\n",
            "        [0.6800],\n",
            "        [0.0800],\n",
            "        [0.7600],\n",
            "        [0.0800],\n",
            "        [0.0500],\n",
            "        [0.5200],\n",
            "        [0.2800],\n",
            "        [0.5600],\n",
            "        [0.7600],\n",
            "        [0.4000],\n",
            "        [0.1600],\n",
            "        [0.8000],\n",
            "        [0.8800],\n",
            "        [0.4400],\n",
            "        [0.8000],\n",
            "        [0.5600],\n",
            "        [0.1000],\n",
            "        [0.5334],\n",
            "        [0.5200],\n",
            "        [0.6000],\n",
            "        [0.2800],\n",
            "        [0.8000],\n",
            "        [0.7600],\n",
            "        [0.4000]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2EmxZIyDW9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8d4b937-a8bf-4783-9481-561954fcc2fb"
      },
      "source": [
        "#glove embedding\n",
        "glove_file = '/content/drive/My Drive/Google_Colab/glove/glove.6B.300d.txt'\n",
        "embedding = Embedding(num_embeddings= vocab_glove.get_vocab_size(),\n",
        "                      embedding_dim=300,\n",
        "                      pretrained_file=glove_file,\n",
        "                      vocab = vocab_glove)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:08, 49128.82it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2joLejOIIXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Textfield embedder\n",
        "glove_embedder = BasicTextFieldEmbedder(token_embedders={'tokens': embedding})"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBzcrW7ZOAMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "44c195f3-6e9a-44e7-e262-9155f703097b"
      },
      "source": [
        "#check batch\n",
        "batch = next(iter(train_data_loader))\n",
        "batch"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first_sent': {'tokens': {'tokens': tensor([[  57,   17,  380,  ...,    0,    0,    0],\n",
              "           [9474,    5,    3,  ...,    0,    0,    0],\n",
              "           [  38,   11,    4,  ...,    0,    0,    0],\n",
              "           ...,\n",
              "           [ 267,   10, 3602,  ...,    0,    0,    0],\n",
              "           [ 153,  176,   16,  ...,    0,    0,    0],\n",
              "           [3175,  273,  115,  ...,    0,    0,    0]])}},\n",
              " 'score': tensor([[0.8000],\n",
              "         [0.8000],\n",
              "         [0.6500],\n",
              "         [0.8000],\n",
              "         [0.1200],\n",
              "         [0.8400],\n",
              "         [0.9200],\n",
              "         [0.4400],\n",
              "         [0.1200],\n",
              "         [0.9600],\n",
              "         [0.9600],\n",
              "         [0.0000],\n",
              "         [0.0000],\n",
              "         [0.5000],\n",
              "         [0.0000],\n",
              "         [0.2400],\n",
              "         [0.8000],\n",
              "         [0.6000],\n",
              "         [0.0000],\n",
              "         [0.2400],\n",
              "         [0.6666],\n",
              "         [0.5600],\n",
              "         [0.6800],\n",
              "         [0.9200],\n",
              "         [0.3600],\n",
              "         [0.1600],\n",
              "         [0.0000],\n",
              "         [0.8400],\n",
              "         [0.2800],\n",
              "         [0.0400],\n",
              "         [0.6400],\n",
              "         [0.6000]]),\n",
              " 'second_sent': {'tokens': {'tokens': tensor([[  149,  1263,   481,  ...,     0,     0,     0],\n",
              "           [ 1499,  9474,   455,  ...,     0,     0,     0],\n",
              "           [ 1587,  7879,     6,  ...,     0,     0,     0],\n",
              "           ...,\n",
              "           [  267,  1651,    10,  ...,     0,     0,     0],\n",
              "           [  153,  4394, 14296,  ...,     0,     0,     0],\n",
              "           [ 3175,   273,   115,  ...,     0,     0,     0]])}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnxViMdaissi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "6893b60c-2165-4444-b598-2c2f0bca54ca"
      },
      "source": [
        "glove_embedder(batch['first_sent'])"
      ],
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0615,  0.1084,  0.7885,  ...,  0.3642, -0.7808,  0.8409],\n",
              "         [-0.0013,  0.3651, -0.0774,  ..., -0.1836, -0.7652,  0.3921],\n",
              "         [ 0.4449,  0.0927, -0.0925,  ...,  0.0595, -0.9138,  0.4311],\n",
              "         ...,\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948]],\n",
              "\n",
              "        [[-0.8988,  0.1483,  0.1223,  ...,  0.0976, -0.0756,  0.3552],\n",
              "         [-0.4440,  0.1282, -0.2525,  ..., -0.2004, -0.0822, -0.0626],\n",
              "         [-0.2971,  0.0940, -0.0967,  ...,  0.0597, -0.2285,  0.2960],\n",
              "         ...,\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948]],\n",
              "\n",
              "        [[-0.0431,  0.4088,  0.4765,  ...,  0.4686, -0.2677, -0.4158],\n",
              "         [-0.0769, -0.0212,  0.2127,  ...,  0.1835, -0.2918, -0.0465],\n",
              "         [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "         ...,\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.1807,  0.3121,  0.1129,  ..., -0.8834, -0.6066, -0.2716],\n",
              "         [-0.2576, -0.0571, -0.6719,  ..., -0.1604,  0.0467, -0.0706],\n",
              "         [-0.4514, -0.0386, -0.5384,  ...,  0.1165, -0.2985, -0.3950],\n",
              "         ...,\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948]],\n",
              "\n",
              "        [[-0.2101, -0.5001, -0.8718,  ..., -0.6656, -0.4254,  0.9596],\n",
              "         [ 0.0971, -0.3955,  0.5006,  ..., -0.0371,  0.1450, -0.2002],\n",
              "         [ 0.0406, -0.0284,  0.2501,  ..., -0.7256, -0.1930,  0.2275],\n",
              "         ...,\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948]],\n",
              "\n",
              "        [[ 0.0808, -0.4380,  0.8254,  ..., -0.0897,  0.4911, -0.3279],\n",
              "         [-0.2295,  0.3255, -0.0927,  ...,  0.5378, -1.0023,  0.8747],\n",
              "         [-0.2671,  0.2390, -0.2607,  ...,  0.1964, -0.5405,  0.3338],\n",
              "         ...,\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948],\n",
              "         [-0.0092,  0.2107,  0.0735,  ..., -0.6152,  0.1192,  0.5948]]],\n",
              "       grad_fn=<CatBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuU_VD2al5jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
